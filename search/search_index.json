{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"BENCHMARK_INFER_cn/","text":"\u63a8\u7406Benchmark \u6d4b\u8bd5\u73af\u5883: CUDA 9.0 CUDNN 7.5 TensorRT-5.1.2.2 PaddlePaddle v1.6 GPU\u5206\u522b\u4e3a: Tesla V100\u548cTesla P4 \u6d4b\u8bd5\u65b9\u5f0f: \u4e3a\u4e86\u65b9\u9762\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8f93\u5165\u91c7\u7528\u540c\u6837\u5927\u5c0f\u7684\u56fe\u7247\uff0c\u4e3a 3x640x640\uff0c\u91c7\u7528 demo/000000014439_640x640.jpg \u56fe\u7247\u3002 Batch Size=1 \u53bb\u6389\u524d10\u8f6ewarmup\u65f6\u95f4\uff0c\u6d4b\u8bd5100\u8f6e\u7684\u5e73\u5747\u65f6\u95f4\uff0c\u5355\u4f4dms/image\uff0c\u5305\u62ec\u8f93\u5165\u6570\u636e\u62f7\u8d1d\u81f3GPU\u7684\u65f6\u95f4\u3001\u8ba1\u7b97\u65f6\u95f4\u3001\u6570\u636e\u62f7\u8d1d\u53eaCPU\u7684\u65f6\u95f4\u3002 \u91c7\u7528Fluid C++\u9884\u6d4b\u5f15\u64ce: \u5305\u542bFluid C++\u9884\u6d4b\u3001Fluid-TensorRT\u9884\u6d4b\uff0c\u4e0b\u9762\u540c\u65f6\u6d4b\u8bd5\u4e86Float32 (FP32) \u548cFloat16 (FP16)\u7684\u63a8\u7406\u901f\u5ea6\u3002 \u6d4b\u8bd5\u65f6\u5f00\u542f\u4e86 FLAGS_cudnn_exhaustive_search=True\uff0c\u4f7f\u7528exhaustive\u65b9\u5f0f\u641c\u7d22\u5377\u79ef\u8ba1\u7b97\u7b97\u6cd5\u3002 \u63a8\u7406\u901f\u5ea6 \u6a21\u578b Tesla V100 Fluid (ms/image) Tesla V100 Fluid-TensorRT-FP32 (ms/image) Tesla V100 Fluid-TensorRT-FP16 (ms/image) Tesla P4 Fluid (ms/image) Tesla P4 Fluid-TensorRT-FP32 (ms/image) faster_rcnn_r50_1x 147.488 146.124 142.416 471.547 471.631 faster_rcnn_r50_2x 147.636 147.73 141.664 471.548 472.86 faster_rcnn_r50_vd_1x 146.588 144.767 141.208 459.357 457.852 faster_rcnn_r50_fpn_1x 25.11 24.758 20.744 59.411 57.585 faster_rcnn_r50_fpn_2x 25.351 24.505 20.509 59.594 57.591 faster_rcnn_r50_vd_fpn_2x 25.514 25.292 21.097 61.026 58.377 faster_rcnn_r50_fpn_gn_2x 36.959 36.173 32.356 101.339 101.212 faster_rcnn_dcn_r50_fpn_1x 28.707 28.162 27.503 68.154 67.443 faster_rcnn_dcn_r50_vd_fpn_2x 28.576 28.271 27.512 68.959 68.448 faster_rcnn_r101_1x 153.267 150.985 144.849 490.104 486.836 faster_rcnn_r101_fpn_1x 30.949 30.331 24.021 73.591 69.736 faster_rcnn_r101_fpn_2x 30.918 29.126 23.677 73.563 70.32 faster_rcnn_r101_vd_fpn_1x 31.144 30.202 23.57 74.767 70.773 faster_rcnn_r101_vd_fpn_2x 30.678 29.969 23.327 74.882 70.842 faster_rcnn_x101_vd_64x4d_fpn_1x 60.36 58.461 45.172 132.178 131.734 faster_rcnn_x101_vd_64x4d_fpn_2x 59.003 59.163 46.065 131.422 132.186 faster_rcnn_dcn_r101_vd_fpn_1x 36.862 37.205 36.539 93.273 92.616 faster_rcnn_dcn_x101_vd_64x4d_fpn_1x 78.476 78.335 77.559 185.976 185.996 faster_rcnn_se154_vd_fpn_s1x 166.282 90.508 80.738 304.653 193.234 mask_rcnn_r50_1x 160.185 160.4 160.322 - - mask_rcnn_r50_2x 159.821 159.527 160.41 - - mask_rcnn_r50_fpn_1x 95.72 95.719 92.455 259.8 258.04 mask_rcnn_r50_fpn_2x 84.545 83.567 79.269 227.284 222.975 mask_rcnn_r50_vd_fpn_2x 82.07 82.442 77.187 223.75 221.683 mask_rcnn_r50_fpn_gn_2x 94.936 94.611 91.42 265.468 263.76 mask_rcnn_dcn_r50_fpn_1x 97.828 97.433 93.76 256.295 258.056 mask_rcnn_dcn_r50_vd_fpn_2x 77.831 79.453 76.983 205.469 204.499 mask_rcnn_r101_fpn_1x 95.543 97.929 90.314 252.997 250.782 mask_rcnn_r101_vd_fpn_1x 98.046 97.647 90.272 261.286 262.108 mask_rcnn_x101_vd_64x4d_fpn_1x 115.461 115.756 102.04 296.066 293.62 mask_rcnn_x101_vd_64x4d_fpn_2x 107.144 107.29 97.275 267.636 267.577 mask_rcnn_dcn_r101_vd_fpn_1x 85.504 84.875 84.907 225.202 226.585 mask_rcnn_dcn_x101_vd_64x4d_fpn_1x 129.937 129.934 127.804 326.786 326.161 mask_rcnn_se154_vd_fpn_s1x 214.188 139.807 121.516 440.391 439.727 cascade_rcnn_r50_fpn_1x 36.866 36.949 36.637 101.851 101.912 cascade_mask_rcnn_r50_fpn_1x 110.344 106.412 100.367 301.703 297.739 cascade_rcnn_dcn_r50_fpn_1x 40.412 39.58 39.853 110.346 110.077 cascade_mask_rcnn_r50_fpn_gn_2x 170.092 168.758 163.298 527.998 529.59 cascade_rcnn_dcn_r101_vd_fpn_1x 48.414 48.849 48.701 134.9 134.846 cascade_rcnn_dcn_x101_vd_64x4d_fpn_1x 90.062 90.218 90.009 228.67 228.396 retinanet_r101_fpn_1x 55.59 54.636 48.489 90.394 83.951 retinanet_r50_fpn_1x 50.048 47.932 44.385 73.819 70.282 retinanet_x101_vd_64x4d_fpn_1x 83.329 83.446 70.76 145.936 146.168 yolov3_darknet 21.427 20.252 13.856 55.173 55.692 yolov3_darknet_voc 17.58 16.241 9.473 51.049 51.249 yolov3_mobilenet_v1 12.869 11.834 9.408 24.887 21.352 yolov3_mobilenet_v1_voc 9.118 8.146 5.575 20.787 17.169 yolov3_r34 14.914 14.125 11.176 20.798 20.822 yolov3_r34_voc 11.288 10.73 7.7 25.874 22.399 ssd_mobilenet_v1_voc 5.763 5.854 4.589 11.75 9.485 ssd_vgg16_300 28.722 29.644 20.399 73.707 74.531 ssd_vgg16_300_voc 18.425 19.288 11.298 56.297 56.201 ssd_vgg16_512 27.471 28.328 19.328 68.685 69.808 ssd_vgg16_512_voc 18.721 19.636 12.004 54.688 56.174 RCNN\u7cfb\u5217\u6a21\u578bFluid-TensorRT\u901f\u5ea6\u76f8\u6bd4Fluid\u9884\u6d4b\u6ca1\u6709\u4f18\u52bf\uff0c\u539f\u56e0\u662f: TensorRT\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u5f53\u524d\u57fa\u4e8eResNet\u7cfb\u5217\u7684RCNN\u6a21\u578b\uff0c\u53ea\u6709backbone\u90e8\u5206\u91c7\u7528\u4e86TensorRT\u5b50\u56fe\u8ba1\u7b97\uff0c\u6bd4\u8f83\u8017\u65f6\u7684stage-5\u6ca1\u6709\u57fa\u4e8eTensorRT\u8ba1\u7b97\u3002 Fluid\u5bf9CNN\u6a21\u578b\u4e5f\u505a\u4e86\u4e00\u7cfb\u5217\u7684\u878d\u5408\u4f18\u5316\u3002\u540e\u7eedTensorRT\u7248\u672c\u5347\u7ea7\u3001\u6216\u6709\u5176\u4ed6\u4f18\u5316\u7b56\u7565\u65f6\u518d\u66f4\u65b0\u6570\u636e\u3002 YOLO v3\u7cfb\u5217\u6a21\u578b\uff0cFluid-TensorRT\u76f8\u6bd4Fluid\u9884\u6d4b\u52a0\u901f5% - 10%\u4e0d\u7b49\u3002 SSD\u548cYOLOv3\u7cfb\u5217\u6a21\u578b TensorRT-FP16\u9884\u6d4b\u901f\u5ea6\u6709\u4e00\u5b9a\u7684\u4f18\u52bf\uff0c\u52a0\u901f\u7ea620% - 40%\u4e0d\u7b49\u3002\u5177\u4f53\u5982\u4e0b\u56fe\u3002","title":"\u63a8\u7406Benchmark"},{"location":"BENCHMARK_INFER_cn/#benchmark","text":"\u6d4b\u8bd5\u73af\u5883: CUDA 9.0 CUDNN 7.5 TensorRT-5.1.2.2 PaddlePaddle v1.6 GPU\u5206\u522b\u4e3a: Tesla V100\u548cTesla P4 \u6d4b\u8bd5\u65b9\u5f0f: \u4e3a\u4e86\u65b9\u9762\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8f93\u5165\u91c7\u7528\u540c\u6837\u5927\u5c0f\u7684\u56fe\u7247\uff0c\u4e3a 3x640x640\uff0c\u91c7\u7528 demo/000000014439_640x640.jpg \u56fe\u7247\u3002 Batch Size=1 \u53bb\u6389\u524d10\u8f6ewarmup\u65f6\u95f4\uff0c\u6d4b\u8bd5100\u8f6e\u7684\u5e73\u5747\u65f6\u95f4\uff0c\u5355\u4f4dms/image\uff0c\u5305\u62ec\u8f93\u5165\u6570\u636e\u62f7\u8d1d\u81f3GPU\u7684\u65f6\u95f4\u3001\u8ba1\u7b97\u65f6\u95f4\u3001\u6570\u636e\u62f7\u8d1d\u53eaCPU\u7684\u65f6\u95f4\u3002 \u91c7\u7528Fluid C++\u9884\u6d4b\u5f15\u64ce: \u5305\u542bFluid C++\u9884\u6d4b\u3001Fluid-TensorRT\u9884\u6d4b\uff0c\u4e0b\u9762\u540c\u65f6\u6d4b\u8bd5\u4e86Float32 (FP32) \u548cFloat16 (FP16)\u7684\u63a8\u7406\u901f\u5ea6\u3002 \u6d4b\u8bd5\u65f6\u5f00\u542f\u4e86 FLAGS_cudnn_exhaustive_search=True\uff0c\u4f7f\u7528exhaustive\u65b9\u5f0f\u641c\u7d22\u5377\u79ef\u8ba1\u7b97\u7b97\u6cd5\u3002","title":"\u63a8\u7406Benchmark"},{"location":"BENCHMARK_INFER_cn/#_1","text":"\u6a21\u578b Tesla V100 Fluid (ms/image) Tesla V100 Fluid-TensorRT-FP32 (ms/image) Tesla V100 Fluid-TensorRT-FP16 (ms/image) Tesla P4 Fluid (ms/image) Tesla P4 Fluid-TensorRT-FP32 (ms/image) faster_rcnn_r50_1x 147.488 146.124 142.416 471.547 471.631 faster_rcnn_r50_2x 147.636 147.73 141.664 471.548 472.86 faster_rcnn_r50_vd_1x 146.588 144.767 141.208 459.357 457.852 faster_rcnn_r50_fpn_1x 25.11 24.758 20.744 59.411 57.585 faster_rcnn_r50_fpn_2x 25.351 24.505 20.509 59.594 57.591 faster_rcnn_r50_vd_fpn_2x 25.514 25.292 21.097 61.026 58.377 faster_rcnn_r50_fpn_gn_2x 36.959 36.173 32.356 101.339 101.212 faster_rcnn_dcn_r50_fpn_1x 28.707 28.162 27.503 68.154 67.443 faster_rcnn_dcn_r50_vd_fpn_2x 28.576 28.271 27.512 68.959 68.448 faster_rcnn_r101_1x 153.267 150.985 144.849 490.104 486.836 faster_rcnn_r101_fpn_1x 30.949 30.331 24.021 73.591 69.736 faster_rcnn_r101_fpn_2x 30.918 29.126 23.677 73.563 70.32 faster_rcnn_r101_vd_fpn_1x 31.144 30.202 23.57 74.767 70.773 faster_rcnn_r101_vd_fpn_2x 30.678 29.969 23.327 74.882 70.842 faster_rcnn_x101_vd_64x4d_fpn_1x 60.36 58.461 45.172 132.178 131.734 faster_rcnn_x101_vd_64x4d_fpn_2x 59.003 59.163 46.065 131.422 132.186 faster_rcnn_dcn_r101_vd_fpn_1x 36.862 37.205 36.539 93.273 92.616 faster_rcnn_dcn_x101_vd_64x4d_fpn_1x 78.476 78.335 77.559 185.976 185.996 faster_rcnn_se154_vd_fpn_s1x 166.282 90.508 80.738 304.653 193.234 mask_rcnn_r50_1x 160.185 160.4 160.322 - - mask_rcnn_r50_2x 159.821 159.527 160.41 - - mask_rcnn_r50_fpn_1x 95.72 95.719 92.455 259.8 258.04 mask_rcnn_r50_fpn_2x 84.545 83.567 79.269 227.284 222.975 mask_rcnn_r50_vd_fpn_2x 82.07 82.442 77.187 223.75 221.683 mask_rcnn_r50_fpn_gn_2x 94.936 94.611 91.42 265.468 263.76 mask_rcnn_dcn_r50_fpn_1x 97.828 97.433 93.76 256.295 258.056 mask_rcnn_dcn_r50_vd_fpn_2x 77.831 79.453 76.983 205.469 204.499 mask_rcnn_r101_fpn_1x 95.543 97.929 90.314 252.997 250.782 mask_rcnn_r101_vd_fpn_1x 98.046 97.647 90.272 261.286 262.108 mask_rcnn_x101_vd_64x4d_fpn_1x 115.461 115.756 102.04 296.066 293.62 mask_rcnn_x101_vd_64x4d_fpn_2x 107.144 107.29 97.275 267.636 267.577 mask_rcnn_dcn_r101_vd_fpn_1x 85.504 84.875 84.907 225.202 226.585 mask_rcnn_dcn_x101_vd_64x4d_fpn_1x 129.937 129.934 127.804 326.786 326.161 mask_rcnn_se154_vd_fpn_s1x 214.188 139.807 121.516 440.391 439.727 cascade_rcnn_r50_fpn_1x 36.866 36.949 36.637 101.851 101.912 cascade_mask_rcnn_r50_fpn_1x 110.344 106.412 100.367 301.703 297.739 cascade_rcnn_dcn_r50_fpn_1x 40.412 39.58 39.853 110.346 110.077 cascade_mask_rcnn_r50_fpn_gn_2x 170.092 168.758 163.298 527.998 529.59 cascade_rcnn_dcn_r101_vd_fpn_1x 48.414 48.849 48.701 134.9 134.846 cascade_rcnn_dcn_x101_vd_64x4d_fpn_1x 90.062 90.218 90.009 228.67 228.396 retinanet_r101_fpn_1x 55.59 54.636 48.489 90.394 83.951 retinanet_r50_fpn_1x 50.048 47.932 44.385 73.819 70.282 retinanet_x101_vd_64x4d_fpn_1x 83.329 83.446 70.76 145.936 146.168 yolov3_darknet 21.427 20.252 13.856 55.173 55.692 yolov3_darknet_voc 17.58 16.241 9.473 51.049 51.249 yolov3_mobilenet_v1 12.869 11.834 9.408 24.887 21.352 yolov3_mobilenet_v1_voc 9.118 8.146 5.575 20.787 17.169 yolov3_r34 14.914 14.125 11.176 20.798 20.822 yolov3_r34_voc 11.288 10.73 7.7 25.874 22.399 ssd_mobilenet_v1_voc 5.763 5.854 4.589 11.75 9.485 ssd_vgg16_300 28.722 29.644 20.399 73.707 74.531 ssd_vgg16_300_voc 18.425 19.288 11.298 56.297 56.201 ssd_vgg16_512 27.471 28.328 19.328 68.685 69.808 ssd_vgg16_512_voc 18.721 19.636 12.004 54.688 56.174 RCNN\u7cfb\u5217\u6a21\u578bFluid-TensorRT\u901f\u5ea6\u76f8\u6bd4Fluid\u9884\u6d4b\u6ca1\u6709\u4f18\u52bf\uff0c\u539f\u56e0\u662f: TensorRT\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u5f53\u524d\u57fa\u4e8eResNet\u7cfb\u5217\u7684RCNN\u6a21\u578b\uff0c\u53ea\u6709backbone\u90e8\u5206\u91c7\u7528\u4e86TensorRT\u5b50\u56fe\u8ba1\u7b97\uff0c\u6bd4\u8f83\u8017\u65f6\u7684stage-5\u6ca1\u6709\u57fa\u4e8eTensorRT\u8ba1\u7b97\u3002 Fluid\u5bf9CNN\u6a21\u578b\u4e5f\u505a\u4e86\u4e00\u7cfb\u5217\u7684\u878d\u5408\u4f18\u5316\u3002\u540e\u7eedTensorRT\u7248\u672c\u5347\u7ea7\u3001\u6216\u6709\u5176\u4ed6\u4f18\u5316\u7b56\u7565\u65f6\u518d\u66f4\u65b0\u6570\u636e\u3002 YOLO v3\u7cfb\u5217\u6a21\u578b\uff0cFluid-TensorRT\u76f8\u6bd4Fluid\u9884\u6d4b\u52a0\u901f5% - 10%\u4e0d\u7b49\u3002 SSD\u548cYOLOv3\u7cfb\u5217\u6a21\u578b TensorRT-FP16\u9884\u6d4b\u901f\u5ea6\u6709\u4e00\u5b9a\u7684\u4f18\u52bf\uff0c\u52a0\u901f\u7ea620% - 40%\u4e0d\u7b49\u3002\u5177\u4f53\u5982\u4e0b\u56fe\u3002","title":"\u63a8\u7406\u901f\u5ea6"},{"location":"CACascadeRCNN/","text":"CACascade RCNN \u7b80\u4ecb CACascade RCNN\u662f\u767e\u5ea6\u89c6\u89c9\u6280\u672f\u90e8\u5728Objects365 2019 Challenge\u4e0a\u593a\u51a0\u7684\u6700\u4f73\u5355\u6a21\u578b\u4e4b\u4e00\uff0cObjects365\u662f\u5728\u901a\u7528\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u7684\u4e00\u4e2a\u5168\u65b0\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u5bf9\u81ea\u7136\u573a\u666f\u4e0d\u540c\u5bf9\u8c61\u7684\u68c0\u6d4b\u7814\u7a76\u3002Objects365\u572863\u4e07\u5f20\u56fe\u50cf\u4e0a\u6807\u6ce8\u4e86365\u4e2a\u5bf9\u8c61\u7c7b\uff0c\u8bad\u7ec3\u96c6\u4e2d\u5171\u6709\u8d85\u8fc71000\u4e07\u4e2a\u8fb9\u754c\u6846\u3002\u8fd9\u91cc\u653e\u51fa\u7684\u662fFull Track\u4efb\u52a1\u4e2d\u6700\u597d\u7684\u5355\u6a21\u578b\u4e4b\u4e00\u3002 \u65b9\u6cd5\u63cf\u8ff0 \u9488\u5bf9\u5927\u89c4\u6a21\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u7684\u7279\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7247\u5305\u542b\u7269\u4f53\u7c7b\u522b\u7684\u6570\u91cf\u7684\u91c7\u6837\u65b9\u5f0f\uff08Class Aware Sampling\uff09\u3002\u57fa\u4e8e\u8fd9\u79cd\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u4f7f\u6a21\u578b\u6536\u655b\u5230\u66f4\u597d\u7684\u6548\u679c\u3002 \u672c\u6b21\u516c\u5e03\u7684\u6700\u597d\u5355\u6a21\u578b\u662f\u4e00\u4e2a\u57fa\u4e8eCascade RCNN\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u5c06Backbone\u66ff\u6362\u4e3a\u66f4\u52a0\u5f3a\u5927\u7684SENet154\u6a21\u578b\uff0cDeformable Conv\u6a21\u5757\u4ee5\u53ca\u66f4\u590d\u6742\u4e8c\u9636\u6bb5\u7f51\u7edc\u7ed3\u6784\uff0c\u9488\u5bf9BatchSize\u6bd4\u8f83\u5c0f\u7684\u60c5\u51b5\u589e\u52a0\u4e86Group Normalization\u64cd\u4f5c\u5e76\u540c\u65f6\u4f7f\u7528\u4e86\u591a\u5c3a\u5ea6\u8bad\u7ec3\uff0c\u6700\u7ec8\u8fbe\u5230\u4e86\u975e\u5e38\u7406\u60f3\u7684\u6548\u679c\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u5148\u540e\u5206\u522b\u5728ImageNet\u548cCOCO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5176\u4e2d\u5728COCO\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u589e\u52a0\u4e86Mask\u5206\u652f\uff0c\u5176\u4f59\u7ed3\u6784\u4e0eCACascade RCNN\u76f8\u540c\uff0c \u4f1a\u5728\u542f\u52a8\u8bad\u7ec3\u65f6\u81ea\u52a8\u4e0b\u8f7d\u3002 \u4f7f\u7528\u65b9\u6cd5 1.\u51c6\u5907\u6570\u636e \u6570\u636e\u9700\u8981\u901a\u8fc7 Objects365\u5b98\u65b9\u7f51\u7ad9 \u8fdb\u884c\u7533\u8bf7\u4e0b\u8f7d\uff0c\u6570\u636e\u4e0b\u8f7d\u540e\u5c06\u6570\u636e\u653e\u7f6e\u5728dataset\u76ee\u5f55\u4e2d\u3002 ${THIS REPO ROOT} \\--dataset \\-- objects365 \\-- annotations |-- train.json |-- val.json \\-- train \\-- val 2.\u542f\u52a8\u8bad\u7ec3\u6a21\u578b python tools/train.py -c configs/obj365/cascade_rcnn_dcnv2_se154_vd_fpn_gn.yml 3.\u6a21\u578b\u9884\u6d4b\u7ed3\u679c \u6a21\u578b \u9a8c\u8bc1\u96c6 mAP \u4e0b\u8f7d\u94fe\u63a5 CACascadeRCNN SE154 31.7 model \u6a21\u578b\u6548\u679c","title":"CACascade RCNN"},{"location":"CACascadeRCNN/#cacascade-rcnn","text":"","title":"CACascade RCNN"},{"location":"CACascadeRCNN/#_1","text":"CACascade RCNN\u662f\u767e\u5ea6\u89c6\u89c9\u6280\u672f\u90e8\u5728Objects365 2019 Challenge\u4e0a\u593a\u51a0\u7684\u6700\u4f73\u5355\u6a21\u578b\u4e4b\u4e00\uff0cObjects365\u662f\u5728\u901a\u7528\u7269\u4f53\u68c0\u6d4b\u9886\u57df\u7684\u4e00\u4e2a\u5168\u65b0\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u5bf9\u81ea\u7136\u573a\u666f\u4e0d\u540c\u5bf9\u8c61\u7684\u68c0\u6d4b\u7814\u7a76\u3002Objects365\u572863\u4e07\u5f20\u56fe\u50cf\u4e0a\u6807\u6ce8\u4e86365\u4e2a\u5bf9\u8c61\u7c7b\uff0c\u8bad\u7ec3\u96c6\u4e2d\u5171\u6709\u8d85\u8fc71000\u4e07\u4e2a\u8fb9\u754c\u6846\u3002\u8fd9\u91cc\u653e\u51fa\u7684\u662fFull Track\u4efb\u52a1\u4e2d\u6700\u597d\u7684\u5355\u6a21\u578b\u4e4b\u4e00\u3002","title":"\u7b80\u4ecb"},{"location":"CACascadeRCNN/#_2","text":"\u9488\u5bf9\u5927\u89c4\u6a21\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u7684\u7279\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7247\u5305\u542b\u7269\u4f53\u7c7b\u522b\u7684\u6570\u91cf\u7684\u91c7\u6837\u65b9\u5f0f\uff08Class Aware Sampling\uff09\u3002\u57fa\u4e8e\u8fd9\u79cd\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u4f7f\u6a21\u578b\u6536\u655b\u5230\u66f4\u597d\u7684\u6548\u679c\u3002 \u672c\u6b21\u516c\u5e03\u7684\u6700\u597d\u5355\u6a21\u578b\u662f\u4e00\u4e2a\u57fa\u4e8eCascade RCNN\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u5c06Backbone\u66ff\u6362\u4e3a\u66f4\u52a0\u5f3a\u5927\u7684SENet154\u6a21\u578b\uff0cDeformable Conv\u6a21\u5757\u4ee5\u53ca\u66f4\u590d\u6742\u4e8c\u9636\u6bb5\u7f51\u7edc\u7ed3\u6784\uff0c\u9488\u5bf9BatchSize\u6bd4\u8f83\u5c0f\u7684\u60c5\u51b5\u589e\u52a0\u4e86Group Normalization\u64cd\u4f5c\u5e76\u540c\u65f6\u4f7f\u7528\u4e86\u591a\u5c3a\u5ea6\u8bad\u7ec3\uff0c\u6700\u7ec8\u8fbe\u5230\u4e86\u975e\u5e38\u7406\u60f3\u7684\u6548\u679c\u3002\u9884\u8bad\u7ec3\u6a21\u578b\u5148\u540e\u5206\u522b\u5728ImageNet\u548cCOCO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5176\u4e2d\u5728COCO\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u65f6\u589e\u52a0\u4e86Mask\u5206\u652f\uff0c\u5176\u4f59\u7ed3\u6784\u4e0eCACascade RCNN\u76f8\u540c\uff0c \u4f1a\u5728\u542f\u52a8\u8bad\u7ec3\u65f6\u81ea\u52a8\u4e0b\u8f7d\u3002","title":"\u65b9\u6cd5\u63cf\u8ff0"},{"location":"CACascadeRCNN/#_3","text":"1.\u51c6\u5907\u6570\u636e \u6570\u636e\u9700\u8981\u901a\u8fc7 Objects365\u5b98\u65b9\u7f51\u7ad9 \u8fdb\u884c\u7533\u8bf7\u4e0b\u8f7d\uff0c\u6570\u636e\u4e0b\u8f7d\u540e\u5c06\u6570\u636e\u653e\u7f6e\u5728dataset\u76ee\u5f55\u4e2d\u3002 ${THIS REPO ROOT} \\--dataset \\-- objects365 \\-- annotations |-- train.json |-- val.json \\-- train \\-- val 2.\u542f\u52a8\u8bad\u7ec3\u6a21\u578b python tools/train.py -c configs/obj365/cascade_rcnn_dcnv2_se154_vd_fpn_gn.yml 3.\u6a21\u578b\u9884\u6d4b\u7ed3\u679c \u6a21\u578b \u9a8c\u8bc1\u96c6 mAP \u4e0b\u8f7d\u94fe\u63a5 CACascadeRCNN SE154 31.7 model","title":"\u4f7f\u7528\u65b9\u6cd5"},{"location":"CACascadeRCNN/#_4","text":"","title":"\u6a21\u578b\u6548\u679c"},{"location":"CONFIG/","text":"English | \u7b80\u4f53\u4e2d\u6587 Config Pipline Introduction PaddleDetection takes a rather principled approach to configuration management. We aim to automate the configuration workflow and to reduce configuration errors. Rationale Presently, configuration in mainstream frameworks are usually dictionary based: the global config is simply a giant, loosely defined Python dictionary. This approach is error prone, e.g., misspelled or displaced keys may lead to serious errors in training process, causing time loss and wasted resources. To avoid the common pitfalls, with automation and static analysis in mind, we propose a configuration design that is user friendly, easy to maintain and extensible. Design The design utilizes some of Python's reflection mechanism to extract configuration schematics from Python class definitions. To be specific, it extracts information from class constructor arguments, including names, docstrings, default values, data types (if type hints are available). This approach advocates modular and testable design, leading to a unified and extensible code base. API Most of the functionality is exposed in ppdet.core.workspace module. register : This decorator register a class as configurable module; it understands several special annotations in the class definition. __category__ : For better organization, modules are classified into categories. __inject__ : A list of constructor arguments, which are intended to take module instances as input, module instances will be created at runtime an injected. The corresponding configuration value can be a class name string, a serialized object, a config key pointing to a serialized object, or a dict (in which case the constructor needs to handle it, see example below). __op__ : Shortcut for wrapping PaddlePaddle operators into a callable objects, together with __append_doc__ (extracting docstring from target PaddlePaddle operator automatically), this can be a real time saver. serializable : This decorator make a class directly serializable in yaml config file, by taking advantage of pyyaml 's serialization mechanism. create : Constructs a module instance according to global configuration. load_config and merge_config : Loading yaml file and merge config settings from command line. Example Take the RPNHead module for example, it is composed of several PaddlePaddle operators. We first wrap those operators into classes, then pass in instances of these classes when instantiating the RPNHead module. # excerpt from `ppdet/modeling/ops.py` from ppdet.core.workspace import register, serializable # ... more operators @register @serializable class GenerateProposals(object): # NOTE this class simply wraps a PaddlePaddle operator __op__ = fluid.layers.generate_proposals # NOTE docstring for args are extracted from PaddlePaddle OP __append_doc__ = True def __init__(self, pre_nms_top_n=6000, post_nms_top_n=1000, nms_thresh=.5, min_size=.1, eta=1.): super(GenerateProposals, self).__init__() self.pre_nms_top_n = pre_nms_top_n self.post_nms_top_n = post_nms_top_n self.nms_thresh = nms_thresh self.min_size = min_size self.eta = eta # ... more operators # excerpt from `ppdet/modeling/anchor_heads/rpn_head.py` from ppdet.core.workspace import register from ppdet.modeling.ops import AnchorGenerator, RPNTargetAssign, GenerateProposals @register class RPNHead(object): \"\"\" RPN Head Args: anchor_generator (object): `AnchorGenerator` instance rpn_target_assign (object): `RPNTargetAssign` instance train_proposal (object): `GenerateProposals` instance for training test_proposal (object): `GenerateProposals` instance for testing \"\"\" __inject__ = [ 'anchor_generator', 'rpn_target_assign', 'train_proposal', 'test_proposal' ] def __init__(self, anchor_generator=AnchorGenerator().__dict__, rpn_target_assign=RPNTargetAssign().__dict__, train_proposal=GenerateProposals(12000, 2000).__dict__, test_proposal=GenerateProposals().__dict__): super(RPNHead, self).__init__() self.anchor_generator = anchor_generator self.rpn_target_assign = rpn_target_assign self.train_proposal = train_proposal self.test_proposal = test_proposal if isinstance(anchor_generator, dict): self.anchor_generator = AnchorGenerator(**anchor_generator) if isinstance(rpn_target_assign, dict): self.rpn_target_assign = RPNTargetAssign(**rpn_target_assign) if isinstance(train_proposal, dict): self.train_proposal = GenerateProposals(**train_proposal) if isinstance(test_proposal, dict): self.test_proposal = GenerateProposals(**test_proposal) The corresponding(generated) YAML snippet is as follows, note this is the configuration in FULL , all the default values can be omitted. In case of the above example, all arguments have default value, meaning nothing is required in the config file. RPNHead: test_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 1000 pre_nms_top_n: 6000 train_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 2000 pre_nms_top_n: 12000 anchor_generator: # ... rpn_target_assign: # ... Example snippet that make use of the RPNHead module. from ppdet.core.workspace import load_config, merge_config, create load_config('some_config_file.yml') merge_config(more_config_options_from_command_line) rpn_head = create('RPNHead') # ... code that use the created module! Configuration file can also have serialized objects in it, denoted with ! , for example LearningRate: base_lr: 0.01 schedulers: - !PiecewiseDecay gamma: 0.1 milestones: [60000, 80000] - !LinearWarmup start_factor: 0.3333333333333333 steps: 500 Complete config files of multiple detection architectures are given and brief description of each parameter. Requirements Two Python packages are used, both are optional. typeguard is used for type checking in Python 3. docstring_parser is needed for docstring parsing. To install them, simply run: pip install typeguard http://github.com/willthefrog/docstring_parser/tarball/master Tooling A small utility ( tools/configure.py ) is included to simplify the configuration process, it provides 4 commands to walk users through the configuration process: list : List currently registered modules by category, one can also specify which category to list with the --category flag. help : Get help information for a module, including description, options, configuration template and example command line flags. analyze : Check configuration file for missing/extraneous options, options with mismatch type (if type hint is given) and missing dependencies, it also highlights user provided values (overridden default values). generate : Generate a configuration template for a given list of modules. By default it generates a complete configuration file, which can be quite verbose; if a --minimal flag is given, it generates a template that only contain non optional settings. For example, to generate a configuration for Faster R-CNN architecture with ResNet backbone and FPN , run: shell python tools/configure.py generate FasterRCNN ResNet RPNHead RoIAlign BBoxAssigner BBoxHead FasterRCNNTrainFeed FasterRCNNTestFeed LearningRate OptimizerBuilder For a minimal version, run: shell python tools/configure.py --minimal generate FasterRCNN BBoxHead FAQ Q: There are some configuration options that are used by multiple modules (e.g., num_classes ), how do I avoid duplication in config files? A: We provided a __shared__ annotation for exactly this purpose, simply annotate like this __shared__ = ['num_classes'] . It works as follows: if num_classes is configured for a module in config file, it takes precedence. if num_classes is not configured for a module but is present in the config file as a global key, its value will be used. otherwise, the default value ( 81 ) will be used.","title":"CONFIG"},{"location":"CONFIG/#config-pipline","text":"","title":"Config Pipline"},{"location":"CONFIG/#introduction","text":"PaddleDetection takes a rather principled approach to configuration management. We aim to automate the configuration workflow and to reduce configuration errors.","title":"Introduction"},{"location":"CONFIG/#rationale","text":"Presently, configuration in mainstream frameworks are usually dictionary based: the global config is simply a giant, loosely defined Python dictionary. This approach is error prone, e.g., misspelled or displaced keys may lead to serious errors in training process, causing time loss and wasted resources. To avoid the common pitfalls, with automation and static analysis in mind, we propose a configuration design that is user friendly, easy to maintain and extensible.","title":"Rationale"},{"location":"CONFIG/#design","text":"The design utilizes some of Python's reflection mechanism to extract configuration schematics from Python class definitions. To be specific, it extracts information from class constructor arguments, including names, docstrings, default values, data types (if type hints are available). This approach advocates modular and testable design, leading to a unified and extensible code base.","title":"Design"},{"location":"CONFIG/#api","text":"Most of the functionality is exposed in ppdet.core.workspace module. register : This decorator register a class as configurable module; it understands several special annotations in the class definition. __category__ : For better organization, modules are classified into categories. __inject__ : A list of constructor arguments, which are intended to take module instances as input, module instances will be created at runtime an injected. The corresponding configuration value can be a class name string, a serialized object, a config key pointing to a serialized object, or a dict (in which case the constructor needs to handle it, see example below). __op__ : Shortcut for wrapping PaddlePaddle operators into a callable objects, together with __append_doc__ (extracting docstring from target PaddlePaddle operator automatically), this can be a real time saver. serializable : This decorator make a class directly serializable in yaml config file, by taking advantage of pyyaml 's serialization mechanism. create : Constructs a module instance according to global configuration. load_config and merge_config : Loading yaml file and merge config settings from command line.","title":"API"},{"location":"CONFIG/#example","text":"Take the RPNHead module for example, it is composed of several PaddlePaddle operators. We first wrap those operators into classes, then pass in instances of these classes when instantiating the RPNHead module. # excerpt from `ppdet/modeling/ops.py` from ppdet.core.workspace import register, serializable # ... more operators @register @serializable class GenerateProposals(object): # NOTE this class simply wraps a PaddlePaddle operator __op__ = fluid.layers.generate_proposals # NOTE docstring for args are extracted from PaddlePaddle OP __append_doc__ = True def __init__(self, pre_nms_top_n=6000, post_nms_top_n=1000, nms_thresh=.5, min_size=.1, eta=1.): super(GenerateProposals, self).__init__() self.pre_nms_top_n = pre_nms_top_n self.post_nms_top_n = post_nms_top_n self.nms_thresh = nms_thresh self.min_size = min_size self.eta = eta # ... more operators # excerpt from `ppdet/modeling/anchor_heads/rpn_head.py` from ppdet.core.workspace import register from ppdet.modeling.ops import AnchorGenerator, RPNTargetAssign, GenerateProposals @register class RPNHead(object): \"\"\" RPN Head Args: anchor_generator (object): `AnchorGenerator` instance rpn_target_assign (object): `RPNTargetAssign` instance train_proposal (object): `GenerateProposals` instance for training test_proposal (object): `GenerateProposals` instance for testing \"\"\" __inject__ = [ 'anchor_generator', 'rpn_target_assign', 'train_proposal', 'test_proposal' ] def __init__(self, anchor_generator=AnchorGenerator().__dict__, rpn_target_assign=RPNTargetAssign().__dict__, train_proposal=GenerateProposals(12000, 2000).__dict__, test_proposal=GenerateProposals().__dict__): super(RPNHead, self).__init__() self.anchor_generator = anchor_generator self.rpn_target_assign = rpn_target_assign self.train_proposal = train_proposal self.test_proposal = test_proposal if isinstance(anchor_generator, dict): self.anchor_generator = AnchorGenerator(**anchor_generator) if isinstance(rpn_target_assign, dict): self.rpn_target_assign = RPNTargetAssign(**rpn_target_assign) if isinstance(train_proposal, dict): self.train_proposal = GenerateProposals(**train_proposal) if isinstance(test_proposal, dict): self.test_proposal = GenerateProposals(**test_proposal) The corresponding(generated) YAML snippet is as follows, note this is the configuration in FULL , all the default values can be omitted. In case of the above example, all arguments have default value, meaning nothing is required in the config file. RPNHead: test_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 1000 pre_nms_top_n: 6000 train_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 2000 pre_nms_top_n: 12000 anchor_generator: # ... rpn_target_assign: # ... Example snippet that make use of the RPNHead module. from ppdet.core.workspace import load_config, merge_config, create load_config('some_config_file.yml') merge_config(more_config_options_from_command_line) rpn_head = create('RPNHead') # ... code that use the created module! Configuration file can also have serialized objects in it, denoted with ! , for example LearningRate: base_lr: 0.01 schedulers: - !PiecewiseDecay gamma: 0.1 milestones: [60000, 80000] - !LinearWarmup start_factor: 0.3333333333333333 steps: 500 Complete config files of multiple detection architectures are given and brief description of each parameter.","title":"Example"},{"location":"CONFIG/#requirements","text":"Two Python packages are used, both are optional. typeguard is used for type checking in Python 3. docstring_parser is needed for docstring parsing. To install them, simply run: pip install typeguard http://github.com/willthefrog/docstring_parser/tarball/master","title":"Requirements"},{"location":"CONFIG/#tooling","text":"A small utility ( tools/configure.py ) is included to simplify the configuration process, it provides 4 commands to walk users through the configuration process: list : List currently registered modules by category, one can also specify which category to list with the --category flag. help : Get help information for a module, including description, options, configuration template and example command line flags. analyze : Check configuration file for missing/extraneous options, options with mismatch type (if type hint is given) and missing dependencies, it also highlights user provided values (overridden default values). generate : Generate a configuration template for a given list of modules. By default it generates a complete configuration file, which can be quite verbose; if a --minimal flag is given, it generates a template that only contain non optional settings. For example, to generate a configuration for Faster R-CNN architecture with ResNet backbone and FPN , run: shell python tools/configure.py generate FasterRCNN ResNet RPNHead RoIAlign BBoxAssigner BBoxHead FasterRCNNTrainFeed FasterRCNNTestFeed LearningRate OptimizerBuilder For a minimal version, run: shell python tools/configure.py --minimal generate FasterRCNN BBoxHead","title":"Tooling"},{"location":"CONFIG/#faq","text":"Q: There are some configuration options that are used by multiple modules (e.g., num_classes ), how do I avoid duplication in config files? A: We provided a __shared__ annotation for exactly this purpose, simply annotate like this __shared__ = ['num_classes'] . It works as follows: if num_classes is configured for a module in config file, it takes precedence. if num_classes is not configured for a module but is present in the config file as a global key, its value will be used. otherwise, the default value ( 81 ) will be used.","title":"FAQ"},{"location":"CONFIG_cn/","text":"\u914d\u7f6e\u6a21\u5757 \u7b80\u4ecb \u4e3a\u4e86\u4f7f\u914d\u7f6e\u8fc7\u7a0b\u66f4\u52a0\u81ea\u52a8\u5316\u5e76\u51cf\u5c11\u914d\u7f6e\u9519\u8bef\uff0cPaddleDetection\u7684\u914d\u7f6e\u7ba1\u7406\u91c7\u53d6\u4e86\u8f83\u4e3a\u4e25\u8c28\u7684\u8bbe\u8ba1\u3002 \u8bbe\u8ba1\u601d\u60f3 \u76ee\u524d\u4e3b\u6d41\u6846\u67b6\u5168\u5c40\u914d\u7f6e\u57fa\u672c\u662f\u4e00\u4e2aPython dict\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5bf9\u914d\u7f6e\u7684\u68c0\u67e5\u5e76\u4e0d\u4e25\u683c\uff0c\u62fc\u5199\u9519\u8bef\u6216\u8005\u9057\u6f0f\u7684\u914d\u7f6e\u9879\u5f80\u5f80\u4f1a\u9020\u6210\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e25\u91cd\u9519\u8bef\uff0c\u8fdb\u800c\u9020\u6210\u65f6\u95f4\u53ca\u8d44\u6e90\u7684\u6d6a\u8d39\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u4e9b\u9677\u9631\uff0c\u4ece\u81ea\u52a8\u5316\u548c\u9759\u6001\u5206\u6790\u7684\u539f\u5219\u51fa\u53d1\uff0cPaddleDetection\u91c7\u7528\u4e86\u4e00\u79cd\u7528\u6237\u53cb\u597d\u3001 \u6613\u4e8e\u7ef4\u62a4\u548c\u6269\u5c55\u7684\u914d\u7f6e\u8bbe\u8ba1\u3002 \u57fa\u672c\u8bbe\u8ba1 \u5229\u7528Python\u7684\u53cd\u5c04\u673a\u5236\uff0cPaddleDection\u7684\u914d\u7f6e\u7cfb\u7edf\u4ecePython\u7c7b\u7684\u6784\u9020\u51fd\u6570\u62bd\u53d6\u591a\u79cd\u4fe1\u606f - \u5982\u53c2\u6570\u540d\u3001\u521d\u59cb\u503c\u3001\u53c2\u6570\u6ce8\u91ca\u3001\u6570\u636e\u7c7b\u578b\uff08\u5982\u679c\u7ed9\u51fatype hint\uff09- \u6765\u4f5c\u4e3a\u914d\u7f6e\u89c4\u5219\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4fbf\u4e8e\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\uff0c\u63d0\u5347\u53ef\u6d4b\u8bd5\u6027\u53ca\u6269\u5c55\u6027\u3002 API \u914d\u7f6e\u7cfb\u7edf\u7684\u5927\u591a\u6570\u529f\u80fd\u7531 ppdet.core.workspace \u6a21\u5757\u63d0\u4f9b register : \u88c5\u9970\u5668\uff0c\u5c06\u7c7b\u6ce8\u518c\u4e3a\u53ef\u914d\u7f6e\u6a21\u5757\uff1b\u80fd\u591f\u8bc6\u522b\u7c7b\u5b9a\u4e49\u4e2d\u7684\u4e00\u4e9b\u7279\u6b8a\u6807\u6ce8\u3002 __category__ : \u4e3a\u4fbf\u4e8e\u7ec4\u7ec7\uff0c\u6a21\u5757\u53ef\u4ee5\u5206\u4e3a\u4e0d\u540c\u7c7b\u522b\u3002 __inject__ : \u5982\u679c\u6a21\u5757\u7531\u591a\u4e2a\u5b50\u6a21\u5757\u7ec4\u6210\uff0c\u53ef\u4ee5\u8fd9\u4e9b\u5b50\u6a21\u5757\u5b9e\u4f8b\u4f5c\u4e3a\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u6ce8\u5165\u3002\u5bf9\u5e94\u7684\u9ed8\u8ba4\u503c\u53ca\u914d\u7f6e\u9879\u53ef\u4ee5\u662f\u7c7b\u540d\u5b57\u7b26\u4e32\uff0cyaml\u5e8f\u5217\u5316\u7684\u5bf9\u8c61\uff0c\u6307\u5411\u5e8f\u5217\u5316\u5bf9\u8c61\u7684\u914d\u7f6e\u952e\u503c\u6216\u8005Python dict\uff08\u6784\u9020\u51fd\u6570\u9700\u8981\u5bf9\u5176\u4f5c\u51fa\u5904\u7406\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\u4f8b\u5b50\uff09\u3002 __op__ : \u914d\u5408 __append_doc__ \uff08\u62bd\u53d6\u76ee\u6807OP\u7684 \u6ce8\u91ca\uff09\u4f7f\u7528\uff0c\u53ef\u4ee5\u65b9\u4fbf\u5feb\u901f\u7684\u5c01\u88c5PaddlePaddle\u5e95\u5c42OP\u3002 serializable : \u88c5\u9970\u5668\uff0c\u5229\u7528 pyyaml \u7684\u5e8f\u5217\u5316\u673a\u5236\uff0c\u53ef\u4ee5\u76f4\u63a5\u5c06\u4e00\u4e2a\u7c7b\u5b9e\u4f8b\u5e8f\u5217\u5316\u53ca\u53cd\u5e8f\u5217\u5316\u3002 create : \u6839\u636e\u5168\u5c40\u914d\u7f6e\u6784\u9020\u4e00\u4e2a\u6a21\u5757\u5b9e\u4f8b\u3002 load_config and merge_config : \u52a0\u8f7dyaml\u6587\u4ef6\uff0c\u5408\u5e76\u547d\u4ee4\u884c\u63d0\u4f9b\u7684\u914d\u7f6e\u9879\u3002 \u793a\u4f8b \u4ee5 RPNHead \u6a21\u5757\u4e3a\u4f8b\uff0c\u8be5\u6a21\u5757\u5305\u542b\u591a\u4e2aPaddlePaddle OP\uff0c\u5148\u5c06\u8fd9\u4e9bOP\u5c01\u88c5\u6210\u7c7b\uff0c\u5e76\u5c06\u5176\u5b9e\u4f8b\u5728\u6784\u9020 RPNHead \u65f6\u6ce8\u5165\u3002 # excerpt from `ppdet/modeling/ops.py` from ppdet.core.workspace import register, serializable # ... more operators @register @serializable class GenerateProposals(object): # NOTE this class simply wraps a PaddlePaddle operator __op__ = fluid.layers.generate_proposals # NOTE docstring for args are extracted from PaddlePaddle OP __append_doc__ = True def __init__(self, pre_nms_top_n=6000, post_nms_top_n=1000, nms_thresh=.5, min_size=.1, eta=1.): super(GenerateProposals, self).__init__() self.pre_nms_top_n = pre_nms_top_n self.post_nms_top_n = post_nms_top_n self.nms_thresh = nms_thresh self.min_size = min_size self.eta = eta # ... more operators # excerpt from `ppdet/modeling/anchor_heads/rpn_head.py` from ppdet.core.workspace import register from ppdet.modeling.ops import AnchorGenerator, RPNTargetAssign, GenerateProposals @register class RPNHead(object): \"\"\" RPN Head Args: anchor_generator (object): `AnchorGenerator` instance rpn_target_assign (object): `RPNTargetAssign` instance train_proposal (object): `GenerateProposals` instance for training test_proposal (object): `GenerateProposals` instance for testing \"\"\" __inject__ = [ 'anchor_generator', 'rpn_target_assign', 'train_proposal', 'test_proposal' ] def __init__(self, anchor_generator=AnchorGenerator().__dict__, rpn_target_assign=RPNTargetAssign().__dict__, train_proposal=GenerateProposals(12000, 2000).__dict__, test_proposal=GenerateProposals().__dict__): super(RPNHead, self).__init__() self.anchor_generator = anchor_generator self.rpn_target_assign = rpn_target_assign self.train_proposal = train_proposal self.test_proposal = test_proposal if isinstance(anchor_generator, dict): self.anchor_generator = AnchorGenerator(**anchor_generator) if isinstance(rpn_target_assign, dict): self.rpn_target_assign = RPNTargetAssign(**rpn_target_assign) if isinstance(train_proposal, dict): self.train_proposal = GenerateProposals(**train_proposal) if isinstance(test_proposal, dict): self.test_proposal = GenerateProposals(**test_proposal) \u5bf9\u5e94\u7684yaml\u914d\u7f6e\u5982\u4e0b\uff0c\u8bf7\u6ce8\u610f\u8fd9\u91cc\u7ed9\u51fa\u7684\u662f \u5b8c\u6574 \u914d\u7f6e\uff0c\u5176\u4e2d\u6240\u6709\u9ed8\u8ba4\u503c\u914d\u7f6e\u9879\u90fd\u53ef\u4ee5\u7701\u7565\u3002\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\u7684\u6a21\u5757\u6240\u6709\u7684\u6784\u9020\u51fd\u6570\u53c2\u6570\u90fd\u63d0\u4f9b\u4e86\u9ed8\u8ba4\u503c\uff0c\u56e0\u6b64\u914d\u7f6e\u6587\u4ef6\u4e2d\u53ef\u4ee5\u5b8c\u5168\u7565\u8fc7\u5176\u914d\u7f6e\u3002 RPNHead: test_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 1000 pre_nms_top_n: 6000 train_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 2000 pre_nms_top_n: 12000 anchor_generator: # ... rpn_target_assign: # ... RPNHead \u6a21\u5757\u5b9e\u9645\u4f7f\u7528\u4ee3\u7801\u793a\u4f8b\u3002 from ppdet.core.workspace import load_config, merge_config, create load_config('some_config_file.yml') merge_config(more_config_options_from_command_line) rpn_head = create('RPNHead') # ... code that use the created module! \u914d\u7f6e\u6587\u4ef6\u7528\u53ef\u4ee5\u76f4\u63a5\u5e8f\u5217\u5316\u6a21\u5757\u5b9e\u4f8b\uff0c\u7528 ! \u6807\u793a\uff0c\u5982 LearningRate: base_lr: 0.01 schedulers: - !PiecewiseDecay gamma: 0.1 milestones: [60000, 80000] - !LinearWarmup start_factor: 0.3333333333333333 steps: 500 \u793a\u4f8b\u914d\u7f6e\u6587\u4ef6 \u4e2d\u7ed9\u51fa\u4e86\u591a\u79cd\u68c0\u6d4b\u7ed3\u6784\u7684\u5b8c\u6574\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u53ca\u5176\u4e2d\u5404\u4e2a\u8d85\u53c2\u7684\u7b80\u8981\u8bf4\u660e\u3002 \u5b89\u88c5\u4f9d\u8d56 \u914d\u7f6e\u7cfb\u7edf\u7528\u5230\u4e24\u4e2aPython\u5305\uff0c\u5747\u4e3a\u53ef\u9009\u5b89\u88c5\u3002 typeguard \u5728Python 3\u4e2d\u7528\u6765\u8fdb\u884c\u6570\u636e\u7c7b\u578b\u9a8c\u8bc1\u3002 docstring_parser \u7528\u6765\u89e3\u6790\u6ce8\u91ca\u3002 \u5982\u9700\u5b89\u88c5\uff0c\u8fd0\u884c\u4e0b\u9762\u547d\u4ee4\u5373\u53ef\u3002 pip install typeguard http://github.com/willthefrog/docstring_parser/tarball/master \u76f8\u5173\u5de5\u5177 \u4e3a\u4e86\u65b9\u4fbf\u7528\u6237\u914d\u7f6e\uff0cPaddleDection\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de5\u5177 ( tools/configure.py )\uff0c \u5171\u652f\u6301\u56db\u4e2a\u5b50\u547d\u4ee4\uff1a list : \u5217\u51fa\u5f53\u524d\u5df2\u6ce8\u518c\u7684\u6a21\u5757\uff0c\u5982\u9700\u5217\u51fa\u5177\u4f53\u7c7b\u522b\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u4f7f\u7528 --category \u6307\u5b9a\u3002 help : \u663e\u793a\u6307\u5b9a\u6a21\u5757\u7684\u5e2e\u52a9\u4fe1\u606f\uff0c\u5982\u63cf\u8ff0\uff0c\u914d\u7f6e\u9879\uff0c\u914d\u7f6e\u6587\u4ef6\u6a21\u677f\u53ca\u547d\u4ee4\u884c\u793a\u4f8b\u3002 analyze : \u68c0\u67e5\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u7f3a\u5c11\u6216\u8005\u591a\u4f59\u7684\u914d\u7f6e\u9879\u4ee5\u53ca\u4f9d\u8d56\u7f3a\u5931\uff0c\u5982\u679c\u7ed9\u51fatype hint\uff0c \u8fd8\u53ef\u4ee5\u68c0\u67e5\u914d\u7f6e\u9879\u4e2d\u9519\u8bef\u7684\u6570\u636e\u7c7b\u578b\u3002\u975e\u9ed8\u8ba4\u914d\u7f6e\u4e5f\u4f1a\u9ad8\u4eae\u663e\u793a\u3002 generate : \u6839\u636e\u7ed9\u51fa\u7684\u6a21\u5757\u5217\u8868\u751f\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u9ed8\u8ba4\u751f\u6210\u5b8c\u6574\u914d\u7f6e\uff0c\u5982\u679c\u6307\u5b9a --minimal \uff0c\u751f\u6210\u6700\u5c0f\u914d\u7f6e\uff0c\u5373\u7701\u7565\u6240\u6709\u9ed8\u8ba4\u914d\u7f6e\u9879\u3002\u4f8b\u5982\uff0c\u6267\u884c\u4e0b\u5217\u547d\u4ee4\u53ef\u4ee5\u751f\u6210Faster R-CNN ( ResNet backbone + FPN ) \u67b6\u6784\u7684\u914d\u7f6e\u6587\u4ef6: shell python tools/configure.py generate FasterRCNN ResNet RPNHead RoIAlign BBoxAssigner BBoxHead FasterRCNNTrainFeed FasterRCNNTestFeed LearningRate OptimizerBuilder \u5982\u9700\u6700\u5c0f\u914d\u7f6e\uff0c\u8fd0\u884c\uff1a shell python tools/configure.py --minimal generate FasterRCNN BBoxHead FAQ Q: \u67d0\u4e9b\u914d\u7f6e\u9879\u4f1a\u5728\u591a\u4e2a\u6a21\u5757\u4e2d\u7528\u5230(\u5982 num_classes )\uff0c\u5982\u4f55\u907f\u514d\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u591a\u6b21\u91cd\u590d\u8bbe\u7f6e\uff1f A: \u6846\u67b6\u63d0\u4f9b\u4e86 __shared__ \u6807\u8bb0\u6765\u5b9e\u73b0\u914d\u7f6e\u7684\u5171\u4eab\uff0c\u7528\u6237\u53ef\u4ee5\u6807\u8bb0\u53c2\u6570\uff0c\u5982 __shared__ = ['num_classes'] \uff0c\u914d\u7f6e\u6570\u503c\u4f5c\u7528\u89c4\u5219\u5982\u4e0b\uff1a \u5982\u679c\u6a21\u5757\u914d\u7f6e\u4e2d\u63d0\u4f9b\u4e86 num_classes \uff0c\u4f1a\u4f18\u5148\u4f7f\u7528\u5176\u6570\u503c\u3002 \u5982\u679c\u6a21\u5757\u914d\u7f6e\u4e2d\u672a\u63d0\u4f9b num_classes \uff0c\u4f46\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b58\u5728\u5168\u5c40\u952e\u503c\uff0c\u90a3\u4e48\u4f1a\u4f7f\u7528\u5168\u5c40\u952e\u503c\u3002 \u4e24\u8005\u5747\u4e3a\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u4f7f\u7528\u9ed8\u8ba4\u503c( 81 )\u3002","title":"\u914d\u7f6e\u6a21\u5757"},{"location":"CONFIG_cn/#_1","text":"","title":"\u914d\u7f6e\u6a21\u5757"},{"location":"CONFIG_cn/#_2","text":"\u4e3a\u4e86\u4f7f\u914d\u7f6e\u8fc7\u7a0b\u66f4\u52a0\u81ea\u52a8\u5316\u5e76\u51cf\u5c11\u914d\u7f6e\u9519\u8bef\uff0cPaddleDetection\u7684\u914d\u7f6e\u7ba1\u7406\u91c7\u53d6\u4e86\u8f83\u4e3a\u4e25\u8c28\u7684\u8bbe\u8ba1\u3002","title":"\u7b80\u4ecb"},{"location":"CONFIG_cn/#_3","text":"\u76ee\u524d\u4e3b\u6d41\u6846\u67b6\u5168\u5c40\u914d\u7f6e\u57fa\u672c\u662f\u4e00\u4e2aPython dict\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u5bf9\u914d\u7f6e\u7684\u68c0\u67e5\u5e76\u4e0d\u4e25\u683c\uff0c\u62fc\u5199\u9519\u8bef\u6216\u8005\u9057\u6f0f\u7684\u914d\u7f6e\u9879\u5f80\u5f80\u4f1a\u9020\u6210\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e25\u91cd\u9519\u8bef\uff0c\u8fdb\u800c\u9020\u6210\u65f6\u95f4\u53ca\u8d44\u6e90\u7684\u6d6a\u8d39\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u4e9b\u9677\u9631\uff0c\u4ece\u81ea\u52a8\u5316\u548c\u9759\u6001\u5206\u6790\u7684\u539f\u5219\u51fa\u53d1\uff0cPaddleDetection\u91c7\u7528\u4e86\u4e00\u79cd\u7528\u6237\u53cb\u597d\u3001 \u6613\u4e8e\u7ef4\u62a4\u548c\u6269\u5c55\u7684\u914d\u7f6e\u8bbe\u8ba1\u3002","title":"\u8bbe\u8ba1\u601d\u60f3"},{"location":"CONFIG_cn/#_4","text":"\u5229\u7528Python\u7684\u53cd\u5c04\u673a\u5236\uff0cPaddleDection\u7684\u914d\u7f6e\u7cfb\u7edf\u4ecePython\u7c7b\u7684\u6784\u9020\u51fd\u6570\u62bd\u53d6\u591a\u79cd\u4fe1\u606f - \u5982\u53c2\u6570\u540d\u3001\u521d\u59cb\u503c\u3001\u53c2\u6570\u6ce8\u91ca\u3001\u6570\u636e\u7c7b\u578b\uff08\u5982\u679c\u7ed9\u51fatype hint\uff09- \u6765\u4f5c\u4e3a\u914d\u7f6e\u89c4\u5219\u3002 \u8fd9\u79cd\u8bbe\u8ba1\u4fbf\u4e8e\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\uff0c\u63d0\u5347\u53ef\u6d4b\u8bd5\u6027\u53ca\u6269\u5c55\u6027\u3002","title":"\u57fa\u672c\u8bbe\u8ba1"},{"location":"CONFIG_cn/#api","text":"\u914d\u7f6e\u7cfb\u7edf\u7684\u5927\u591a\u6570\u529f\u80fd\u7531 ppdet.core.workspace \u6a21\u5757\u63d0\u4f9b register : \u88c5\u9970\u5668\uff0c\u5c06\u7c7b\u6ce8\u518c\u4e3a\u53ef\u914d\u7f6e\u6a21\u5757\uff1b\u80fd\u591f\u8bc6\u522b\u7c7b\u5b9a\u4e49\u4e2d\u7684\u4e00\u4e9b\u7279\u6b8a\u6807\u6ce8\u3002 __category__ : \u4e3a\u4fbf\u4e8e\u7ec4\u7ec7\uff0c\u6a21\u5757\u53ef\u4ee5\u5206\u4e3a\u4e0d\u540c\u7c7b\u522b\u3002 __inject__ : \u5982\u679c\u6a21\u5757\u7531\u591a\u4e2a\u5b50\u6a21\u5757\u7ec4\u6210\uff0c\u53ef\u4ee5\u8fd9\u4e9b\u5b50\u6a21\u5757\u5b9e\u4f8b\u4f5c\u4e3a\u6784\u9020\u51fd\u6570\u7684\u53c2\u6570\u6ce8\u5165\u3002\u5bf9\u5e94\u7684\u9ed8\u8ba4\u503c\u53ca\u914d\u7f6e\u9879\u53ef\u4ee5\u662f\u7c7b\u540d\u5b57\u7b26\u4e32\uff0cyaml\u5e8f\u5217\u5316\u7684\u5bf9\u8c61\uff0c\u6307\u5411\u5e8f\u5217\u5316\u5bf9\u8c61\u7684\u914d\u7f6e\u952e\u503c\u6216\u8005Python dict\uff08\u6784\u9020\u51fd\u6570\u9700\u8981\u5bf9\u5176\u4f5c\u51fa\u5904\u7406\uff0c\u53c2\u89c1\u4e0b\u9762\u7684\u4f8b\u5b50\uff09\u3002 __op__ : \u914d\u5408 __append_doc__ \uff08\u62bd\u53d6\u76ee\u6807OP\u7684 \u6ce8\u91ca\uff09\u4f7f\u7528\uff0c\u53ef\u4ee5\u65b9\u4fbf\u5feb\u901f\u7684\u5c01\u88c5PaddlePaddle\u5e95\u5c42OP\u3002 serializable : \u88c5\u9970\u5668\uff0c\u5229\u7528 pyyaml \u7684\u5e8f\u5217\u5316\u673a\u5236\uff0c\u53ef\u4ee5\u76f4\u63a5\u5c06\u4e00\u4e2a\u7c7b\u5b9e\u4f8b\u5e8f\u5217\u5316\u53ca\u53cd\u5e8f\u5217\u5316\u3002 create : \u6839\u636e\u5168\u5c40\u914d\u7f6e\u6784\u9020\u4e00\u4e2a\u6a21\u5757\u5b9e\u4f8b\u3002 load_config and merge_config : \u52a0\u8f7dyaml\u6587\u4ef6\uff0c\u5408\u5e76\u547d\u4ee4\u884c\u63d0\u4f9b\u7684\u914d\u7f6e\u9879\u3002","title":"API"},{"location":"CONFIG_cn/#_5","text":"\u4ee5 RPNHead \u6a21\u5757\u4e3a\u4f8b\uff0c\u8be5\u6a21\u5757\u5305\u542b\u591a\u4e2aPaddlePaddle OP\uff0c\u5148\u5c06\u8fd9\u4e9bOP\u5c01\u88c5\u6210\u7c7b\uff0c\u5e76\u5c06\u5176\u5b9e\u4f8b\u5728\u6784\u9020 RPNHead \u65f6\u6ce8\u5165\u3002 # excerpt from `ppdet/modeling/ops.py` from ppdet.core.workspace import register, serializable # ... more operators @register @serializable class GenerateProposals(object): # NOTE this class simply wraps a PaddlePaddle operator __op__ = fluid.layers.generate_proposals # NOTE docstring for args are extracted from PaddlePaddle OP __append_doc__ = True def __init__(self, pre_nms_top_n=6000, post_nms_top_n=1000, nms_thresh=.5, min_size=.1, eta=1.): super(GenerateProposals, self).__init__() self.pre_nms_top_n = pre_nms_top_n self.post_nms_top_n = post_nms_top_n self.nms_thresh = nms_thresh self.min_size = min_size self.eta = eta # ... more operators # excerpt from `ppdet/modeling/anchor_heads/rpn_head.py` from ppdet.core.workspace import register from ppdet.modeling.ops import AnchorGenerator, RPNTargetAssign, GenerateProposals @register class RPNHead(object): \"\"\" RPN Head Args: anchor_generator (object): `AnchorGenerator` instance rpn_target_assign (object): `RPNTargetAssign` instance train_proposal (object): `GenerateProposals` instance for training test_proposal (object): `GenerateProposals` instance for testing \"\"\" __inject__ = [ 'anchor_generator', 'rpn_target_assign', 'train_proposal', 'test_proposal' ] def __init__(self, anchor_generator=AnchorGenerator().__dict__, rpn_target_assign=RPNTargetAssign().__dict__, train_proposal=GenerateProposals(12000, 2000).__dict__, test_proposal=GenerateProposals().__dict__): super(RPNHead, self).__init__() self.anchor_generator = anchor_generator self.rpn_target_assign = rpn_target_assign self.train_proposal = train_proposal self.test_proposal = test_proposal if isinstance(anchor_generator, dict): self.anchor_generator = AnchorGenerator(**anchor_generator) if isinstance(rpn_target_assign, dict): self.rpn_target_assign = RPNTargetAssign(**rpn_target_assign) if isinstance(train_proposal, dict): self.train_proposal = GenerateProposals(**train_proposal) if isinstance(test_proposal, dict): self.test_proposal = GenerateProposals(**test_proposal) \u5bf9\u5e94\u7684yaml\u914d\u7f6e\u5982\u4e0b\uff0c\u8bf7\u6ce8\u610f\u8fd9\u91cc\u7ed9\u51fa\u7684\u662f \u5b8c\u6574 \u914d\u7f6e\uff0c\u5176\u4e2d\u6240\u6709\u9ed8\u8ba4\u503c\u914d\u7f6e\u9879\u90fd\u53ef\u4ee5\u7701\u7565\u3002\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\u7684\u6a21\u5757\u6240\u6709\u7684\u6784\u9020\u51fd\u6570\u53c2\u6570\u90fd\u63d0\u4f9b\u4e86\u9ed8\u8ba4\u503c\uff0c\u56e0\u6b64\u914d\u7f6e\u6587\u4ef6\u4e2d\u53ef\u4ee5\u5b8c\u5168\u7565\u8fc7\u5176\u914d\u7f6e\u3002 RPNHead: test_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 1000 pre_nms_top_n: 6000 train_proposal: eta: 1.0 min_size: 0.1 nms_thresh: 0.5 post_nms_top_n: 2000 pre_nms_top_n: 12000 anchor_generator: # ... rpn_target_assign: # ... RPNHead \u6a21\u5757\u5b9e\u9645\u4f7f\u7528\u4ee3\u7801\u793a\u4f8b\u3002 from ppdet.core.workspace import load_config, merge_config, create load_config('some_config_file.yml') merge_config(more_config_options_from_command_line) rpn_head = create('RPNHead') # ... code that use the created module! \u914d\u7f6e\u6587\u4ef6\u7528\u53ef\u4ee5\u76f4\u63a5\u5e8f\u5217\u5316\u6a21\u5757\u5b9e\u4f8b\uff0c\u7528 ! \u6807\u793a\uff0c\u5982 LearningRate: base_lr: 0.01 schedulers: - !PiecewiseDecay gamma: 0.1 milestones: [60000, 80000] - !LinearWarmup start_factor: 0.3333333333333333 steps: 500 \u793a\u4f8b\u914d\u7f6e\u6587\u4ef6 \u4e2d\u7ed9\u51fa\u4e86\u591a\u79cd\u68c0\u6d4b\u7ed3\u6784\u7684\u5b8c\u6574\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u53ca\u5176\u4e2d\u5404\u4e2a\u8d85\u53c2\u7684\u7b80\u8981\u8bf4\u660e\u3002","title":"\u793a\u4f8b"},{"location":"CONFIG_cn/#_6","text":"\u914d\u7f6e\u7cfb\u7edf\u7528\u5230\u4e24\u4e2aPython\u5305\uff0c\u5747\u4e3a\u53ef\u9009\u5b89\u88c5\u3002 typeguard \u5728Python 3\u4e2d\u7528\u6765\u8fdb\u884c\u6570\u636e\u7c7b\u578b\u9a8c\u8bc1\u3002 docstring_parser \u7528\u6765\u89e3\u6790\u6ce8\u91ca\u3002 \u5982\u9700\u5b89\u88c5\uff0c\u8fd0\u884c\u4e0b\u9762\u547d\u4ee4\u5373\u53ef\u3002 pip install typeguard http://github.com/willthefrog/docstring_parser/tarball/master","title":"\u5b89\u88c5\u4f9d\u8d56"},{"location":"CONFIG_cn/#_7","text":"\u4e3a\u4e86\u65b9\u4fbf\u7528\u6237\u914d\u7f6e\uff0cPaddleDection\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de5\u5177 ( tools/configure.py )\uff0c \u5171\u652f\u6301\u56db\u4e2a\u5b50\u547d\u4ee4\uff1a list : \u5217\u51fa\u5f53\u524d\u5df2\u6ce8\u518c\u7684\u6a21\u5757\uff0c\u5982\u9700\u5217\u51fa\u5177\u4f53\u7c7b\u522b\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u4f7f\u7528 --category \u6307\u5b9a\u3002 help : \u663e\u793a\u6307\u5b9a\u6a21\u5757\u7684\u5e2e\u52a9\u4fe1\u606f\uff0c\u5982\u63cf\u8ff0\uff0c\u914d\u7f6e\u9879\uff0c\u914d\u7f6e\u6587\u4ef6\u6a21\u677f\u53ca\u547d\u4ee4\u884c\u793a\u4f8b\u3002 analyze : \u68c0\u67e5\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u7f3a\u5c11\u6216\u8005\u591a\u4f59\u7684\u914d\u7f6e\u9879\u4ee5\u53ca\u4f9d\u8d56\u7f3a\u5931\uff0c\u5982\u679c\u7ed9\u51fatype hint\uff0c \u8fd8\u53ef\u4ee5\u68c0\u67e5\u914d\u7f6e\u9879\u4e2d\u9519\u8bef\u7684\u6570\u636e\u7c7b\u578b\u3002\u975e\u9ed8\u8ba4\u914d\u7f6e\u4e5f\u4f1a\u9ad8\u4eae\u663e\u793a\u3002 generate : \u6839\u636e\u7ed9\u51fa\u7684\u6a21\u5757\u5217\u8868\u751f\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u9ed8\u8ba4\u751f\u6210\u5b8c\u6574\u914d\u7f6e\uff0c\u5982\u679c\u6307\u5b9a --minimal \uff0c\u751f\u6210\u6700\u5c0f\u914d\u7f6e\uff0c\u5373\u7701\u7565\u6240\u6709\u9ed8\u8ba4\u914d\u7f6e\u9879\u3002\u4f8b\u5982\uff0c\u6267\u884c\u4e0b\u5217\u547d\u4ee4\u53ef\u4ee5\u751f\u6210Faster R-CNN ( ResNet backbone + FPN ) \u67b6\u6784\u7684\u914d\u7f6e\u6587\u4ef6: shell python tools/configure.py generate FasterRCNN ResNet RPNHead RoIAlign BBoxAssigner BBoxHead FasterRCNNTrainFeed FasterRCNNTestFeed LearningRate OptimizerBuilder \u5982\u9700\u6700\u5c0f\u914d\u7f6e\uff0c\u8fd0\u884c\uff1a shell python tools/configure.py --minimal generate FasterRCNN BBoxHead","title":"\u76f8\u5173\u5de5\u5177"},{"location":"CONFIG_cn/#faq","text":"Q: \u67d0\u4e9b\u914d\u7f6e\u9879\u4f1a\u5728\u591a\u4e2a\u6a21\u5757\u4e2d\u7528\u5230(\u5982 num_classes )\uff0c\u5982\u4f55\u907f\u514d\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u591a\u6b21\u91cd\u590d\u8bbe\u7f6e\uff1f A: \u6846\u67b6\u63d0\u4f9b\u4e86 __shared__ \u6807\u8bb0\u6765\u5b9e\u73b0\u914d\u7f6e\u7684\u5171\u4eab\uff0c\u7528\u6237\u53ef\u4ee5\u6807\u8bb0\u53c2\u6570\uff0c\u5982 __shared__ = ['num_classes'] \uff0c\u914d\u7f6e\u6570\u503c\u4f5c\u7528\u89c4\u5219\u5982\u4e0b\uff1a \u5982\u679c\u6a21\u5757\u914d\u7f6e\u4e2d\u63d0\u4f9b\u4e86 num_classes \uff0c\u4f1a\u4f18\u5148\u4f7f\u7528\u5176\u6570\u503c\u3002 \u5982\u679c\u6a21\u5757\u914d\u7f6e\u4e2d\u672a\u63d0\u4f9b num_classes \uff0c\u4f46\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b58\u5728\u5168\u5c40\u952e\u503c\uff0c\u90a3\u4e48\u4f1a\u4f7f\u7528\u5168\u5c40\u952e\u503c\u3002 \u4e24\u8005\u5747\u4e3a\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u4f7f\u7528\u9ed8\u8ba4\u503c( 81 )\u3002","title":"FAQ"},{"location":"DATA/","text":"English | \u7b80\u4f53\u4e2d\u6587 Data Pipline Introduction The data pipeline is responsible for loading and converting data. Each resulting data sample is a tuple of np.ndarrays. For example, Faster R-CNN training uses samples of this format: [(im, im_info, im_id, gt_bbox, gt_class, is_crowd), (...)] . Implementation The data pipeline consists of four sub-systems: data parsing, image pre-processing, data conversion and data feeding APIs. Data samples are collected to form data.Dataset s, usually 3 sets are needed for training, validation, and testing respectively. First, data.source loads the data files into memory, then data.transform processes them, and lastly, the batched samples are fetched by data.Reader . Sub-systems details: 1. Data parsing Parses various data sources and creates data.Dataset instances. Currently, following data sources are supported: COCO data source Loads COCO type datasets with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC data source Loads Pascal VOC like datasets with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Roidb data source A generalized data source serialized as pickle files, which have the following structure: (records, cname2id) # `cname2id` is a `dict` which maps category name to class IDs # and `records` is a list of dict of this structure: { 'im_file': im_fname, # image file name 'im_id': im_id, # image ID 'h': im_h, # height of image 'w': im_w, # width of image 'is_crowd': is_crowd, # crowd marker 'gt_class': gt_class, # ground truth class 'gt_bbox': gt_bbox, # ground truth bounding box 'gt_poly': gt_poly, # ground truth segmentation } We provide a tool to generate roidb data sources. To convert COCO or VOC like dataset, run this command: # --type: the type of original data (xml or json) # --annotation: the path of file, which contains the name of annotation files # --save-dir: the save path # --samples: the number of samples (default is -1, which mean all datas in dataset) python ./ppdet/data/tools/generate_data_for_training.py --type=json \\ --annotation=./annotations/instances_val2017.json \\ --save-dir=./roidb \\ --samples=-1 Image preprocessing the data.transform.operator module provides operations such as image decoding, expanding, cropping, etc. Multiple operators are combined to form larger processing pipelines. Data transformer Transform a data.Dataset to achieve various desired effects, Notably: the data.transform.paralle_map transformer accelerates image processing with multi-threads or multi-processes. More transformers can be found in data.transform.transformer . Data feeding apis To facilitate data pipeline building, we combine multiple data.Dataset to form a data.Reader which can provide data for training, validation and testing respectively. Users can simply call Reader.[train|eval|infer] to get the corresponding data stream. Many aspect of the Reader , such as storage location, preprocessing pipeline, acceleration mode can be configured with yaml files. APIs The main APIs are as follows: Data parsing source/coco_loader.py : COCO dataset parser. source source/voc_loader.py : Pascal VOC dataset parser. source [Note] To use a non-default label list for VOC datasets, a label_list.txt file is needed, one can use the provided label list ( data/pascalvoc/ImageSets/Main/label_list.txt ) or generate a custom one (with tools/generate_data_for_training.py ). Also, use_default_label option should be set to false in the configuration file source/loader.py : Roidb dataset parser. source Operator transform/operators.py : Contains a variety of data augmentation methods, including: DecodeImage : Read images in RGB format. RandomFlipImage : Horizontal flip. RandomDistort : Distort brightness, contrast, saturation, and hue. ResizeImage : Resize image with interpolation. RandomInterpImage : Use a random interpolation method to resize the image. CropImage : Crop image with respect to different scale, aspect ratio, and overlap. ExpandImage : Pad image to a larger size, padding filled with mean image value. NormalizeImage : Normalize image pixel values. NormalizeBox : Normalize the bounding box. Permute : Arrange the channels of the image and optionally convert image to BGR format. MixupImage : Mixup two images with given fraction 1 . [1] Please refer to this paper \u3002 transform/arrange_sample.py : Assemble the data samples needed by different models. 3. Transformer transform/post_map.py : Transformations that operates on whole batches, mainly for: - Padding whole batch to given stride values - Resize images to Multi-scales - Randomly adjust the image size of the batch data transform/transformer.py : Data filtering batching. transform/parallel_map.py : Accelerate data processing with multi-threads/multi-processes. 4. Reader reader.py : Combine source and transforms, return batch data according to max_iter . data_feed.py : Configure default parameters for reader.py . Usage Canned Datasets Preset for common datasets, e.g., COCO and Pascal Voc are included. In most cases, user can simply use these canned dataset as is. Moreover, the whole data pipeline is fully customizable through the yaml configuration files. Custom Datasets Option 1: Convert the dataset to COCO format. # a small utility (`tools/x2coco.py`) is provided to convert # Labelme-annotated dataset or cityscape dataset to COCO format. python ./ppdet/data/tools/x2coco.py --dataset_type labelme --json_input_dir ./labelme_annos/ --image_input_dir ./labelme_imgs/ --output_dir ./cocome/ --train_proportion 0.8 --val_proportion 0.2 --test_proportion 0.0 # --dataset_type: The data format which is need to be converted. Currently supported are: 'labelme' and 'cityscape' # --json_input_dir\uff1aThe path of json files which are annotated by Labelme. # --image_input_dir\uff1aThe path of images. # --output_dir\uff1aThe path of coverted COCO dataset. # --train_proportion\uff1aThe train proportion of annatation data. # --val_proportion\uff1aThe validation proportion of annatation data. # --test_proportion: The inference proportion of annatation data. Option 2: Add source/XX_loader.py and implement the load function, following the example of source/coco_loader.py and source/voc_loader.py . Modify the load function in source/loader.py to make use of the newly added data loader. Modify /source/__init__.py accordingly. if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource']: source_type = 'RoiDbSource' # Replace the above code with the following code: if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource', 'XXSource']: source_type = 'RoiDbSource' In the configure file, define the type of dataset as XXSource . How to add data pre-processing\uff1f To add pre-processing operation for a single image, refer to the classes in transform/operators.py , and implement the desired transformation with a new class. To add pre-processing for a batch, one needs to modify the build_post_map function in transform/post_map.py .","title":"DATA"},{"location":"DATA/#data-pipline","text":"","title":"Data Pipline"},{"location":"DATA/#introduction","text":"The data pipeline is responsible for loading and converting data. Each resulting data sample is a tuple of np.ndarrays. For example, Faster R-CNN training uses samples of this format: [(im, im_info, im_id, gt_bbox, gt_class, is_crowd), (...)] .","title":"Introduction"},{"location":"DATA/#implementation","text":"The data pipeline consists of four sub-systems: data parsing, image pre-processing, data conversion and data feeding APIs. Data samples are collected to form data.Dataset s, usually 3 sets are needed for training, validation, and testing respectively. First, data.source loads the data files into memory, then data.transform processes them, and lastly, the batched samples are fetched by data.Reader . Sub-systems details: 1. Data parsing Parses various data sources and creates data.Dataset instances. Currently, following data sources are supported: COCO data source Loads COCO type datasets with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC data source Loads Pascal VOC like datasets with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Roidb data source A generalized data source serialized as pickle files, which have the following structure: (records, cname2id) # `cname2id` is a `dict` which maps category name to class IDs # and `records` is a list of dict of this structure: { 'im_file': im_fname, # image file name 'im_id': im_id, # image ID 'h': im_h, # height of image 'w': im_w, # width of image 'is_crowd': is_crowd, # crowd marker 'gt_class': gt_class, # ground truth class 'gt_bbox': gt_bbox, # ground truth bounding box 'gt_poly': gt_poly, # ground truth segmentation } We provide a tool to generate roidb data sources. To convert COCO or VOC like dataset, run this command: # --type: the type of original data (xml or json) # --annotation: the path of file, which contains the name of annotation files # --save-dir: the save path # --samples: the number of samples (default is -1, which mean all datas in dataset) python ./ppdet/data/tools/generate_data_for_training.py --type=json \\ --annotation=./annotations/instances_val2017.json \\ --save-dir=./roidb \\ --samples=-1 Image preprocessing the data.transform.operator module provides operations such as image decoding, expanding, cropping, etc. Multiple operators are combined to form larger processing pipelines. Data transformer Transform a data.Dataset to achieve various desired effects, Notably: the data.transform.paralle_map transformer accelerates image processing with multi-threads or multi-processes. More transformers can be found in data.transform.transformer . Data feeding apis To facilitate data pipeline building, we combine multiple data.Dataset to form a data.Reader which can provide data for training, validation and testing respectively. Users can simply call Reader.[train|eval|infer] to get the corresponding data stream. Many aspect of the Reader , such as storage location, preprocessing pipeline, acceleration mode can be configured with yaml files.","title":"Implementation"},{"location":"DATA/#apis","text":"The main APIs are as follows: Data parsing source/coco_loader.py : COCO dataset parser. source source/voc_loader.py : Pascal VOC dataset parser. source [Note] To use a non-default label list for VOC datasets, a label_list.txt file is needed, one can use the provided label list ( data/pascalvoc/ImageSets/Main/label_list.txt ) or generate a custom one (with tools/generate_data_for_training.py ). Also, use_default_label option should be set to false in the configuration file source/loader.py : Roidb dataset parser. source Operator transform/operators.py : Contains a variety of data augmentation methods, including: DecodeImage : Read images in RGB format. RandomFlipImage : Horizontal flip. RandomDistort : Distort brightness, contrast, saturation, and hue. ResizeImage : Resize image with interpolation. RandomInterpImage : Use a random interpolation method to resize the image. CropImage : Crop image with respect to different scale, aspect ratio, and overlap. ExpandImage : Pad image to a larger size, padding filled with mean image value. NormalizeImage : Normalize image pixel values. NormalizeBox : Normalize the bounding box. Permute : Arrange the channels of the image and optionally convert image to BGR format. MixupImage : Mixup two images with given fraction 1 . [1] Please refer to this paper \u3002 transform/arrange_sample.py : Assemble the data samples needed by different models. 3. Transformer transform/post_map.py : Transformations that operates on whole batches, mainly for: - Padding whole batch to given stride values - Resize images to Multi-scales - Randomly adjust the image size of the batch data transform/transformer.py : Data filtering batching. transform/parallel_map.py : Accelerate data processing with multi-threads/multi-processes. 4. Reader reader.py : Combine source and transforms, return batch data according to max_iter . data_feed.py : Configure default parameters for reader.py .","title":"APIs"},{"location":"DATA/#usage","text":"","title":"Usage"},{"location":"DATA/#canned-datasets","text":"Preset for common datasets, e.g., COCO and Pascal Voc are included. In most cases, user can simply use these canned dataset as is. Moreover, the whole data pipeline is fully customizable through the yaml configuration files.","title":"Canned Datasets"},{"location":"DATA/#custom-datasets","text":"Option 1: Convert the dataset to COCO format. # a small utility (`tools/x2coco.py`) is provided to convert # Labelme-annotated dataset or cityscape dataset to COCO format. python ./ppdet/data/tools/x2coco.py --dataset_type labelme --json_input_dir ./labelme_annos/ --image_input_dir ./labelme_imgs/ --output_dir ./cocome/ --train_proportion 0.8 --val_proportion 0.2 --test_proportion 0.0 # --dataset_type: The data format which is need to be converted. Currently supported are: 'labelme' and 'cityscape' # --json_input_dir\uff1aThe path of json files which are annotated by Labelme. # --image_input_dir\uff1aThe path of images. # --output_dir\uff1aThe path of coverted COCO dataset. # --train_proportion\uff1aThe train proportion of annatation data. # --val_proportion\uff1aThe validation proportion of annatation data. # --test_proportion: The inference proportion of annatation data. Option 2: Add source/XX_loader.py and implement the load function, following the example of source/coco_loader.py and source/voc_loader.py . Modify the load function in source/loader.py to make use of the newly added data loader. Modify /source/__init__.py accordingly. if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource']: source_type = 'RoiDbSource' # Replace the above code with the following code: if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource', 'XXSource']: source_type = 'RoiDbSource' In the configure file, define the type of dataset as XXSource .","title":"Custom Datasets"},{"location":"DATA/#how-to-add-data-pre-processing","text":"To add pre-processing operation for a single image, refer to the classes in transform/operators.py , and implement the desired transformation with a new class. To add pre-processing for a batch, one needs to modify the build_post_map function in transform/post_map.py .","title":"How to add data pre-processing\uff1f"},{"location":"DATA_cn/","text":"\u6570\u636e\u6a21\u5757 \u4ecb\u7ecd \u672c\u6a21\u5757\u662f\u4e00\u4e2aPython\u6a21\u5757\uff0c\u7528\u4e8e\u52a0\u8f7d\u6570\u636e\u5e76\u5c06\u5176\u8f6c\u6362\u6210\u9002\u7528\u4e8e\u68c0\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u6240\u9700\u8981\u7684\u683c\u5f0f\u2014\u2014\u7531\u591a\u4e2anp.ndarray\u7ec4\u6210\u7684tuple\u6570\u7ec4\uff0c\u4f8b\u5982\u7528\u4e8eFaster R-CNN\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u4e3a\uff1a [(im, im_info, im_id, gt_bbox, gt_class, is_crowd), (...)] \u3002 \u5b9e\u73b0 \u8be5\u6a21\u5757\u5185\u90e8\u53ef\u5206\u4e3a4\u4e2a\u5b50\u529f\u80fd\uff1a\u6570\u636e\u89e3\u6790\u3001\u56fe\u7247\u9884\u5904\u7406\u3001\u6570\u636e\u8f6c\u6362\u548c\u6570\u636e\u83b7\u53d6\u63a5\u53e3\u3002 \u6211\u4eec\u91c7\u7528 data.Dataset \u8868\u793a\u4e00\u4efd\u6570\u636e\uff0c\u6bd4\u5982 COCO \u6570\u636e\u5305\u542b3\u4efd\u6570\u636e\uff0c\u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u3002\u539f\u59cb\u6570\u636e\u5b58\u50a8\u4e0e\u6587\u4ef6\u4e2d\uff0c\u901a\u8fc7 data.source \u52a0\u8f7d\u5230\u5185\u5b58\uff0c\u7136\u540e\u4f7f\u7528 data.transform \u5bf9\u6570\u636e\u8fdb\u884c\u5904\u7406\u8f6c\u6362\uff0c\u6700\u7ec8\u901a\u8fc7 data.Reader \u7684\u63a5\u53e3\u53ef\u4ee5\u83b7\u5f97\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u7684batch\u6570\u636e\u3002 \u5b50\u529f\u80fd\u4ecb\u7ecd\uff1a \u6570\u636e\u89e3\u6790 \u6570\u636e\u89e3\u6790\u5f97\u5230\u7684\u662f data.Dataset ,\u5b9e\u73b0\u903b\u8f91\u4f4d\u4e8e data.source \u4e2d\u3002\u901a\u8fc7\u5b83\u53ef\u4ee5\u5b9e\u73b0\u89e3\u6790\u4e0d\u540c\u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5df2\u652f\u6301\u7684\u6570\u636e\u6e90\u5305\u62ec\uff1a COCO\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u76ee\u524d\u5206\u4e3aCOCO2014\u548cCOCO2017\uff0c\u4e3b\u8981\u7531json\u6587\u4ef6\u548cimage\u6587\u4ef6\u7ec4\u6210\uff0c\u5176\u7ec4\u7ec7\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u76ee\u524d\u5206\u4e3aVOC2007\u548cVOC2012\uff0c\u4e3b\u8981\u7531xml\u6587\u4ef6\u548cimage\u6587\u4ef6\u7ec4\u6210\uff0c\u5176\u7ec4\u7ec7\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... \u8bf4\u660e\uff1a \u5982\u679c\u4f60\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e use_default_label=False , \u5c06\u4ece label_list.txt \u4e2d\u8bfb\u53d6\u7c7b\u522b\u5217\u8868\uff0c\u53cd\u4e4b\u5219\u53ef\u4ee5\u6ca1\u6709 label_list.txt \u6587\u4ef6\uff0c\u68c0\u6d4b\u5e93\u4f1a\u4f7f\u7528Pascal VOC\u6570\u636e\u96c6\u7684\u9ed8 \u8ba4\u7c7b\u522b\u5217\u8868\uff0c\u9ed8\u8ba4\u7c7b\u522b\u5217\u8868\u5b9a\u4e49\u5728 voc_loader.py Roidb\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u4e3b\u8981\u7531COCO\u6570\u636e\u96c6\u548cPascal VOC\u6570\u636e\u96c6\u8f6c\u6362\u800c\u6210\u7684pickle\u6587\u4ef6\uff0c\u5305\u542b\u4e00\u4e2adict\uff0c\u800cdict\u4e2d\u53ea\u5305\u542b\u4e00\u4e2a\u547d\u540d\u4e3a\u2018records\u2019\u7684list\uff08\u53ef\u80fd\u8fd8\u6709\u4e00\u4e2a\u547d\u540d\u4e3a\u2018cname2cid\u2019\u7684\u5b57\u5178\uff09\uff0c\u5176\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a (records, catname2clsid) 'records'\u662f\u4e00\u4e2alist\u5e76\u4e14\u5b83\u7684\u7ed3\u6784\u5982\u4e0b: { 'im_file': im_fname, # \u56fe\u50cf\u6587\u4ef6\u540d 'im_id': im_id, # \u56fe\u50cfid 'h': im_h, # \u56fe\u50cf\u9ad8\u5ea6 'w': im_w, # \u56fe\u50cf\u5bbd\u5ea6 'is_crowd': is_crowd, # \u662f\u5426\u91cd\u53e0 'gt_class': gt_class, # \u771f\u5b9e\u6846\u7c7b\u522b 'gt_bbox': gt_bbox, # \u771f\u5b9e\u6846\u5750\u6807 'gt_poly': gt_poly, # \u591a\u8fb9\u5f62\u5750\u6807 } 'cname2id'\u662f\u4e00\u4e2adict\uff0c\u4fdd\u5b58\u4e86\u7c7b\u522b\u540d\u5230id\u7684\u6620\u5c04 \u6211\u4eec\u5728 ./tools/ \u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u6210roidb\u6570\u636e\u96c6\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u547d\u4ee4\u5b9e\u73b0\u8be5\u529f\u80fd\u3002 # --type: \u539f\u59cb\u6570\u636e\u96c6\u7684\u7c7b\u522b\uff08\u53ea\u80fd\u662fxml\u6216\u8005json\uff09 # --annotation: \u4e00\u4e2a\u5305\u542b\u6240\u9700\u6807\u6ce8\u6587\u4ef6\u540d\u7684\u6587\u4ef6\u7684\u8def\u5f84 # --save-dir: \u4fdd\u5b58\u8def\u5f84 # --samples: sample\u7684\u4e2a\u6570\uff08\u9ed8\u8ba4\u662f-1\uff0c\u4ee3\u8868\u4f7f\u7528\u6240\u6709sample\uff09 python ./ppdet/data/tools/generate_data_for_training.py --type=json \\ --annotation=./annotations/instances_val2017.json \\ --save-dir=./roidb \\ --samples=-1 \u56fe\u7247\u9884\u5904\u7406 \u56fe\u7247\u9884\u5904\u7406\u901a\u8fc7\u5305\u62ec\u56fe\u7247\u89e3\u7801\u3001\u7f29\u653e\u3001\u88c1\u526a\u7b49\u64cd\u4f5c\uff0c\u6211\u4eec\u91c7\u7528 data.transform.operator \u7b97\u5b50\u7684\u65b9\u5f0f\u6765\u7edf\u4e00\u5b9e\u73b0\uff0c\u8fd9\u6837\u80fd\u65b9\u4fbf\u6269\u5c55\u3002\u6b64\u5916\uff0c\u591a\u4e2a\u7b97\u5b50\u8fd8\u53ef\u4ee5\u7ec4\u5408\u5f62\u6210\u590d\u6742\u7684\u5904\u7406\u6d41\u7a0b, \u5e76\u88ab data.transformer \u4e2d\u7684\u8f6c\u6362\u5668\u4f7f\u7528\uff0c\u6bd4\u5982\u591a\u7ebf\u7a0b\u5b8c\u6210\u4e00\u4e2a\u590d\u6742\u7684\u9884\u5904\u7406\u6d41\u7a0b\u3002 \u6570\u636e\u8f6c\u6362\u5668 \u6570\u636e\u8f6c\u6362\u5668\u7684\u529f\u80fd\u662f\u5b8c\u6210\u5bf9\u67d0\u4e2a data.Dataset \u8fdb\u884c\u8f6c\u6362\u5904\u7406\uff0c\u4ece\u800c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 data.Dataset \u3002\u6211\u4eec\u91c7\u7528\u88c5\u9970\u5668\u6a21\u5f0f\u5b9e\u73b0\u5404\u79cd\u4e0d\u540c\u7684 data.transform.transformer \u3002\u6bd4\u5982\u7528\u4e8e\u591a\u8fdb\u7a0b\u9884\u5904\u7406\u7684 dataset.transform.paralle_map \u8f6c\u6362\u5668\u3002 \u6570\u636e\u83b7\u53d6\u63a5\u53e3 \u4e3a\u65b9\u4fbf\u8bad\u7ec3\u65f6\u7684\u6570\u636e\u83b7\u53d6\uff0c\u6211\u4eec\u5c06\u591a\u4e2a data.Dataset \u7ec4\u5408\u5728\u4e00\u8d77\u6784\u6210\u4e00\u4e2a data.Reader \u4e3a\u7528\u6237\u63d0\u4f9b\u6570\u636e\uff0c\u7528\u6237\u53ea\u9700\u8981\u8c03\u7528 Reader.[train|eval|infer] \u5373\u53ef\u83b7\u5f97\u5bf9\u5e94\u7684\u6570\u636e\u6d41\u3002 Reader \u652f\u6301yaml\u6587\u4ef6\u914d\u7f6e\u6570\u636e\u5730\u5740\u3001\u9884\u5904\u7406\u8fc7\u7a0b\u3001\u52a0\u901f\u65b9\u5f0f\u7b49\u3002 APIs \u4e3b\u8981\u7684APIs\u5982\u4e0b\uff1a \u6570\u636e\u89e3\u6790 source/coco_loader.py \uff1a\u7528\u4e8e\u89e3\u6790COCO\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 source/voc_loader.py \uff1a\u7528\u4e8e\u89e3\u6790Pascal VOC\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 [\u6ce8\u610f]\u5728\u4f7f\u7528VOC\u6570\u636e\u96c6\u65f6\uff0c\u82e5\u4e0d\u4f7f\u7528\u9ed8\u8ba4\u7684label\u5217\u8868\uff0c\u5219\u9700\u8981\u5148\u4f7f\u7528 tools/generate_data_for_training.py \u751f\u6210 label_list.txt \uff08\u4f7f\u7528\u65b9\u5f0f\u4e0e\u6570\u636e\u89e3\u6790\u4e2d\u7684roidb\u6570\u636e\u96c6\u83b7\u53d6\u8fc7\u7a0b\u4e00\u81f4\uff09\uff0c\u6216\u63d0\u4f9b label_list.txt \u653e\u7f6e\u4e8e data/pascalvoc/ImageSets/Main \u4e2d\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e\u53c2\u6570 use_default_label \u4e3a true \u3002 source/loader.py \uff1a\u7528\u4e8e\u89e3\u6790Roidb\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 \u7b97\u5b50 transform/operators.py \uff1a\u5305\u542b\u591a\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u5f0f\uff0c\u4e3b\u8981\u5305\u62ec\uff1a RandomFlipImage\uff1a\u6c34\u5e73\u7ffb\u8f6c\u3002 RandomDistort\uff1a\u968f\u673a\u6270\u52a8\u56fe\u7247\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u548c\u8272\u76f8\u3002 ResizeImage\uff1a\u6839\u636e\u7279\u5b9a\u7684\u63d2\u503c\u65b9\u5f0f\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\u3002 RandomInterpImage\uff1a\u4f7f\u7528\u968f\u673a\u7684\u63d2\u503c\u65b9\u5f0f\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\u3002 CropImage\uff1a\u6839\u636e\u7f29\u653e\u6bd4\u4f8b\u3001\u957f\u5bbd\u6bd4\u4f8b\u4e24\u4e2a\u53c2\u6570\u751f\u6210\u82e5\u5e72\u5019\u9009\u6846\uff0c\u518d\u4f9d\u636e\u8fd9\u4e9b\u5019\u9009\u6846\u548c\u6807\u6ce8\u6846\u7684\u9762\u79ef\u4ea4\u5e76\u6bd4(IoU)\u6311\u9009\u51fa\u7b26\u5408\u8981\u6c42\u7684\u88c1\u526a\u7ed3\u679c\u3002 ExpandImage\uff1a\u5c06\u539f\u59cb\u56fe\u7247\u653e\u8fdb\u4e00\u5f20\u4f7f\u7528\u50cf\u7d20\u5747\u503c\u586b\u5145(\u968f\u540e\u4f1a\u5728\u51cf\u5747\u503c\u64cd\u4f5c\u4e2d\u51cf\u6389)\u7684\u6269\u5f20\u56fe\u4e2d\uff0c\u518d\u5bf9\u6b64\u56fe\u8fdb\u884c\u88c1\u526a\u3001\u7f29\u653e\u548c\u7ffb\u8f6c\u3002 DecodeImage\uff1a\u4ee5RGB\u683c\u5f0f\u8bfb\u53d6\u56fe\u50cf\u3002 Permute\uff1a\u5bf9\u56fe\u50cf\u7684\u901a\u9053\u8fdb\u884c\u6392\u5217\u5e76\u8f6c\u4e3aBGR\u683c\u5f0f\u3002 NormalizeImage\uff1a\u5bf9\u56fe\u50cf\u50cf\u7d20\u503c\u8fdb\u884c\u5f52\u4e00\u5316\u3002 NormalizeBox\uff1a\u5bf9bounding box\u8fdb\u884c\u5f52\u4e00\u5316\u3002 MixupImage\uff1a\u6309\u6bd4\u4f8b\u53e0\u52a0\u4e24\u5f20\u56fe\u50cf\u3002 [\u6ce8\u610f]\uff1aMixup\u7684\u64cd\u4f5c\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 transform/arrange_sample.py \uff1a\u5b9e\u73b0\u5bf9\u8f93\u5165\u7f51\u7edc\u6570\u636e\u7684\u6392\u5e8f\u3002 3. \u8f6c\u6362 transform/post_map.py \uff1a\u7528\u4e8e\u5b8c\u6210\u6279\u6570\u636e\u7684\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u5176\u4e3b\u8981\u5305\u62ec\uff1a \u968f\u673a\u8c03\u6574\u6279\u6570\u636e\u7684\u56fe\u50cf\u5927\u5c0f \u591a\u5c3a\u5ea6\u8c03\u6574\u56fe\u50cf\u5927\u5c0f padding\u64cd\u4f5c transform/transformer.py \uff1a\u7528\u4e8e\u8fc7\u6ee4\u65e0\u7528\u7684\u6570\u636e\uff0c\u5e76\u8fd4\u56de\u6279\u6570\u636e\u3002 transform/parallel_map.py \uff1a\u7528\u4e8e\u5b9e\u73b0\u52a0\u901f\u3002 4. \u8bfb\u53d6 reader.py \uff1a\u7528\u4e8e\u7ec4\u5408source\u548ctransformer\u64cd\u4f5c\uff0c\u6839\u636e max_iter \u8fd4\u56debatch\u6570\u636e\u3002 data_feed.py : \u7528\u4e8e\u914d\u7f6e reader.py \u4e2d\u6240\u9700\u7684\u9ed8\u8ba4\u53c2\u6570. \u4f7f\u7528 \u5e38\u89c4\u4f7f\u7528 \u7ed3\u5408yaml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u5b8c\u6210\u672c\u6a21\u5757\u7684\u529f\u80fd\u3002yaml\u6587\u4ef6\u7684\u4f7f\u7528\u53ef\u4ee5\u53c2\u89c1\u914d\u7f6e\u6587\u4ef6\u90e8\u5206\u3002 \u8bfb\u53d6\u7528\u4e8e\u8bad\u7ec3\u7684\u6570\u636e ccfg = load_cfg('./config.yml') coco = Reader(ccfg.DATA, ccfg.TRANSFORM, maxiter=-1) \u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff1f \u9009\u62e91\uff1a\u5c06\u6570\u636e\u96c6\u8f6c\u6362\u4e3aCOCO\u683c\u5f0f\u3002 # \u5728./tools/\u4e2d\u63d0\u4f9b\u4e86x2coco.py\u7528\u4e8e\u5c06labelme\u6807\u6ce8\u7684\u6570\u636e\u96c6\u6216cityscape\u6570\u636e\u96c6\u8f6c\u6362\u4e3aCOCO\u6570\u636e\u96c6 python ./ppdet/data/tools/x2coco.py --dataset_type labelme --json_input_dir ./labelme_annos/ --image_input_dir ./labelme_imgs/ --output_dir ./cocome/ --train_proportion 0.8 --val_proportion 0.2 --test_proportion 0.0 # --dataset_type\uff1a\u9700\u8981\u8f6c\u6362\u7684\u6570\u636e\u683c\u5f0f\uff0c\u76ee\u524d\u652f\u6301\uff1a\u2019labelme\u2018\u548c\u2019cityscape\u2018 # --json_input_dir\uff1a\u4f7f\u7528labelme\u6807\u6ce8\u7684json\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 # --image_input_dir\uff1a\u56fe\u50cf\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 # --output_dir\uff1a\u8f6c\u6362\u540e\u7684COCO\u683c\u5f0f\u6570\u636e\u96c6\u5b58\u653e\u4f4d\u7f6e # --train_proportion\uff1a\u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8etrain\u7684\u6bd4\u4f8b # --val_proportion\uff1a\u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8evalidation\u7684\u6bd4\u4f8b # --test_proportion: \u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8einfer\u7684\u6bd4\u4f8b \u9009\u62e92\uff1a \u4eff\u7167 ./source/coco_loader.py \u548c ./source/voc_loader.py \uff0c\u6dfb\u52a0 ./source/XX_loader.py \u5e76\u5b9e\u73b0 load \u51fd\u6570\u3002 \u5728 ./source/loader.py \u7684 load \u51fd\u6570\u4e2d\u6dfb\u52a0\u4f7f\u7528 ./source/XX_loader.py \u7684\u5165\u53e3\u3002 \u4fee\u6539 ./source/__init__.py \uff1a if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource']: source_type = 'RoiDbSource' # \u5c06\u4e0a\u8ff0\u4ee3\u7801\u66ff\u6362\u4e3a\u5982\u4e0b\u4ee3\u7801\uff1a if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource', 'XXSource']: source_type = 'RoiDbSource' \u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4fee\u6539 dataset \u4e0b\u7684 type \u4e3a XXSource \u3002 \u5982\u4f55\u589e\u52a0\u6570\u636e\u9884\u5904\u7406\uff1f \u82e5\u589e\u52a0\u5355\u5f20\u56fe\u50cf\u7684\u589e\u5f3a\u9884\u5904\u7406\uff0c\u53ef\u5728 transform/operators.py \u4e2d\u53c2\u8003\u6bcf\u4e2a\u7c7b\u7684\u4ee3\u7801\uff0c\u65b0\u5efa\u4e00\u4e2a\u7c7b\u6765\u5b9e\u73b0\u65b0\u7684\u6570\u636e\u589e\u5f3a\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u8be5\u9884\u5904\u7406\u3002 \u82e5\u589e\u52a0\u5355\u4e2abatch\u7684\u56fe\u50cf\u9884\u5904\u7406\uff0c\u53ef\u5728 transform/post_map.py \u4e2d\u53c2\u8003 build_post_map \u4e2d\u6bcf\u4e2a\u51fd\u6570\u7684\u4ee3\u7801\uff0c\u65b0\u5efa\u4e00\u4e2a\u5185\u90e8\u51fd\u6570\u6765\u5b9e\u73b0\u65b0\u7684\u6279\u6570\u636e\u9884\u5904\u7406\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u8be5\u9884\u5904\u7406\u3002","title":"\u6570\u636e\u6a21\u5757"},{"location":"DATA_cn/#_1","text":"","title":"\u6570\u636e\u6a21\u5757"},{"location":"DATA_cn/#_2","text":"\u672c\u6a21\u5757\u662f\u4e00\u4e2aPython\u6a21\u5757\uff0c\u7528\u4e8e\u52a0\u8f7d\u6570\u636e\u5e76\u5c06\u5176\u8f6c\u6362\u6210\u9002\u7528\u4e8e\u68c0\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u6240\u9700\u8981\u7684\u683c\u5f0f\u2014\u2014\u7531\u591a\u4e2anp.ndarray\u7ec4\u6210\u7684tuple\u6570\u7ec4\uff0c\u4f8b\u5982\u7528\u4e8eFaster R-CNN\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u4e3a\uff1a [(im, im_info, im_id, gt_bbox, gt_class, is_crowd), (...)] \u3002","title":"\u4ecb\u7ecd"},{"location":"DATA_cn/#_3","text":"\u8be5\u6a21\u5757\u5185\u90e8\u53ef\u5206\u4e3a4\u4e2a\u5b50\u529f\u80fd\uff1a\u6570\u636e\u89e3\u6790\u3001\u56fe\u7247\u9884\u5904\u7406\u3001\u6570\u636e\u8f6c\u6362\u548c\u6570\u636e\u83b7\u53d6\u63a5\u53e3\u3002 \u6211\u4eec\u91c7\u7528 data.Dataset \u8868\u793a\u4e00\u4efd\u6570\u636e\uff0c\u6bd4\u5982 COCO \u6570\u636e\u5305\u542b3\u4efd\u6570\u636e\uff0c\u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u3002\u539f\u59cb\u6570\u636e\u5b58\u50a8\u4e0e\u6587\u4ef6\u4e2d\uff0c\u901a\u8fc7 data.source \u52a0\u8f7d\u5230\u5185\u5b58\uff0c\u7136\u540e\u4f7f\u7528 data.transform \u5bf9\u6570\u636e\u8fdb\u884c\u5904\u7406\u8f6c\u6362\uff0c\u6700\u7ec8\u901a\u8fc7 data.Reader \u7684\u63a5\u53e3\u53ef\u4ee5\u83b7\u5f97\u7528\u4e8e\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u7684batch\u6570\u636e\u3002 \u5b50\u529f\u80fd\u4ecb\u7ecd\uff1a \u6570\u636e\u89e3\u6790 \u6570\u636e\u89e3\u6790\u5f97\u5230\u7684\u662f data.Dataset ,\u5b9e\u73b0\u903b\u8f91\u4f4d\u4e8e data.source \u4e2d\u3002\u901a\u8fc7\u5b83\u53ef\u4ee5\u5b9e\u73b0\u89e3\u6790\u4e0d\u540c\u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5df2\u652f\u6301\u7684\u6570\u636e\u6e90\u5305\u62ec\uff1a COCO\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u76ee\u524d\u5206\u4e3aCOCO2014\u548cCOCO2017\uff0c\u4e3b\u8981\u7531json\u6587\u4ef6\u548cimage\u6587\u4ef6\u7ec4\u6210\uff0c\u5176\u7ec4\u7ec7\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u76ee\u524d\u5206\u4e3aVOC2007\u548cVOC2012\uff0c\u4e3b\u8981\u7531xml\u6587\u4ef6\u548cimage\u6587\u4ef6\u7ec4\u6210\uff0c\u5176\u7ec4\u7ec7\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... \u8bf4\u660e\uff1a \u5982\u679c\u4f60\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e use_default_label=False , \u5c06\u4ece label_list.txt \u4e2d\u8bfb\u53d6\u7c7b\u522b\u5217\u8868\uff0c\u53cd\u4e4b\u5219\u53ef\u4ee5\u6ca1\u6709 label_list.txt \u6587\u4ef6\uff0c\u68c0\u6d4b\u5e93\u4f1a\u4f7f\u7528Pascal VOC\u6570\u636e\u96c6\u7684\u9ed8 \u8ba4\u7c7b\u522b\u5217\u8868\uff0c\u9ed8\u8ba4\u7c7b\u522b\u5217\u8868\u5b9a\u4e49\u5728 voc_loader.py Roidb\u6570\u636e\u6e90 \u8be5\u6570\u636e\u96c6\u4e3b\u8981\u7531COCO\u6570\u636e\u96c6\u548cPascal VOC\u6570\u636e\u96c6\u8f6c\u6362\u800c\u6210\u7684pickle\u6587\u4ef6\uff0c\u5305\u542b\u4e00\u4e2adict\uff0c\u800cdict\u4e2d\u53ea\u5305\u542b\u4e00\u4e2a\u547d\u540d\u4e3a\u2018records\u2019\u7684list\uff08\u53ef\u80fd\u8fd8\u6709\u4e00\u4e2a\u547d\u540d\u4e3a\u2018cname2cid\u2019\u7684\u5b57\u5178\uff09\uff0c\u5176\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a (records, catname2clsid) 'records'\u662f\u4e00\u4e2alist\u5e76\u4e14\u5b83\u7684\u7ed3\u6784\u5982\u4e0b: { 'im_file': im_fname, # \u56fe\u50cf\u6587\u4ef6\u540d 'im_id': im_id, # \u56fe\u50cfid 'h': im_h, # \u56fe\u50cf\u9ad8\u5ea6 'w': im_w, # \u56fe\u50cf\u5bbd\u5ea6 'is_crowd': is_crowd, # \u662f\u5426\u91cd\u53e0 'gt_class': gt_class, # \u771f\u5b9e\u6846\u7c7b\u522b 'gt_bbox': gt_bbox, # \u771f\u5b9e\u6846\u5750\u6807 'gt_poly': gt_poly, # \u591a\u8fb9\u5f62\u5750\u6807 } 'cname2id'\u662f\u4e00\u4e2adict\uff0c\u4fdd\u5b58\u4e86\u7c7b\u522b\u540d\u5230id\u7684\u6620\u5c04 \u6211\u4eec\u5728 ./tools/ \u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u6210roidb\u6570\u636e\u96c6\u7684\u4ee3\u7801\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e0b\u9762\u547d\u4ee4\u5b9e\u73b0\u8be5\u529f\u80fd\u3002 # --type: \u539f\u59cb\u6570\u636e\u96c6\u7684\u7c7b\u522b\uff08\u53ea\u80fd\u662fxml\u6216\u8005json\uff09 # --annotation: \u4e00\u4e2a\u5305\u542b\u6240\u9700\u6807\u6ce8\u6587\u4ef6\u540d\u7684\u6587\u4ef6\u7684\u8def\u5f84 # --save-dir: \u4fdd\u5b58\u8def\u5f84 # --samples: sample\u7684\u4e2a\u6570\uff08\u9ed8\u8ba4\u662f-1\uff0c\u4ee3\u8868\u4f7f\u7528\u6240\u6709sample\uff09 python ./ppdet/data/tools/generate_data_for_training.py --type=json \\ --annotation=./annotations/instances_val2017.json \\ --save-dir=./roidb \\ --samples=-1 \u56fe\u7247\u9884\u5904\u7406 \u56fe\u7247\u9884\u5904\u7406\u901a\u8fc7\u5305\u62ec\u56fe\u7247\u89e3\u7801\u3001\u7f29\u653e\u3001\u88c1\u526a\u7b49\u64cd\u4f5c\uff0c\u6211\u4eec\u91c7\u7528 data.transform.operator \u7b97\u5b50\u7684\u65b9\u5f0f\u6765\u7edf\u4e00\u5b9e\u73b0\uff0c\u8fd9\u6837\u80fd\u65b9\u4fbf\u6269\u5c55\u3002\u6b64\u5916\uff0c\u591a\u4e2a\u7b97\u5b50\u8fd8\u53ef\u4ee5\u7ec4\u5408\u5f62\u6210\u590d\u6742\u7684\u5904\u7406\u6d41\u7a0b, \u5e76\u88ab data.transformer \u4e2d\u7684\u8f6c\u6362\u5668\u4f7f\u7528\uff0c\u6bd4\u5982\u591a\u7ebf\u7a0b\u5b8c\u6210\u4e00\u4e2a\u590d\u6742\u7684\u9884\u5904\u7406\u6d41\u7a0b\u3002 \u6570\u636e\u8f6c\u6362\u5668 \u6570\u636e\u8f6c\u6362\u5668\u7684\u529f\u80fd\u662f\u5b8c\u6210\u5bf9\u67d0\u4e2a data.Dataset \u8fdb\u884c\u8f6c\u6362\u5904\u7406\uff0c\u4ece\u800c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 data.Dataset \u3002\u6211\u4eec\u91c7\u7528\u88c5\u9970\u5668\u6a21\u5f0f\u5b9e\u73b0\u5404\u79cd\u4e0d\u540c\u7684 data.transform.transformer \u3002\u6bd4\u5982\u7528\u4e8e\u591a\u8fdb\u7a0b\u9884\u5904\u7406\u7684 dataset.transform.paralle_map \u8f6c\u6362\u5668\u3002 \u6570\u636e\u83b7\u53d6\u63a5\u53e3 \u4e3a\u65b9\u4fbf\u8bad\u7ec3\u65f6\u7684\u6570\u636e\u83b7\u53d6\uff0c\u6211\u4eec\u5c06\u591a\u4e2a data.Dataset \u7ec4\u5408\u5728\u4e00\u8d77\u6784\u6210\u4e00\u4e2a data.Reader \u4e3a\u7528\u6237\u63d0\u4f9b\u6570\u636e\uff0c\u7528\u6237\u53ea\u9700\u8981\u8c03\u7528 Reader.[train|eval|infer] \u5373\u53ef\u83b7\u5f97\u5bf9\u5e94\u7684\u6570\u636e\u6d41\u3002 Reader \u652f\u6301yaml\u6587\u4ef6\u914d\u7f6e\u6570\u636e\u5730\u5740\u3001\u9884\u5904\u7406\u8fc7\u7a0b\u3001\u52a0\u901f\u65b9\u5f0f\u7b49\u3002","title":"\u5b9e\u73b0"},{"location":"DATA_cn/#apis","text":"\u4e3b\u8981\u7684APIs\u5982\u4e0b\uff1a \u6570\u636e\u89e3\u6790 source/coco_loader.py \uff1a\u7528\u4e8e\u89e3\u6790COCO\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 source/voc_loader.py \uff1a\u7528\u4e8e\u89e3\u6790Pascal VOC\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 [\u6ce8\u610f]\u5728\u4f7f\u7528VOC\u6570\u636e\u96c6\u65f6\uff0c\u82e5\u4e0d\u4f7f\u7528\u9ed8\u8ba4\u7684label\u5217\u8868\uff0c\u5219\u9700\u8981\u5148\u4f7f\u7528 tools/generate_data_for_training.py \u751f\u6210 label_list.txt \uff08\u4f7f\u7528\u65b9\u5f0f\u4e0e\u6570\u636e\u89e3\u6790\u4e2d\u7684roidb\u6570\u636e\u96c6\u83b7\u53d6\u8fc7\u7a0b\u4e00\u81f4\uff09\uff0c\u6216\u63d0\u4f9b label_list.txt \u653e\u7f6e\u4e8e data/pascalvoc/ImageSets/Main \u4e2d\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e\u53c2\u6570 use_default_label \u4e3a true \u3002 source/loader.py \uff1a\u7528\u4e8e\u89e3\u6790Roidb\u6570\u636e\u96c6\u3002 \u8be6\u89c1\u4ee3\u7801 \u7b97\u5b50 transform/operators.py \uff1a\u5305\u542b\u591a\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u5f0f\uff0c\u4e3b\u8981\u5305\u62ec\uff1a RandomFlipImage\uff1a\u6c34\u5e73\u7ffb\u8f6c\u3002 RandomDistort\uff1a\u968f\u673a\u6270\u52a8\u56fe\u7247\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9971\u548c\u5ea6\u548c\u8272\u76f8\u3002 ResizeImage\uff1a\u6839\u636e\u7279\u5b9a\u7684\u63d2\u503c\u65b9\u5f0f\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\u3002 RandomInterpImage\uff1a\u4f7f\u7528\u968f\u673a\u7684\u63d2\u503c\u65b9\u5f0f\u8c03\u6574\u56fe\u50cf\u5927\u5c0f\u3002 CropImage\uff1a\u6839\u636e\u7f29\u653e\u6bd4\u4f8b\u3001\u957f\u5bbd\u6bd4\u4f8b\u4e24\u4e2a\u53c2\u6570\u751f\u6210\u82e5\u5e72\u5019\u9009\u6846\uff0c\u518d\u4f9d\u636e\u8fd9\u4e9b\u5019\u9009\u6846\u548c\u6807\u6ce8\u6846\u7684\u9762\u79ef\u4ea4\u5e76\u6bd4(IoU)\u6311\u9009\u51fa\u7b26\u5408\u8981\u6c42\u7684\u88c1\u526a\u7ed3\u679c\u3002 ExpandImage\uff1a\u5c06\u539f\u59cb\u56fe\u7247\u653e\u8fdb\u4e00\u5f20\u4f7f\u7528\u50cf\u7d20\u5747\u503c\u586b\u5145(\u968f\u540e\u4f1a\u5728\u51cf\u5747\u503c\u64cd\u4f5c\u4e2d\u51cf\u6389)\u7684\u6269\u5f20\u56fe\u4e2d\uff0c\u518d\u5bf9\u6b64\u56fe\u8fdb\u884c\u88c1\u526a\u3001\u7f29\u653e\u548c\u7ffb\u8f6c\u3002 DecodeImage\uff1a\u4ee5RGB\u683c\u5f0f\u8bfb\u53d6\u56fe\u50cf\u3002 Permute\uff1a\u5bf9\u56fe\u50cf\u7684\u901a\u9053\u8fdb\u884c\u6392\u5217\u5e76\u8f6c\u4e3aBGR\u683c\u5f0f\u3002 NormalizeImage\uff1a\u5bf9\u56fe\u50cf\u50cf\u7d20\u503c\u8fdb\u884c\u5f52\u4e00\u5316\u3002 NormalizeBox\uff1a\u5bf9bounding box\u8fdb\u884c\u5f52\u4e00\u5316\u3002 MixupImage\uff1a\u6309\u6bd4\u4f8b\u53e0\u52a0\u4e24\u5f20\u56fe\u50cf\u3002 [\u6ce8\u610f]\uff1aMixup\u7684\u64cd\u4f5c\u53ef\u53c2\u8003 \u8bba\u6587 \u3002 transform/arrange_sample.py \uff1a\u5b9e\u73b0\u5bf9\u8f93\u5165\u7f51\u7edc\u6570\u636e\u7684\u6392\u5e8f\u3002 3. \u8f6c\u6362 transform/post_map.py \uff1a\u7528\u4e8e\u5b8c\u6210\u6279\u6570\u636e\u7684\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u5176\u4e3b\u8981\u5305\u62ec\uff1a \u968f\u673a\u8c03\u6574\u6279\u6570\u636e\u7684\u56fe\u50cf\u5927\u5c0f \u591a\u5c3a\u5ea6\u8c03\u6574\u56fe\u50cf\u5927\u5c0f padding\u64cd\u4f5c transform/transformer.py \uff1a\u7528\u4e8e\u8fc7\u6ee4\u65e0\u7528\u7684\u6570\u636e\uff0c\u5e76\u8fd4\u56de\u6279\u6570\u636e\u3002 transform/parallel_map.py \uff1a\u7528\u4e8e\u5b9e\u73b0\u52a0\u901f\u3002 4. \u8bfb\u53d6 reader.py \uff1a\u7528\u4e8e\u7ec4\u5408source\u548ctransformer\u64cd\u4f5c\uff0c\u6839\u636e max_iter \u8fd4\u56debatch\u6570\u636e\u3002 data_feed.py : \u7528\u4e8e\u914d\u7f6e reader.py \u4e2d\u6240\u9700\u7684\u9ed8\u8ba4\u53c2\u6570.","title":"APIs"},{"location":"DATA_cn/#_4","text":"","title":"\u4f7f\u7528"},{"location":"DATA_cn/#_5","text":"\u7ed3\u5408yaml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u5b8c\u6210\u672c\u6a21\u5757\u7684\u529f\u80fd\u3002yaml\u6587\u4ef6\u7684\u4f7f\u7528\u53ef\u4ee5\u53c2\u89c1\u914d\u7f6e\u6587\u4ef6\u90e8\u5206\u3002 \u8bfb\u53d6\u7528\u4e8e\u8bad\u7ec3\u7684\u6570\u636e ccfg = load_cfg('./config.yml') coco = Reader(ccfg.DATA, ccfg.TRANSFORM, maxiter=-1)","title":"\u5e38\u89c4\u4f7f\u7528"},{"location":"DATA_cn/#_6","text":"\u9009\u62e91\uff1a\u5c06\u6570\u636e\u96c6\u8f6c\u6362\u4e3aCOCO\u683c\u5f0f\u3002 # \u5728./tools/\u4e2d\u63d0\u4f9b\u4e86x2coco.py\u7528\u4e8e\u5c06labelme\u6807\u6ce8\u7684\u6570\u636e\u96c6\u6216cityscape\u6570\u636e\u96c6\u8f6c\u6362\u4e3aCOCO\u6570\u636e\u96c6 python ./ppdet/data/tools/x2coco.py --dataset_type labelme --json_input_dir ./labelme_annos/ --image_input_dir ./labelme_imgs/ --output_dir ./cocome/ --train_proportion 0.8 --val_proportion 0.2 --test_proportion 0.0 # --dataset_type\uff1a\u9700\u8981\u8f6c\u6362\u7684\u6570\u636e\u683c\u5f0f\uff0c\u76ee\u524d\u652f\u6301\uff1a\u2019labelme\u2018\u548c\u2019cityscape\u2018 # --json_input_dir\uff1a\u4f7f\u7528labelme\u6807\u6ce8\u7684json\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 # --image_input_dir\uff1a\u56fe\u50cf\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 # --output_dir\uff1a\u8f6c\u6362\u540e\u7684COCO\u683c\u5f0f\u6570\u636e\u96c6\u5b58\u653e\u4f4d\u7f6e # --train_proportion\uff1a\u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8etrain\u7684\u6bd4\u4f8b # --val_proportion\uff1a\u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8evalidation\u7684\u6bd4\u4f8b # --test_proportion: \u6807\u6ce8\u6570\u636e\u4e2d\u7528\u4e8einfer\u7684\u6bd4\u4f8b \u9009\u62e92\uff1a \u4eff\u7167 ./source/coco_loader.py \u548c ./source/voc_loader.py \uff0c\u6dfb\u52a0 ./source/XX_loader.py \u5e76\u5b9e\u73b0 load \u51fd\u6570\u3002 \u5728 ./source/loader.py \u7684 load \u51fd\u6570\u4e2d\u6dfb\u52a0\u4f7f\u7528 ./source/XX_loader.py \u7684\u5165\u53e3\u3002 \u4fee\u6539 ./source/__init__.py \uff1a if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource']: source_type = 'RoiDbSource' # \u5c06\u4e0a\u8ff0\u4ee3\u7801\u66ff\u6362\u4e3a\u5982\u4e0b\u4ee3\u7801\uff1a if data_cf['type'] in ['VOCSource', 'COCOSource', 'RoiDbSource', 'XXSource']: source_type = 'RoiDbSource' \u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4fee\u6539 dataset \u4e0b\u7684 type \u4e3a XXSource \u3002","title":"\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff1f"},{"location":"DATA_cn/#_7","text":"\u82e5\u589e\u52a0\u5355\u5f20\u56fe\u50cf\u7684\u589e\u5f3a\u9884\u5904\u7406\uff0c\u53ef\u5728 transform/operators.py \u4e2d\u53c2\u8003\u6bcf\u4e2a\u7c7b\u7684\u4ee3\u7801\uff0c\u65b0\u5efa\u4e00\u4e2a\u7c7b\u6765\u5b9e\u73b0\u65b0\u7684\u6570\u636e\u589e\u5f3a\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u8be5\u9884\u5904\u7406\u3002 \u82e5\u589e\u52a0\u5355\u4e2abatch\u7684\u56fe\u50cf\u9884\u5904\u7406\uff0c\u53ef\u5728 transform/post_map.py \u4e2d\u53c2\u8003 build_post_map \u4e2d\u6bcf\u4e2a\u51fd\u6570\u7684\u4ee3\u7801\uff0c\u65b0\u5efa\u4e00\u4e2a\u5185\u90e8\u51fd\u6570\u6765\u5b9e\u73b0\u65b0\u7684\u6279\u6570\u636e\u9884\u5904\u7406\uff1b\u540c\u65f6\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u8be5\u9884\u5904\u7406\u3002","title":"\u5982\u4f55\u589e\u52a0\u6570\u636e\u9884\u5904\u7406\uff1f"},{"location":"EXPORT_MODEL/","text":"\u6a21\u578b\u5bfc\u51fa \u8bad\u7ec3\u5f97\u5230\u4e00\u4e2a\u6ee1\u8db3\u8981\u6c42\u7684\u6a21\u578b\u540e\uff0c\u5982\u679c\u60f3\u8981\u5c06\u8be5\u6a21\u578b\u63a5\u5165\u5230C++\u9884\u6d4b\u5e93\u6216\u8005Serving\u670d\u52a1\uff0c\u9700\u8981\u901a\u8fc7 tools/export_model.py \u5bfc\u51fa\u8be5\u6a21\u578b\u3002 \u542f\u52a8\u53c2\u6570\u8bf4\u660e FLAG \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None --output_dir \u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./output \u6a21\u578b\u9ed8\u8ba4\u4fdd\u5b58\u5728 output/\u914d\u7f6e\u6587\u4ef6\u540d/ \u8def\u5f84\u4e0b \u4f7f\u7528\u793a\u4f8b \u4f7f\u7528 \u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad \u4e2d\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u8fdb\u884c\u8bd5\u7528\uff0c\u811a\u672c\u5982\u4e0b # \u5bfc\u51faFasterRCNN\u6a21\u578b, \u6a21\u578b\u4e2ddata\u5c42\u9ed8\u8ba4\u7684shape\u4e3a3x800x1333 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ \u9884\u6d4b\u6a21\u578b\u4f1a\u5bfc\u51fa\u5230 inference_model/faster_rcnn_r50_1x \u76ee\u5f55\u4e0b\uff0c\u6a21\u578b\u540d\u548c\u53c2\u6570\u540d\u5206\u522b\u4e3a __model__ \u548c __params__ \u3002 \u8bbe\u7f6e\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u5927\u5c0f \u4f7f\u7528Fluid-TensorRT\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u7531\u4e8e<=TensorRT 5.1\u7684\u7248\u672c\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u4fdd\u5b58\u6a21\u578b\u7684 data \u5c42\u7684\u56fe\u7247\u5927\u5c0f\u9700\u8981\u548c\u5b9e\u9645\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4e00\u81f4\u3002\u800cFluid C++\u9884\u6d4b\u5f15\u64ce\u6ca1\u6709\u6b64\u9650\u5236\u3002\u53ef\u901a\u8fc7\u8bbe\u7f6eTestFeed\u7684 image_shape \u53ef\u4ee5\u4fee\u6539\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u3002\u793a\u4f8b\u5982\u4e0b: # \u5bfc\u51faFasterRCNN\u6a21\u578b\uff0c\u8f93\u5165\u662f3x640x640 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ FasterRCNNTestFeed.image_shape=[3,640,640] # \u5bfc\u51faYOLOv3\u6a21\u578b\uff0c\u8f93\u5165\u662f3x320x320 python tools/export_model.py -c configs/yolov3_darknet.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_darknet.tar \\ YoloTestFeed.image_shape=[3,320,320] # \u5bfc\u51faSSD\u6a21\u578b\uff0c\u8f93\u5165\u662f3x300x300 python tools/export_model.py -c configs/ssd/ssd_mobilenet_v1_voc.yml \\ --output_dir=./inference_model \\ -o weights= https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_voc.tar \\ SSDTestFeed.image_shape=[3,300,300]","title":"\u6a21\u578b\u5bfc\u51fa"},{"location":"EXPORT_MODEL/#_1","text":"\u8bad\u7ec3\u5f97\u5230\u4e00\u4e2a\u6ee1\u8db3\u8981\u6c42\u7684\u6a21\u578b\u540e\uff0c\u5982\u679c\u60f3\u8981\u5c06\u8be5\u6a21\u578b\u63a5\u5165\u5230C++\u9884\u6d4b\u5e93\u6216\u8005Serving\u670d\u52a1\uff0c\u9700\u8981\u901a\u8fc7 tools/export_model.py \u5bfc\u51fa\u8be5\u6a21\u578b\u3002","title":"\u6a21\u578b\u5bfc\u51fa"},{"location":"EXPORT_MODEL/#_2","text":"FLAG \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None --output_dir \u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./output \u6a21\u578b\u9ed8\u8ba4\u4fdd\u5b58\u5728 output/\u914d\u7f6e\u6587\u4ef6\u540d/ \u8def\u5f84\u4e0b","title":"\u542f\u52a8\u53c2\u6570\u8bf4\u660e"},{"location":"EXPORT_MODEL/#_3","text":"\u4f7f\u7528 \u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad \u4e2d\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u8fdb\u884c\u8bd5\u7528\uff0c\u811a\u672c\u5982\u4e0b # \u5bfc\u51faFasterRCNN\u6a21\u578b, \u6a21\u578b\u4e2ddata\u5c42\u9ed8\u8ba4\u7684shape\u4e3a3x800x1333 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ \u9884\u6d4b\u6a21\u578b\u4f1a\u5bfc\u51fa\u5230 inference_model/faster_rcnn_r50_1x \u76ee\u5f55\u4e0b\uff0c\u6a21\u578b\u540d\u548c\u53c2\u6570\u540d\u5206\u522b\u4e3a __model__ \u548c __params__ \u3002","title":"\u4f7f\u7528\u793a\u4f8b"},{"location":"EXPORT_MODEL/#_4","text":"\u4f7f\u7528Fluid-TensorRT\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u7531\u4e8e<=TensorRT 5.1\u7684\u7248\u672c\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u4fdd\u5b58\u6a21\u578b\u7684 data \u5c42\u7684\u56fe\u7247\u5927\u5c0f\u9700\u8981\u548c\u5b9e\u9645\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4e00\u81f4\u3002\u800cFluid C++\u9884\u6d4b\u5f15\u64ce\u6ca1\u6709\u6b64\u9650\u5236\u3002\u53ef\u901a\u8fc7\u8bbe\u7f6eTestFeed\u7684 image_shape \u53ef\u4ee5\u4fee\u6539\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u3002\u793a\u4f8b\u5982\u4e0b: # \u5bfc\u51faFasterRCNN\u6a21\u578b\uff0c\u8f93\u5165\u662f3x640x640 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ FasterRCNNTestFeed.image_shape=[3,640,640] # \u5bfc\u51faYOLOv3\u6a21\u578b\uff0c\u8f93\u5165\u662f3x320x320 python tools/export_model.py -c configs/yolov3_darknet.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_darknet.tar \\ YoloTestFeed.image_shape=[3,320,320] # \u5bfc\u51faSSD\u6a21\u578b\uff0c\u8f93\u5165\u662f3x300x300 python tools/export_model.py -c configs/ssd/ssd_mobilenet_v1_voc.yml \\ --output_dir=./inference_model \\ -o weights= https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_voc.tar \\ SSDTestFeed.image_shape=[3,300,300]","title":"\u8bbe\u7f6e\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u5927\u5c0f"},{"location":"GETTING_STARTED/","text":"English | \u7b80\u4f53\u4e2d\u6587 Getting Started For setting up the running environment, please refer to installation instructions . Training/Evaluation/Inference PaddleDetection provides scripots for training, evalution and inference with various features according to different configure. # set PYTHONPATH export PYTHONPATH=$PYTHONPATH:. # training in single-GPU and multi-GPU. specify different GPU numbers by CUDA_VISIBLE_DEVICES export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU evalution export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # Inference python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg Optional argument list list below can be viewed by --help FLAG script supported description default remark -c ALL Select config file None The whole description of configure can refer to config_example -o ALL Set parameters in configure file None -o has higher priority to file configured by -c . Such as -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train Checkpoint path for resuming training None -r output/faster_rcnn_r50_1x/10000 --eval train Whether to perform evaluation in training False --output_eval train/eval json path in evalution current path --output_eval ./json_result -d/--dataset_dir train/eval path for dataset, same as dataset_dir in configs None -d dataset/coco --fp16 train Whether to enable mixed precision training False GPU training is required --loss_scale train Loss scaling factor for mixed precision training 8.0 enable when --fp16 is True --json_eval eval Whether to evaluate with already existed bbox.json or mask.json False json path is set in --output_eval --output_dir infer Directory for storing the output visualization files ./output --output_dir output --draw_threshold infer Threshold to reserve the result for visualization 0.5 --draw_threshold 0.7 --infer_dir infer Directory for images to perform inference on None --infer_img infer Image path None higher priority over --infer_dir --use_tb train/infer Whether to record the data with tb-paddle , so as to display in Tensorboard False --tb_log_dir train/infer tb-paddle logging directory for image train: tb_log_dir/scalar infer: tb_log_dir/image Examples Training Perform evaluation in training bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval Perform training and evalution alternatively and evaluate at each snapshot_iter. Meanwhile, the best model with highest MAP is saved at each snapshot_iter which has the same path as model_final . If evaluation dataset is large, we suggest decreasing evaluation times or evaluating after training. Fine-tune other task When using pre-trained model to fine-tune other task, two methods can be used: The excluded pre-trained parameters can be set by finetune_exclude_pretrained_params in YAML config Set -o finetune_exclude_pretrained_params in the arguments. bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params = ['cls_score','bbox_pred'] NOTES CUDA_VISIBLE_DEVICES can specify different gpu numbers. Such as: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU calculation rules can refer FAQ Dataset will be downloaded automatically and cached in ~/.cache/paddle/dataset if not be found locally. Pretrained model is downloaded automatically and cached in ~/.cache/paddle/weights . Checkpoints are saved in output by default, and can be revised from save_dir in configure files. RCNN models training on CPU is not supported on PaddlePaddle<=1.5.1 and will be fixed on later version. Mixed Precision Training Mixed precision training can be enabled with --fp16 flag. Currently Faster-FPN, Mask-FPN and Yolov3 have been verified to be working with little to no loss of precision (less than 0.2 mAP) To speed up mixed precision training, it is recommended to train in multi-process mode, for example python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml If loss becomes NaN during training, try tweak the --loss_scale value. Please refer to the Nvidia documentation on mixed precision training for details. Also, please note mixed precision training currently requires changing norm_type from affine_channel to bn . Evaluation Evaluate by specified weights path and dataset path bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco The path of model to be evaluted can be both local path and link in MODEL_ZOO . Evaluate with json bash export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ -f evaluation/ The json file must be named bbox.json or mask.json, placed in the evaluation/ directory. NOTES Multi-GPU evaluation for R-CNN and SSD models is not supported at the moment, but it is a planned feature Inference Output specified directory && Set up threshold bash export CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --use_tb=Ture --draw_threshold is an optional argument. Default is 0.5. Different thresholds will produce different results depending on the calculation of NMS . Export model bash python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ FasterRCNNTestFeed.image_shape=[3,800,1333] Save inference model tools/export_model.py , which can be loaded by PaddlePaddle predict library. FAQ Q: Why do I get NaN loss values during single GPU training? A: The default learning rate is tuned to multi-GPU training (8x GPUs), it must be adapted for single GPU training accordingly (e.g., divide by 8). The calculation rules are as follows\uff0cthey are equivalent: GPU number Learning rate Max_iters Milestones 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: How to reduce GPU memory usage? A: Setting environment variable FLAGS_conv_workspace_size_limit to a smaller number can reduce GPU memory footprint without affecting training speed. Take Mask-RCNN (R50) as example, by setting export FLAGS_conv_workspace_size_limit=512 , batch size could reach 4 per GPU (Tesla V100 16GB). Q: How to change data preprocessing? A: Set sample_transform in configuration. Note that the whole transforms need to be added in configuration. For example, DecodeImage , NormalizeImage and Permute in RCNN models. For detail description, please refer to config_example .","title":"GETTING STARTED"},{"location":"GETTING_STARTED/#getting-started","text":"For setting up the running environment, please refer to installation instructions .","title":"Getting Started"},{"location":"GETTING_STARTED/#trainingevaluationinference","text":"PaddleDetection provides scripots for training, evalution and inference with various features according to different configure. # set PYTHONPATH export PYTHONPATH=$PYTHONPATH:. # training in single-GPU and multi-GPU. specify different GPU numbers by CUDA_VISIBLE_DEVICES export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU evalution export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # Inference python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg","title":"Training/Evaluation/Inference"},{"location":"GETTING_STARTED/#optional-argument-list","text":"list below can be viewed by --help FLAG script supported description default remark -c ALL Select config file None The whole description of configure can refer to config_example -o ALL Set parameters in configure file None -o has higher priority to file configured by -c . Such as -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train Checkpoint path for resuming training None -r output/faster_rcnn_r50_1x/10000 --eval train Whether to perform evaluation in training False --output_eval train/eval json path in evalution current path --output_eval ./json_result -d/--dataset_dir train/eval path for dataset, same as dataset_dir in configs None -d dataset/coco --fp16 train Whether to enable mixed precision training False GPU training is required --loss_scale train Loss scaling factor for mixed precision training 8.0 enable when --fp16 is True --json_eval eval Whether to evaluate with already existed bbox.json or mask.json False json path is set in --output_eval --output_dir infer Directory for storing the output visualization files ./output --output_dir output --draw_threshold infer Threshold to reserve the result for visualization 0.5 --draw_threshold 0.7 --infer_dir infer Directory for images to perform inference on None --infer_img infer Image path None higher priority over --infer_dir --use_tb train/infer Whether to record the data with tb-paddle , so as to display in Tensorboard False --tb_log_dir train/infer tb-paddle logging directory for image train: tb_log_dir/scalar infer: tb_log_dir/image","title":"Optional argument list"},{"location":"GETTING_STARTED/#examples","text":"","title":"Examples"},{"location":"GETTING_STARTED/#training","text":"Perform evaluation in training bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval Perform training and evalution alternatively and evaluate at each snapshot_iter. Meanwhile, the best model with highest MAP is saved at each snapshot_iter which has the same path as model_final . If evaluation dataset is large, we suggest decreasing evaluation times or evaluating after training. Fine-tune other task When using pre-trained model to fine-tune other task, two methods can be used: The excluded pre-trained parameters can be set by finetune_exclude_pretrained_params in YAML config Set -o finetune_exclude_pretrained_params in the arguments. bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params = ['cls_score','bbox_pred']","title":"Training"},{"location":"GETTING_STARTED/#notes","text":"CUDA_VISIBLE_DEVICES can specify different gpu numbers. Such as: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU calculation rules can refer FAQ Dataset will be downloaded automatically and cached in ~/.cache/paddle/dataset if not be found locally. Pretrained model is downloaded automatically and cached in ~/.cache/paddle/weights . Checkpoints are saved in output by default, and can be revised from save_dir in configure files. RCNN models training on CPU is not supported on PaddlePaddle<=1.5.1 and will be fixed on later version.","title":"NOTES"},{"location":"GETTING_STARTED/#mixed-precision-training","text":"Mixed precision training can be enabled with --fp16 flag. Currently Faster-FPN, Mask-FPN and Yolov3 have been verified to be working with little to no loss of precision (less than 0.2 mAP) To speed up mixed precision training, it is recommended to train in multi-process mode, for example python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml If loss becomes NaN during training, try tweak the --loss_scale value. Please refer to the Nvidia documentation on mixed precision training for details. Also, please note mixed precision training currently requires changing norm_type from affine_channel to bn .","title":"Mixed Precision Training"},{"location":"GETTING_STARTED/#evaluation","text":"Evaluate by specified weights path and dataset path bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco The path of model to be evaluted can be both local path and link in MODEL_ZOO . Evaluate with json bash export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ -f evaluation/ The json file must be named bbox.json or mask.json, placed in the evaluation/ directory.","title":"Evaluation"},{"location":"GETTING_STARTED/#notes_1","text":"Multi-GPU evaluation for R-CNN and SSD models is not supported at the moment, but it is a planned feature","title":"NOTES"},{"location":"GETTING_STARTED/#inference","text":"Output specified directory && Set up threshold bash export CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --use_tb=Ture --draw_threshold is an optional argument. Default is 0.5. Different thresholds will produce different results depending on the calculation of NMS . Export model bash python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ FasterRCNNTestFeed.image_shape=[3,800,1333] Save inference model tools/export_model.py , which can be loaded by PaddlePaddle predict library.","title":"Inference"},{"location":"GETTING_STARTED/#faq","text":"Q: Why do I get NaN loss values during single GPU training? A: The default learning rate is tuned to multi-GPU training (8x GPUs), it must be adapted for single GPU training accordingly (e.g., divide by 8). The calculation rules are as follows\uff0cthey are equivalent: GPU number Learning rate Max_iters Milestones 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: How to reduce GPU memory usage? A: Setting environment variable FLAGS_conv_workspace_size_limit to a smaller number can reduce GPU memory footprint without affecting training speed. Take Mask-RCNN (R50) as example, by setting export FLAGS_conv_workspace_size_limit=512 , batch size could reach 4 per GPU (Tesla V100 16GB). Q: How to change data preprocessing? A: Set sample_transform in configuration. Note that the whole transforms need to be added in configuration. For example, DecodeImage , NormalizeImage and Permute in RCNN models. For detail description, please refer to config_example .","title":"FAQ"},{"location":"GETTING_STARTED_cn/","text":"\u5f00\u59cb \u5173\u4e8e\u914d\u7f6e\u8fd0\u884c\u73af\u5883\uff0c\u8bf7\u53c2\u8003 \u5b89\u88c5\u6307\u5357 \u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad PaddleDetection\u63d0\u4f9b\u4e86\u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad\u4e09\u4e2a\u529f\u80fd\u7684\u4f7f\u7528\u811a\u672c\uff0c\u652f\u6301\u901a\u8fc7\u4e0d\u540c\u53ef\u9009\u53c2\u6570\u5b9e\u73b0\u7279\u5b9a\u529f\u80fd # \u8bbe\u7f6ePYTHONPATH\u8def\u5f84 export PYTHONPATH=$PYTHONPATH:. # GPU\u8bad\u7ec3 \u652f\u6301\u5355\u5361\uff0c\u591a\u5361\u8bad\u7ec3\uff0c\u901a\u8fc7CUDA_VISIBLE_DEVICES\u6307\u5b9a\u5361\u53f7 export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU\u8bc4\u4f30 export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # \u63a8\u65ad python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg \u53ef\u9009\u53c2\u6570\u5217\u8868 \u4ee5\u4e0b\u5217\u8868\u53ef\u4ee5\u901a\u8fc7 --help \u67e5\u770b FLAG \u652f\u6301\u811a\u672c \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c ALL \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None \u5b8c\u6574\u914d\u7f6e\u8bf4\u660e\u8bf7\u53c2\u8003 \u914d\u7f6e\u6848\u4f8b -o ALL \u8bbe\u7f6e\u914d\u7f6e\u6587\u4ef6\u91cc\u7684\u53c2\u6570\u5185\u5bb9 None \u4f7f\u7528-o\u914d\u7f6e\u76f8\u8f83\u4e8e-c\u9009\u62e9\u7684\u914d\u7f6e\u6587\u4ef6\u5177\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\u3002\u4f8b\u5982\uff1a -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train \u4ece\u67d0\u4e00\u68c0\u67e5\u70b9\u6062\u590d\u8bad\u7ec3 None -r output/faster_rcnn_r50_1x/10000 --eval train \u662f\u5426\u8fb9\u8bad\u7ec3\u8fb9\u6d4b\u8bd5 False --output_eval train/eval \u7f16\u8f91\u8bc4\u6d4b\u4fdd\u5b58json\u8def\u5f84 \u5f53\u524d\u8def\u5f84 --output_eval ./json_result -d/--dataset_dir train/eval \u6570\u636e\u96c6\u8def\u5f84, \u540c\u914d\u7f6e\u6587\u4ef6\u91cc\u7684dataset_dir None -d dataset/coco --fp16 train \u662f\u5426\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5f0f False \u9700\u4f7f\u7528GPU\u8bad\u7ec3 --loss_scale train \u8bbe\u7f6e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5f0f\u4e2d\u635f\u5931\u503c\u7684\u7f29\u653e\u6bd4\u4f8b 8.0 \u9700\u5148\u5f00\u542f --fp16 \u540e\u4f7f\u7528 --json_eval eval \u662f\u5426\u901a\u8fc7\u5df2\u5b58\u5728\u7684bbox.json\u6216\u8005mask.json\u8fdb\u884c\u8bc4\u4f30 False json\u6587\u4ef6\u8def\u5f84\u5728 --output_eval \u4e2d\u8bbe\u7f6e --output_dir infer \u8f93\u51fa\u63a8\u65ad\u540e\u53ef\u89c6\u5316\u6587\u4ef6 ./output --output_dir output --draw_threshold infer \u53ef\u89c6\u5316\u65f6\u5206\u6570\u9608\u503c 0.5 --draw_threshold 0.7 --infer_dir infer \u7528\u4e8e\u63a8\u65ad\u7684\u56fe\u7247\u6587\u4ef6\u5939\u8def\u5f84 None --infer_img infer \u7528\u4e8e\u63a8\u65ad\u7684\u56fe\u7247\u8def\u5f84 None \u76f8\u8f83\u4e8e --infer_dir \u5177\u6709\u66f4\u9ad8\u4f18\u5148\u7ea7 --use_tb train/infer \u662f\u5426\u4f7f\u7528 tb-paddle \u8bb0\u5f55\u6570\u636e\uff0c\u8fdb\u800c\u5728TensorBoard\u4e2d\u663e\u793a False --tb_log_dir train/infer \u6307\u5b9a tb-paddle \u8bb0\u5f55\u6570\u636e\u7684\u5b58\u50a8\u8def\u5f84 train: tb_log_dir/scalar infer: tb_log_dir/image \u4f7f\u7528\u793a\u4f8b \u6a21\u578b\u8bad\u7ec3 \u8fb9\u8bad\u7ec3\u8fb9\u6d4b\u8bd5 bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval -d dataset/coco \u5728\u8bad\u7ec3\u4e2d\u4ea4\u66ff\u6267\u884c\u8bc4\u4f30, \u8bc4\u4f30\u5728\u6bcf\u4e2asnapshot_iter\u65f6\u5f00\u59cb\u3002\u6bcf\u6b21\u8bc4\u4f30\u540e\u8fd8\u4f1a\u8bc4\u51fa\u6700\u4f73mAP\u6a21\u578b\u4fdd\u5b58\u5230 best_model \u6587\u4ef6\u5939\u4e0b\u3002 \u5982\u679c\u9a8c\u8bc1\u96c6\u5f88\u5927\uff0c\u6d4b\u8bd5\u5c06\u4f1a\u6bd4\u8f83\u8017\u65f6\uff0c\u5efa\u8bae\u51cf\u5c11\u8bc4\u4f30\u6b21\u6570\uff0c\u6216\u8bad\u7ec3\u5b8c\u518d\u8fdb\u884c\u8bc4\u4f30\u3002 Fine-tune\u5176\u4ed6\u4efb\u52a1 \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578bfine-tune\u5176\u4ed6\u4efb\u52a1\u65f6\uff0c\u53ef\u91c7\u7528\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a \u5728YAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e finetune_exclude_pretrained_params \u5728\u547d\u4ee4\u884c\u4e2d\u6dfb\u52a0-o finetune_exclude_pretrained_params\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9009\u62e9\u6027\u52a0\u8f7d\u3002 bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] \u8be6\u7ec6\u8bf4\u660e\u8bf7\u53c2\u8003 Transfer Learning \u63d0\u793a CUDA_VISIBLE_DEVICES \u53c2\u6570\u53ef\u4ee5\u6307\u5b9a\u4e0d\u540c\u7684GPU\u3002\u4f8b\u5982: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU\u8ba1\u7b97\u89c4\u5219\u53ef\u4ee5\u53c2\u8003 FAQ \u82e5\u672c\u5730\u672a\u627e\u5230\u6570\u636e\u96c6\uff0c\u5c06\u81ea\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4fdd\u5b58\u5728 ~/.cache/paddle/dataset \u4e2d\u3002 \u9884\u8bad\u7ec3\u6a21\u578b\u81ea\u52a8\u4e0b\u8f7d\u5e76\u4fdd\u5b58\u5728 \u301c/.cache/paddle/weights \u4e2d\u3002 \u6a21\u578bcheckpoints\u9ed8\u8ba4\u4fdd\u5b58\u5728 output \u4e2d\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2dsave_dir\u8fdb\u884c\u914d\u7f6e\u3002 RCNN\u7cfb\u5217\u6a21\u578bCPU\u8bad\u7ec3\u5728PaddlePaddle 1.5.1\u53ca\u4ee5\u4e0b\u7248\u672c\u6682\u4e0d\u652f\u6301\u3002 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 \u901a\u8fc7\u8bbe\u7f6e --fp16 \u547d\u4ee4\u884c\u9009\u9879\u53ef\u4ee5\u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u76ee\u524d\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u5df2\u7ecf\u5728Faster-FPN, Mask-FPN \u53ca Yolov3 \u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u51e0\u4e4e\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931\uff08\u5c0f\u4e8e0.2 mAP)\u3002 \u5efa\u8bae\u4f7f\u7528\u591a\u8fdb\u7a0b\u65b9\u5f0f\u6765\u8fdb\u4e00\u6b65\u52a0\u901f\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u793a\u4f8b\u5982\u4e0b\u3002 python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml \u5982\u679c\u8bad\u7ec3\u8fc7\u7a0b\u4e2dloss\u51fa\u73b0 NaN \uff0c\u8bf7\u5c1d\u8bd5\u8c03\u8282 --loss_scale \u9009\u9879\u6570\u503c\uff0c\u7ec6\u8282\u8bf7\u53c2\u770b\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u76f8\u5173\u7684 Nvidia\u6587\u6863 \u3002 \u53e6\u5916\uff0c\u8bf7\u6ce8\u610f\u5c06\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 norm_type \u7531 affine_channel \u6539\u4e3a bn \u3002 \u6a21\u578b\u8bc4\u4f30 \u6307\u5b9a\u6743\u91cd\u548c\u6570\u636e\u96c6\u8def\u5f84 bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco \u8bc4\u4f30\u6a21\u578b\u53ef\u4ee5\u4e3a\u672c\u5730\u8def\u5f84\uff0c\u4f8b\u5982 output/faster_rcnn_r50_1x/model_final/ , \u4e5f\u53ef\u4ee5\u4e3a MODEL_ZOO \u4e2d\u7ed9\u51fa\u7684\u6a21\u578b\u94fe\u63a5\u3002 \u901a\u8fc7json\u6587\u4ef6\u8bc4\u4f30 bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ --output_eval evaluation/ json\u6587\u4ef6\u5fc5\u987b\u547d\u540d\u4e3abbox.json\u6216\u8005mask.json\uff0c\u653e\u5728 evaluation/ \u76ee\u5f55\u4e0b\u3002 \u63d0\u793a R-CNN\u548cSSD\u6a21\u578b\u76ee\u524d\u6682\u4e0d\u652f\u6301\u591aGPU\u8bc4\u4f30\uff0c\u5c06\u5728\u540e\u7eed\u7248\u672c\u652f\u6301 \u6a21\u578b\u63a8\u65ad \u8bbe\u7f6e\u8f93\u51fa\u8def\u5f84 && \u8bbe\u7f6e\u63a8\u65ad\u9608\u503c bash export CUDA_VISIBLE_DEVICES=0 python -u tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --draw_threshold \u662f\u4e2a\u53ef\u9009\u53c2\u6570. \u6839\u636e NMS \u7684\u8ba1\u7b97\uff0c \u4e0d\u540c\u9608\u503c\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u5982\u679c\u7528\u6237\u9700\u8981\u5bf9\u81ea\u5b9a\u4e49\u8def\u5f84\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u65ad\uff0c\u53ef\u4ee5\u8bbe\u7f6e -o weights \u6307\u5b9a\u6a21\u578b\u8def\u5f84\u3002 FAQ Q: \u4e3a\u4ec0\u4e48\u6211\u4f7f\u7528\u5355GPU\u8bad\u7ec3loss\u4f1a\u51fa NaN ? A: \u9ed8\u8ba4\u5b66\u4e60\u7387\u662f\u9002\u914d\u591aGPU\u8bad\u7ec3(8x GPU)\uff0c\u82e5\u4f7f\u7528\u5355GPU\u8bad\u7ec3\uff0c\u987b\u5bf9\u5e94\u8c03\u6574\u5b66\u4e60\u7387\uff08\u4f8b\u5982\uff0c\u9664\u4ee58\uff09\u3002 \u8ba1\u7b97\u89c4\u5219\u8868\u5982\u4e0b\u6240\u793a\uff0c\u5b83\u4eec\u662f\u7b49\u4ef7\u7684\uff0c\u8868\u4e2d\u53d8\u5316\u8282\u70b9\u5373\u4e3a piecewise decay \u91cc\u7684 boundaries : GPU\u6570 \u5b66\u4e60\u7387 \u6700\u5927\u8f6e\u6570 \u53d8\u5316\u8282\u70b9 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: \u5982\u4f55\u51cf\u5c11GPU\u663e\u5b58\u4f7f\u7528\u7387? A: \u53ef\u901a\u8fc7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf FLAGS_conv_workspace_size_limit \u4e3a\u8f83\u5c0f\u7684\u503c\u6765\u51cf\u5c11\u663e\u5b58\u6d88\u8017\uff0c\u5e76\u4e14\u4e0d \u4f1a\u5f71\u54cd\u8bad\u7ec3\u901f\u5ea6\u3002\u4ee5Mask-RCNN\uff08R50\uff09\u4e3a\u4f8b\uff0c\u8bbe\u7f6e export FLAGS_conv_workspace_size_limit = 512 \uff0c batch size\u53ef\u4ee5\u8fbe\u5230\u6bcfGPU 4 (Tesla V100 16GB)\u3002 Q: \u5982\u4f55\u4fee\u6539\u6570\u636e\u9884\u5904\u7406? A: \u53ef\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e sample_transform \u3002\u6ce8\u610f\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u52a0\u5165 \u5b8c\u6574\u9884\u5904\u7406 \u4f8b\u5982RCNN\u6a21\u578b\u4e2d DecodeImage , NormalizeImage and Permute \u3002\u66f4\u591a\u8be6\u7ec6\u63cf\u8ff0\u8bf7\u53c2\u8003 \u914d\u7f6e\u6848\u4f8b \u3002 Q: affine_channel\u548cbatch norm\u662f\u4ec0\u4e48\u5173\u7cfb? A: \u5728RCNN\u7cfb\u5217\u6a21\u578b\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\uff0c\u6709\u65f6\u5019\u4f1a\u56fa\u5b9a\u4f4fbatch norm\u7684\u53c2\u6570, \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u5168\u5c40\u5747\u503c\u548c\u65b9\u5f0f\uff0c\u5e76\u4e14batch norm\u7684scale\u548cbias\u53c2\u6570\u4e0d\u66f4\u65b0\uff0c\u5df2\u53d1\u5e03\u7684\u5927\u591aResNet\u7cfb\u5217\u7684RCNN\u6a21\u578b\u91c7\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5728config\u4e2d\u8bbe\u7f6enorm_type\u4e3abn\u6216affine_channel, freeze_norm\u4e3atrue (\u9ed8\u8ba4\u4e3atrue)\uff0c\u4e24\u79cd\u65b9\u5f0f\u7b49\u4ef7\u3002affne_channel\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a scale * x + bias \u3002\u53ea\u4e0d\u8fc7\u8bbe\u7f6eaffine_channel\u65f6\uff0c\u5185\u90e8\u5bf9batch norm\u7684\u53c2\u6570\u81ea\u52a8\u505a\u4e86\u878d\u5408\u3002\u5982\u679c\u8bad\u7ec3\u4f7f\u7528\u7684affine_channel\uff0c\u7528\u4fdd\u5b58\u7684\u6a21\u578b\u505a\u521d\u59cb\u5316\uff0c\u8bad\u7ec3\u5176\u4ed6\u4efb\u52a1\u65f6\uff0c\u5373\u53ef\u4f7f\u7528affine_channel, \u4e5f\u53ef\u4f7f\u7528batch norm, \u53c2\u6570\u5747\u53ef\u6b63\u786e\u52a0\u8f7d\u3002","title":"\u5f00\u59cb"},{"location":"GETTING_STARTED_cn/#_1","text":"\u5173\u4e8e\u914d\u7f6e\u8fd0\u884c\u73af\u5883\uff0c\u8bf7\u53c2\u8003 \u5b89\u88c5\u6307\u5357","title":"\u5f00\u59cb"},{"location":"GETTING_STARTED_cn/#_2","text":"PaddleDetection\u63d0\u4f9b\u4e86\u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad\u4e09\u4e2a\u529f\u80fd\u7684\u4f7f\u7528\u811a\u672c\uff0c\u652f\u6301\u901a\u8fc7\u4e0d\u540c\u53ef\u9009\u53c2\u6570\u5b9e\u73b0\u7279\u5b9a\u529f\u80fd # \u8bbe\u7f6ePYTHONPATH\u8def\u5f84 export PYTHONPATH=$PYTHONPATH:. # GPU\u8bad\u7ec3 \u652f\u6301\u5355\u5361\uff0c\u591a\u5361\u8bad\u7ec3\uff0c\u901a\u8fc7CUDA_VISIBLE_DEVICES\u6307\u5b9a\u5361\u53f7 export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU\u8bc4\u4f30 export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # \u63a8\u65ad python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg","title":"\u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad"},{"location":"GETTING_STARTED_cn/#_3","text":"\u4ee5\u4e0b\u5217\u8868\u53ef\u4ee5\u901a\u8fc7 --help \u67e5\u770b FLAG \u652f\u6301\u811a\u672c \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c ALL \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None \u5b8c\u6574\u914d\u7f6e\u8bf4\u660e\u8bf7\u53c2\u8003 \u914d\u7f6e\u6848\u4f8b -o ALL \u8bbe\u7f6e\u914d\u7f6e\u6587\u4ef6\u91cc\u7684\u53c2\u6570\u5185\u5bb9 None \u4f7f\u7528-o\u914d\u7f6e\u76f8\u8f83\u4e8e-c\u9009\u62e9\u7684\u914d\u7f6e\u6587\u4ef6\u5177\u6709\u66f4\u9ad8\u7684\u4f18\u5148\u7ea7\u3002\u4f8b\u5982\uff1a -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train \u4ece\u67d0\u4e00\u68c0\u67e5\u70b9\u6062\u590d\u8bad\u7ec3 None -r output/faster_rcnn_r50_1x/10000 --eval train \u662f\u5426\u8fb9\u8bad\u7ec3\u8fb9\u6d4b\u8bd5 False --output_eval train/eval \u7f16\u8f91\u8bc4\u6d4b\u4fdd\u5b58json\u8def\u5f84 \u5f53\u524d\u8def\u5f84 --output_eval ./json_result -d/--dataset_dir train/eval \u6570\u636e\u96c6\u8def\u5f84, \u540c\u914d\u7f6e\u6587\u4ef6\u91cc\u7684dataset_dir None -d dataset/coco --fp16 train \u662f\u5426\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5f0f False \u9700\u4f7f\u7528GPU\u8bad\u7ec3 --loss_scale train \u8bbe\u7f6e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5f0f\u4e2d\u635f\u5931\u503c\u7684\u7f29\u653e\u6bd4\u4f8b 8.0 \u9700\u5148\u5f00\u542f --fp16 \u540e\u4f7f\u7528 --json_eval eval \u662f\u5426\u901a\u8fc7\u5df2\u5b58\u5728\u7684bbox.json\u6216\u8005mask.json\u8fdb\u884c\u8bc4\u4f30 False json\u6587\u4ef6\u8def\u5f84\u5728 --output_eval \u4e2d\u8bbe\u7f6e --output_dir infer \u8f93\u51fa\u63a8\u65ad\u540e\u53ef\u89c6\u5316\u6587\u4ef6 ./output --output_dir output --draw_threshold infer \u53ef\u89c6\u5316\u65f6\u5206\u6570\u9608\u503c 0.5 --draw_threshold 0.7 --infer_dir infer \u7528\u4e8e\u63a8\u65ad\u7684\u56fe\u7247\u6587\u4ef6\u5939\u8def\u5f84 None --infer_img infer \u7528\u4e8e\u63a8\u65ad\u7684\u56fe\u7247\u8def\u5f84 None \u76f8\u8f83\u4e8e --infer_dir \u5177\u6709\u66f4\u9ad8\u4f18\u5148\u7ea7 --use_tb train/infer \u662f\u5426\u4f7f\u7528 tb-paddle \u8bb0\u5f55\u6570\u636e\uff0c\u8fdb\u800c\u5728TensorBoard\u4e2d\u663e\u793a False --tb_log_dir train/infer \u6307\u5b9a tb-paddle \u8bb0\u5f55\u6570\u636e\u7684\u5b58\u50a8\u8def\u5f84 train: tb_log_dir/scalar infer: tb_log_dir/image","title":"\u53ef\u9009\u53c2\u6570\u5217\u8868"},{"location":"GETTING_STARTED_cn/#_4","text":"","title":"\u4f7f\u7528\u793a\u4f8b"},{"location":"GETTING_STARTED_cn/#_5","text":"\u8fb9\u8bad\u7ec3\u8fb9\u6d4b\u8bd5 bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval -d dataset/coco \u5728\u8bad\u7ec3\u4e2d\u4ea4\u66ff\u6267\u884c\u8bc4\u4f30, \u8bc4\u4f30\u5728\u6bcf\u4e2asnapshot_iter\u65f6\u5f00\u59cb\u3002\u6bcf\u6b21\u8bc4\u4f30\u540e\u8fd8\u4f1a\u8bc4\u51fa\u6700\u4f73mAP\u6a21\u578b\u4fdd\u5b58\u5230 best_model \u6587\u4ef6\u5939\u4e0b\u3002 \u5982\u679c\u9a8c\u8bc1\u96c6\u5f88\u5927\uff0c\u6d4b\u8bd5\u5c06\u4f1a\u6bd4\u8f83\u8017\u65f6\uff0c\u5efa\u8bae\u51cf\u5c11\u8bc4\u4f30\u6b21\u6570\uff0c\u6216\u8bad\u7ec3\u5b8c\u518d\u8fdb\u884c\u8bc4\u4f30\u3002 Fine-tune\u5176\u4ed6\u4efb\u52a1 \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578bfine-tune\u5176\u4ed6\u4efb\u52a1\u65f6\uff0c\u53ef\u91c7\u7528\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a \u5728YAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e finetune_exclude_pretrained_params \u5728\u547d\u4ee4\u884c\u4e2d\u6dfb\u52a0-o finetune_exclude_pretrained_params\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9009\u62e9\u6027\u52a0\u8f7d\u3002 bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] \u8be6\u7ec6\u8bf4\u660e\u8bf7\u53c2\u8003 Transfer Learning","title":"\u6a21\u578b\u8bad\u7ec3"},{"location":"GETTING_STARTED_cn/#_6","text":"CUDA_VISIBLE_DEVICES \u53c2\u6570\u53ef\u4ee5\u6307\u5b9a\u4e0d\u540c\u7684GPU\u3002\u4f8b\u5982: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU\u8ba1\u7b97\u89c4\u5219\u53ef\u4ee5\u53c2\u8003 FAQ \u82e5\u672c\u5730\u672a\u627e\u5230\u6570\u636e\u96c6\uff0c\u5c06\u81ea\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\u5e76\u4fdd\u5b58\u5728 ~/.cache/paddle/dataset \u4e2d\u3002 \u9884\u8bad\u7ec3\u6a21\u578b\u81ea\u52a8\u4e0b\u8f7d\u5e76\u4fdd\u5b58\u5728 \u301c/.cache/paddle/weights \u4e2d\u3002 \u6a21\u578bcheckpoints\u9ed8\u8ba4\u4fdd\u5b58\u5728 output \u4e2d\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u4e2dsave_dir\u8fdb\u884c\u914d\u7f6e\u3002 RCNN\u7cfb\u5217\u6a21\u578bCPU\u8bad\u7ec3\u5728PaddlePaddle 1.5.1\u53ca\u4ee5\u4e0b\u7248\u672c\u6682\u4e0d\u652f\u6301\u3002","title":"\u63d0\u793a"},{"location":"GETTING_STARTED_cn/#_7","text":"\u901a\u8fc7\u8bbe\u7f6e --fp16 \u547d\u4ee4\u884c\u9009\u9879\u53ef\u4ee5\u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u76ee\u524d\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u5df2\u7ecf\u5728Faster-FPN, Mask-FPN \u53ca Yolov3 \u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u51e0\u4e4e\u6ca1\u6709\u7cbe\u5ea6\u635f\u5931\uff08\u5c0f\u4e8e0.2 mAP)\u3002 \u5efa\u8bae\u4f7f\u7528\u591a\u8fdb\u7a0b\u65b9\u5f0f\u6765\u8fdb\u4e00\u6b65\u52a0\u901f\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u793a\u4f8b\u5982\u4e0b\u3002 python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml \u5982\u679c\u8bad\u7ec3\u8fc7\u7a0b\u4e2dloss\u51fa\u73b0 NaN \uff0c\u8bf7\u5c1d\u8bd5\u8c03\u8282 --loss_scale \u9009\u9879\u6570\u503c\uff0c\u7ec6\u8282\u8bf7\u53c2\u770b\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u76f8\u5173\u7684 Nvidia\u6587\u6863 \u3002 \u53e6\u5916\uff0c\u8bf7\u6ce8\u610f\u5c06\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 norm_type \u7531 affine_channel \u6539\u4e3a bn \u3002","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3"},{"location":"GETTING_STARTED_cn/#_8","text":"\u6307\u5b9a\u6743\u91cd\u548c\u6570\u636e\u96c6\u8def\u5f84 bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco \u8bc4\u4f30\u6a21\u578b\u53ef\u4ee5\u4e3a\u672c\u5730\u8def\u5f84\uff0c\u4f8b\u5982 output/faster_rcnn_r50_1x/model_final/ , \u4e5f\u53ef\u4ee5\u4e3a MODEL_ZOO \u4e2d\u7ed9\u51fa\u7684\u6a21\u578b\u94fe\u63a5\u3002 \u901a\u8fc7json\u6587\u4ef6\u8bc4\u4f30 bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ --output_eval evaluation/ json\u6587\u4ef6\u5fc5\u987b\u547d\u540d\u4e3abbox.json\u6216\u8005mask.json\uff0c\u653e\u5728 evaluation/ \u76ee\u5f55\u4e0b\u3002","title":"\u6a21\u578b\u8bc4\u4f30"},{"location":"GETTING_STARTED_cn/#_9","text":"R-CNN\u548cSSD\u6a21\u578b\u76ee\u524d\u6682\u4e0d\u652f\u6301\u591aGPU\u8bc4\u4f30\uff0c\u5c06\u5728\u540e\u7eed\u7248\u672c\u652f\u6301","title":"\u63d0\u793a"},{"location":"GETTING_STARTED_cn/#_10","text":"\u8bbe\u7f6e\u8f93\u51fa\u8def\u5f84 && \u8bbe\u7f6e\u63a8\u65ad\u9608\u503c bash export CUDA_VISIBLE_DEVICES=0 python -u tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --draw_threshold \u662f\u4e2a\u53ef\u9009\u53c2\u6570. \u6839\u636e NMS \u7684\u8ba1\u7b97\uff0c \u4e0d\u540c\u9608\u503c\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u5982\u679c\u7528\u6237\u9700\u8981\u5bf9\u81ea\u5b9a\u4e49\u8def\u5f84\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u65ad\uff0c\u53ef\u4ee5\u8bbe\u7f6e -o weights \u6307\u5b9a\u6a21\u578b\u8def\u5f84\u3002","title":"\u6a21\u578b\u63a8\u65ad"},{"location":"GETTING_STARTED_cn/#faq","text":"Q: \u4e3a\u4ec0\u4e48\u6211\u4f7f\u7528\u5355GPU\u8bad\u7ec3loss\u4f1a\u51fa NaN ? A: \u9ed8\u8ba4\u5b66\u4e60\u7387\u662f\u9002\u914d\u591aGPU\u8bad\u7ec3(8x GPU)\uff0c\u82e5\u4f7f\u7528\u5355GPU\u8bad\u7ec3\uff0c\u987b\u5bf9\u5e94\u8c03\u6574\u5b66\u4e60\u7387\uff08\u4f8b\u5982\uff0c\u9664\u4ee58\uff09\u3002 \u8ba1\u7b97\u89c4\u5219\u8868\u5982\u4e0b\u6240\u793a\uff0c\u5b83\u4eec\u662f\u7b49\u4ef7\u7684\uff0c\u8868\u4e2d\u53d8\u5316\u8282\u70b9\u5373\u4e3a piecewise decay \u91cc\u7684 boundaries : GPU\u6570 \u5b66\u4e60\u7387 \u6700\u5927\u8f6e\u6570 \u53d8\u5316\u8282\u70b9 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: \u5982\u4f55\u51cf\u5c11GPU\u663e\u5b58\u4f7f\u7528\u7387? A: \u53ef\u901a\u8fc7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf FLAGS_conv_workspace_size_limit \u4e3a\u8f83\u5c0f\u7684\u503c\u6765\u51cf\u5c11\u663e\u5b58\u6d88\u8017\uff0c\u5e76\u4e14\u4e0d \u4f1a\u5f71\u54cd\u8bad\u7ec3\u901f\u5ea6\u3002\u4ee5Mask-RCNN\uff08R50\uff09\u4e3a\u4f8b\uff0c\u8bbe\u7f6e export FLAGS_conv_workspace_size_limit = 512 \uff0c batch size\u53ef\u4ee5\u8fbe\u5230\u6bcfGPU 4 (Tesla V100 16GB)\u3002 Q: \u5982\u4f55\u4fee\u6539\u6570\u636e\u9884\u5904\u7406? A: \u53ef\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e sample_transform \u3002\u6ce8\u610f\u9700\u8981\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u52a0\u5165 \u5b8c\u6574\u9884\u5904\u7406 \u4f8b\u5982RCNN\u6a21\u578b\u4e2d DecodeImage , NormalizeImage and Permute \u3002\u66f4\u591a\u8be6\u7ec6\u63cf\u8ff0\u8bf7\u53c2\u8003 \u914d\u7f6e\u6848\u4f8b \u3002 Q: affine_channel\u548cbatch norm\u662f\u4ec0\u4e48\u5173\u7cfb? A: \u5728RCNN\u7cfb\u5217\u6a21\u578b\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\uff0c\u6709\u65f6\u5019\u4f1a\u56fa\u5b9a\u4f4fbatch norm\u7684\u53c2\u6570, \u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u5168\u5c40\u5747\u503c\u548c\u65b9\u5f0f\uff0c\u5e76\u4e14batch norm\u7684scale\u548cbias\u53c2\u6570\u4e0d\u66f4\u65b0\uff0c\u5df2\u53d1\u5e03\u7684\u5927\u591aResNet\u7cfb\u5217\u7684RCNN\u6a21\u578b\u91c7\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5728config\u4e2d\u8bbe\u7f6enorm_type\u4e3abn\u6216affine_channel, freeze_norm\u4e3atrue (\u9ed8\u8ba4\u4e3atrue)\uff0c\u4e24\u79cd\u65b9\u5f0f\u7b49\u4ef7\u3002affne_channel\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e3a scale * x + bias \u3002\u53ea\u4e0d\u8fc7\u8bbe\u7f6eaffine_channel\u65f6\uff0c\u5185\u90e8\u5bf9batch norm\u7684\u53c2\u6570\u81ea\u52a8\u505a\u4e86\u878d\u5408\u3002\u5982\u679c\u8bad\u7ec3\u4f7f\u7528\u7684affine_channel\uff0c\u7528\u4fdd\u5b58\u7684\u6a21\u578b\u505a\u521d\u59cb\u5316\uff0c\u8bad\u7ec3\u5176\u4ed6\u4efb\u52a1\u65f6\uff0c\u5373\u53ef\u4f7f\u7528affine_channel, \u4e5f\u53ef\u4f7f\u7528batch norm, \u53c2\u6570\u5747\u53ef\u6b63\u786e\u52a0\u8f7d\u3002","title":"FAQ"},{"location":"INSTALL/","text":"English | \u7b80\u4f53\u4e2d\u6587 Installation Table of Contents Introduction PaddlePaddle Other Dependencies PaddleDetection Datasets Introduction This document covers how to install PaddleDetection, its dependencies (including PaddlePaddle), together with COCO and Pascal VOC dataset. For general information about PaddleDetection, please see README.md . PaddlePaddle Running PaddleDetection requires PaddlePaddle Fluid v.1.6 and later. please follow the instructions in installation document . Please make sure your PaddlePaddle installation was successful and the version of your PaddlePaddle is not lower than required. Verify with the following commands. # To check PaddlePaddle installation in your Python interpreter >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # To check PaddlePaddle version python -c \"import paddle; print(paddle.__version__)\" Requirements: Python2 or Python3 (Only support Python3 for windows) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2 Other Dependencies COCO-API : COCO-API is needed for running. Installation is as follows: git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # if cython is not installed pip install Cython # Install into global site-packages make install # Alternatively, if you do not have permissions or prefer # not to install the COCO API into global site-packages python setup.py install --user Installation of COCO-API in windows: # if cython is not installed pip install Cython # Because the origin version of cocoapi does not support windows, another version is used which only supports Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI PaddleDetection Clone Paddle models repository: You can clone PaddleDetection with the following commands: cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git Install Python dependencies: Required python packages are specified in requirements.txt , and can be installed with: pip install -r requirements.txt Make sure the tests pass: export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py Datasets PaddleDetection includes support for COCO and Pascal VOC by default, please follow these instructions to set up the dataset. Create symlinks for local datasets: Default dataset path in config files is dataset/coco and dataset/voc , if the datasets are already available on disk, you can simply create symlinks to their directories: ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc For Pascal VOC dataset, you should create file list by: export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py Download datasets manually: On the other hand, to download the datasets, run the following commands: COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO dataset with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC dataset with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Download datasets automatically: If a training session is started but the dataset is not setup properly (e.g, not found in dataset/coco or dataset/voc ), PaddleDetection can automatically download them from COCO-2017 and VOC2012 , the decompressed datasets will be cached in ~/.cache/paddle/dataset/ and can be discovered automatically subsequently. NOTE: For further informations on the datasets, please see DATA.md","title":"INSTALL"},{"location":"INSTALL/#installation","text":"","title":"Installation"},{"location":"INSTALL/#table-of-contents","text":"Introduction PaddlePaddle Other Dependencies PaddleDetection Datasets","title":"Table of Contents"},{"location":"INSTALL/#introduction","text":"This document covers how to install PaddleDetection, its dependencies (including PaddlePaddle), together with COCO and Pascal VOC dataset. For general information about PaddleDetection, please see README.md .","title":"Introduction"},{"location":"INSTALL/#paddlepaddle","text":"Running PaddleDetection requires PaddlePaddle Fluid v.1.6 and later. please follow the instructions in installation document . Please make sure your PaddlePaddle installation was successful and the version of your PaddlePaddle is not lower than required. Verify with the following commands. # To check PaddlePaddle installation in your Python interpreter >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # To check PaddlePaddle version python -c \"import paddle; print(paddle.__version__)\"","title":"PaddlePaddle"},{"location":"INSTALL/#requirements","text":"Python2 or Python3 (Only support Python3 for windows) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2","title":"Requirements:"},{"location":"INSTALL/#other-dependencies","text":"COCO-API : COCO-API is needed for running. Installation is as follows: git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # if cython is not installed pip install Cython # Install into global site-packages make install # Alternatively, if you do not have permissions or prefer # not to install the COCO API into global site-packages python setup.py install --user Installation of COCO-API in windows: # if cython is not installed pip install Cython # Because the origin version of cocoapi does not support windows, another version is used which only supports Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI","title":"Other Dependencies"},{"location":"INSTALL/#paddledetection","text":"Clone Paddle models repository: You can clone PaddleDetection with the following commands: cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git Install Python dependencies: Required python packages are specified in requirements.txt , and can be installed with: pip install -r requirements.txt Make sure the tests pass: export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py","title":"PaddleDetection"},{"location":"INSTALL/#datasets","text":"PaddleDetection includes support for COCO and Pascal VOC by default, please follow these instructions to set up the dataset. Create symlinks for local datasets: Default dataset path in config files is dataset/coco and dataset/voc , if the datasets are already available on disk, you can simply create symlinks to their directories: ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc For Pascal VOC dataset, you should create file list by: export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py Download datasets manually: On the other hand, to download the datasets, run the following commands: COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO dataset with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC dataset with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Download datasets automatically: If a training session is started but the dataset is not setup properly (e.g, not found in dataset/coco or dataset/voc ), PaddleDetection can automatically download them from COCO-2017 and VOC2012 , the decompressed datasets will be cached in ~/.cache/paddle/dataset/ and can be discovered automatically subsequently. NOTE: For further informations on the datasets, please see DATA.md","title":"Datasets"},{"location":"INSTALL_cn/","text":"\u5b89\u88c5\u6587\u6863 \u76ee\u5f55 \u7b80\u4ecb PaddlePaddle \u5176\u4ed6\u4f9d\u8d56\u5b89\u88c5 PaddleDetection \u6570\u636e\u96c6 \u7b80\u4ecb \u8fd9\u4efd\u6587\u6863\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5PaddleDetection\u53ca\u5176\u4f9d\u8d56\u9879(\u5305\u62ecPaddlePaddle)\uff0c\u4ee5\u53caCOCO\u548cPascal VOC\u6570\u636e\u96c6\u3002 PaddleDetection\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003 README.md . PaddlePaddle \u8fd0\u884cPaddleDetection\u9700\u8981PaddlePaddle Fluid v.1.6\u53ca\u66f4\u9ad8\u7248\u672c\u3002\u8bf7\u6309\u7167 \u5b89\u88c5\u6587\u6863 \u4e2d\u7684\u8bf4\u660e\u8fdb\u884c\u64cd\u4f5c\u3002 \u8bf7\u786e\u4fdd\u60a8\u7684PaddlePaddle\u5b89\u88c5\u6210\u529f\u5e76\u4e14\u7248\u672c\u4e0d\u4f4e\u4e8e\u9700\u6c42\u7248\u672c\u3002\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u9a8c\u8bc1\u3002 # \u5728\u60a8\u7684Python\u89e3\u91ca\u5668\u4e2d\u786e\u8ba4PaddlePaddle\u5b89\u88c5\u6210\u529f >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # \u786e\u8ba4PaddlePaddle\u7248\u672c python -c \"import paddle; print(paddle.__version__)\" \u73af\u5883\u9700\u6c42: Python2 or Python3 (windows\u7cfb\u7edf\u4ec5\u652f\u6301Python3) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2 \u5176\u4ed6\u4f9d\u8d56\u5b89\u88c5 COCO-API : \u8fd0\u884c\u9700\u8981COCO-API\uff0c\u5b89\u88c5\u65b9\u5f0f\u5982\u4e0b\uff1a git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # \u82e5Cython\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5Cython pip install Cython # \u5b89\u88c5\u81f3\u5168\u5c40site-packages make install # \u82e5\u60a8\u6ca1\u6709\u6743\u9650\u6216\u66f4\u503e\u5411\u4e0d\u5b89\u88c5\u81f3\u5168\u5c40site-packages python setup.py install --user windows\u7528\u6237\u5b89\u88c5COCO-API\u65b9\u5f0f\uff1a # \u82e5Cython\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5Cython pip install Cython # \u7531\u4e8e\u539f\u7248cocoapi\u4e0d\u652f\u6301windows\uff0c\u91c7\u7528\u7b2c\u4e09\u65b9\u5b9e\u73b0\u7248\u672c\uff0c\u8be5\u7248\u672c\u4ec5\u652f\u6301Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI PaddleDetection \u514b\u9686Paddle models\u6a21\u578b\u5e93\uff1a \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u514b\u9686PaddleDetection\uff1a cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git \u5b89\u88c5Python\u4f9d\u8d56\u5e93\uff1a Python\u4f9d\u8d56\u5e93\u5728 requirements.txt \u4e2d\u7ed9\u51fa\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a pip install -r requirements.txt \u786e\u8ba4\u6d4b\u8bd5\u901a\u8fc7\uff1a export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py \u6570\u636e\u96c6 PaddleDetection\u9ed8\u8ba4\u652f\u6301 COCO \u548c Pascal VOC \uff0c \u8bf7\u6309\u7167\u5982\u4e0b\u6b65\u9aa4\u8bbe\u7f6e\u6570\u636e\u96c6\u3002 \u4e3a\u672c\u5730\u6570\u636e\u96c6\u521b\u5efa\u8f6f\u94fe\u63a5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u9ed8\u8ba4\u7684\u6570\u636e\u96c6\u8def\u5f84\u662f dataset/coco \u548c dataset/voc \uff0c\u5982\u679c\u60a8\u672c\u5730\u78c1\u76d8\u4e0a\u5df2\u6709\u6570\u636e\u96c6\uff0c \u53ea\u9700\u521b\u5efa\u8f6f\u94fe\u63a5\u81f3\u6570\u636e\u96c6\u76ee\u5f55\uff1a ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc \u5bf9\u4e8ePascal VOC\u6570\u636e\u96c6\uff0c\u9700\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6587\u4ef6\u5217\u8868\uff1a export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py \u624b\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\uff1a \u82e5\u60a8\u672c\u5730\u6ca1\u6709\u6570\u636e\u96c6\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\uff1a COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO \u6570\u636e\u96c6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC \u6570\u636e\u96c6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... \u8bf4\u660e\uff1a \u5982\u679c\u4f60\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e use_default_label=False , \u5c06\u4ece label_list.txt \u4e2d\u8bfb\u53d6\u7c7b\u522b\u5217\u8868\uff0c\u53cd\u4e4b\u5219\u53ef\u4ee5\u6ca1\u6709 label_list.txt \u6587\u4ef6\uff0c\u68c0\u6d4b\u5e93\u4f1a\u4f7f\u7528Pascal VOC\u6570\u636e\u96c6\u7684\u9ed8 \u8ba4\u7c7b\u522b\u5217\u8868\uff0c\u9ed8\u8ba4\u7c7b\u522b\u5217\u8868\u5b9a\u4e49\u5728 voc_loader.py \u81ea\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\uff1a \u82e5\u60a8\u5728\u6570\u636e\u96c6\u672a\u6210\u529f\u8bbe\u7f6e\uff08\u4f8b\u5982\uff0c\u5728 dataset/coco \u6216 dataset/voc \u4e2d\u627e\u4e0d\u5230\uff09\u7684\u60c5\u51b5\u4e0b\u5f00\u59cb\u8fd0\u884c\uff0c PaddleDetection\u5c06\u81ea\u52a8\u4ece COCO-2017 \u6216 VOC2012 \u4e0b\u8f7d\uff0c\u89e3\u538b\u540e\u7684\u6570\u636e\u96c6\u5c06\u88ab\u4fdd\u5b58\u5728 \u301c/.cache/paddle/dataset/ \u76ee\u5f55\u4e0b\uff0c\u4e0b\u6b21\u8fd0\u884c\u65f6\uff0c\u4e5f\u53ef\u81ea\u52a8\u4ece\u8be5\u76ee\u5f55\u53d1\u73b0\u6570\u636e\u96c6\u3002 \u8bf4\u660e\uff1a \u66f4\u591a\u6709\u5173\u6570\u636e\u96c6\u7684\u4ecb\u7ecd\uff0c\u8bf7\u53c2\u8003 DATA.md","title":"\u5b89\u88c5\u6587\u6863"},{"location":"INSTALL_cn/#_1","text":"","title":"\u5b89\u88c5\u6587\u6863"},{"location":"INSTALL_cn/#_2","text":"\u7b80\u4ecb PaddlePaddle \u5176\u4ed6\u4f9d\u8d56\u5b89\u88c5 PaddleDetection \u6570\u636e\u96c6","title":"\u76ee\u5f55"},{"location":"INSTALL_cn/#_3","text":"\u8fd9\u4efd\u6587\u6863\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5PaddleDetection\u53ca\u5176\u4f9d\u8d56\u9879(\u5305\u62ecPaddlePaddle)\uff0c\u4ee5\u53caCOCO\u548cPascal VOC\u6570\u636e\u96c6\u3002 PaddleDetection\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003 README.md .","title":"\u7b80\u4ecb"},{"location":"INSTALL_cn/#paddlepaddle","text":"\u8fd0\u884cPaddleDetection\u9700\u8981PaddlePaddle Fluid v.1.6\u53ca\u66f4\u9ad8\u7248\u672c\u3002\u8bf7\u6309\u7167 \u5b89\u88c5\u6587\u6863 \u4e2d\u7684\u8bf4\u660e\u8fdb\u884c\u64cd\u4f5c\u3002 \u8bf7\u786e\u4fdd\u60a8\u7684PaddlePaddle\u5b89\u88c5\u6210\u529f\u5e76\u4e14\u7248\u672c\u4e0d\u4f4e\u4e8e\u9700\u6c42\u7248\u672c\u3002\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u9a8c\u8bc1\u3002 # \u5728\u60a8\u7684Python\u89e3\u91ca\u5668\u4e2d\u786e\u8ba4PaddlePaddle\u5b89\u88c5\u6210\u529f >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # \u786e\u8ba4PaddlePaddle\u7248\u672c python -c \"import paddle; print(paddle.__version__)\"","title":"PaddlePaddle"},{"location":"INSTALL_cn/#_4","text":"Python2 or Python3 (windows\u7cfb\u7edf\u4ec5\u652f\u6301Python3) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2","title":"\u73af\u5883\u9700\u6c42:"},{"location":"INSTALL_cn/#_5","text":"COCO-API : \u8fd0\u884c\u9700\u8981COCO-API\uff0c\u5b89\u88c5\u65b9\u5f0f\u5982\u4e0b\uff1a git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # \u82e5Cython\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5Cython pip install Cython # \u5b89\u88c5\u81f3\u5168\u5c40site-packages make install # \u82e5\u60a8\u6ca1\u6709\u6743\u9650\u6216\u66f4\u503e\u5411\u4e0d\u5b89\u88c5\u81f3\u5168\u5c40site-packages python setup.py install --user windows\u7528\u6237\u5b89\u88c5COCO-API\u65b9\u5f0f\uff1a # \u82e5Cython\u672a\u5b89\u88c5\uff0c\u8bf7\u5b89\u88c5Cython pip install Cython # \u7531\u4e8e\u539f\u7248cocoapi\u4e0d\u652f\u6301windows\uff0c\u91c7\u7528\u7b2c\u4e09\u65b9\u5b9e\u73b0\u7248\u672c\uff0c\u8be5\u7248\u672c\u4ec5\u652f\u6301Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI","title":"\u5176\u4ed6\u4f9d\u8d56\u5b89\u88c5"},{"location":"INSTALL_cn/#paddledetection","text":"\u514b\u9686Paddle models\u6a21\u578b\u5e93\uff1a \u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u514b\u9686PaddleDetection\uff1a cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git \u5b89\u88c5Python\u4f9d\u8d56\u5e93\uff1a Python\u4f9d\u8d56\u5e93\u5728 requirements.txt \u4e2d\u7ed9\u51fa\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a pip install -r requirements.txt \u786e\u8ba4\u6d4b\u8bd5\u901a\u8fc7\uff1a export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py","title":"PaddleDetection"},{"location":"INSTALL_cn/#_6","text":"PaddleDetection\u9ed8\u8ba4\u652f\u6301 COCO \u548c Pascal VOC \uff0c \u8bf7\u6309\u7167\u5982\u4e0b\u6b65\u9aa4\u8bbe\u7f6e\u6570\u636e\u96c6\u3002 \u4e3a\u672c\u5730\u6570\u636e\u96c6\u521b\u5efa\u8f6f\u94fe\u63a5\uff1a \u914d\u7f6e\u6587\u4ef6\u4e2d\u9ed8\u8ba4\u7684\u6570\u636e\u96c6\u8def\u5f84\u662f dataset/coco \u548c dataset/voc \uff0c\u5982\u679c\u60a8\u672c\u5730\u78c1\u76d8\u4e0a\u5df2\u6709\u6570\u636e\u96c6\uff0c \u53ea\u9700\u521b\u5efa\u8f6f\u94fe\u63a5\u81f3\u6570\u636e\u96c6\u76ee\u5f55\uff1a ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc \u5bf9\u4e8ePascal VOC\u6570\u636e\u96c6\uff0c\u9700\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u6587\u4ef6\u5217\u8868\uff1a export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py \u624b\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\uff1a \u82e5\u60a8\u672c\u5730\u6ca1\u6709\u6570\u636e\u96c6\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\uff1a COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO \u6570\u636e\u96c6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC \u6570\u636e\u96c6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... \u8bf4\u660e\uff1a \u5982\u679c\u4f60\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6e use_default_label=False , \u5c06\u4ece label_list.txt \u4e2d\u8bfb\u53d6\u7c7b\u522b\u5217\u8868\uff0c\u53cd\u4e4b\u5219\u53ef\u4ee5\u6ca1\u6709 label_list.txt \u6587\u4ef6\uff0c\u68c0\u6d4b\u5e93\u4f1a\u4f7f\u7528Pascal VOC\u6570\u636e\u96c6\u7684\u9ed8 \u8ba4\u7c7b\u522b\u5217\u8868\uff0c\u9ed8\u8ba4\u7c7b\u522b\u5217\u8868\u5b9a\u4e49\u5728 voc_loader.py \u81ea\u52a8\u4e0b\u8f7d\u6570\u636e\u96c6\uff1a \u82e5\u60a8\u5728\u6570\u636e\u96c6\u672a\u6210\u529f\u8bbe\u7f6e\uff08\u4f8b\u5982\uff0c\u5728 dataset/coco \u6216 dataset/voc \u4e2d\u627e\u4e0d\u5230\uff09\u7684\u60c5\u51b5\u4e0b\u5f00\u59cb\u8fd0\u884c\uff0c PaddleDetection\u5c06\u81ea\u52a8\u4ece COCO-2017 \u6216 VOC2012 \u4e0b\u8f7d\uff0c\u89e3\u538b\u540e\u7684\u6570\u636e\u96c6\u5c06\u88ab\u4fdd\u5b58\u5728 \u301c/.cache/paddle/dataset/ \u76ee\u5f55\u4e0b\uff0c\u4e0b\u6b21\u8fd0\u884c\u65f6\uff0c\u4e5f\u53ef\u81ea\u52a8\u4ece\u8be5\u76ee\u5f55\u53d1\u73b0\u6570\u636e\u96c6\u3002 \u8bf4\u660e\uff1a \u66f4\u591a\u6709\u5173\u6570\u636e\u96c6\u7684\u4ecb\u7ecd\uff0c\u8bf7\u53c2\u8003 DATA.md","title":"\u6570\u636e\u96c6"},{"location":"MODEL_ZOO/","text":"English | \u7b80\u4f53\u4e2d\u6587 Model Zoo and Benchmark Environment Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2 Common settings All models below were trained on coco_2017_train , and tested on coco_2017_val . Batch Normalization layers in backbones are replaced by Affine Channel layers. Unless otherwise noted, all ResNet backbones adopt the ResNet-B variant.. For RCNN and RetinaNet models, only horizontal flipping data augmentation was used in the training phase and no augmentations were used in the testing phase. Inf time (fps) : the inference time is measured with fps (image/s) on a single GPU (Tesla V100) with cuDNN 7.5 by running 'tools/eval.py' on all validation set, which including data loadding, network forward and post processing. The batch size is 1. Training Schedules We adopt exactly the same training schedules as Detectron . 1x indicates the schedule starts at a LR of 0.02 and is decreased by a factor of 10 after 60k and 80k iterations and eventually terminates at 90k iterations for minibatch size 16. For batch size 8, LR is decreased to 0.01, total training iterations are doubled, and the decay milestones are scaled by 2. 2x schedule is twice as long as 1x, with the LR milestones scaled accordingly. ImageNet Pretrained Models The backbone models pretrained on ImageNet are available. All backbone models are pretrained on standard ImageNet-1k dataset and can be downloaded here . Notes: The ResNet50 model was trained with cosine LR decay schedule and can be downloaded here . Baselines Faster & Mask R-CNN Backbone Type Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50 Faster 1 1x 12.747 35.2 - model ResNet50 Faster 1 2x 12.686 37.1 - model ResNet50 Mask 1 1x 11.615 36.5 32.2 model ResNet50 Mask 1 2x 11.494 38.2 33.4 model ResNet50-vd Faster 1 1x 12.575 36.4 - model ResNet50-FPN Faster 2 1x 22.273 37.2 - model ResNet50-FPN Faster 2 2x 22.297 37.7 - model ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 model ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 model ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - model ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 model ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - model ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 model CBResNet50-vd-FPN Faster 2 1x - 39.7 - model ResNet101 Faster 1 1x 9.316 38.3 - model ResNet101-FPN Faster 1 1x 17.297 38.7 - model ResNet101-FPN Faster 1 2x 17.246 39.1 - model ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 model ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - model ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - model ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 model CBResNet101-vd-FPN Faster 2 1x - 42.7 - model ResNeXt101-vd-64x4d-FPN Faster 1 1x 8.815 42.2 - model ResNeXt101-vd-64x4d-FPN Faster 1 2x 8.809 41.7 - model ResNeXt101-vd-64x4d-FPN Mask 1 1x 7.689 42.9 37.9 model ResNeXt101-vd-64x4d-FPN Mask 1 2x 7.859 42.6 37.6 model SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - model SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 model ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - model Deformable ConvNets v2 Backbone Type Conv Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - model ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - model ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - model ResNeXt101-vd-64x4d-FPN Faster c3-c5 1 1x 7.209 45.2 - model ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 model ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 model ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 model ResNeXt101-vd-64x4d-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 model ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - model ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - model ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - model SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 model ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - model CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - model Notes: Deformable ConvNets v2(dcn_v2) reference from Deformable ConvNets v2 . c3-c5 means adding dcn in resnet stage 3 to 5. Detailed configuration file in configs/dcn Group Normalization Backbone Type Image/gpu Lr schd Box AP Mask AP Download ResNet50-FPN Faster 2 2x 39.7 - model ResNet50-FPN Mask 1 2x 40.1 35.8 model Notes: Group Normalization reference from Group Normalization . Detailed configuration file in configs/gn YOLO v3 Backbone Pretrain dataset Size deformable Conv Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 (paper) ImageNet 608 False 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 False 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 False 8 270e - 28.2 - DarkNet53 ImageNet 608 False 8 270e 45.571 38.9 model DarkNet53 ImageNet 416 False 8 270e - 37.5 model DarkNet53 ImageNet 320 False 8 270e - 34.8 model MobileNet-V1 ImageNet 608 False 8 270e 78.302 29.3 model MobileNet-V1 ImageNet 416 False 8 270e - 29.3 model MobileNet-V1 ImageNet 320 False 8 270e - 27.1 model ResNet34 ImageNet 608 False 8 270e 63.356 36.2 model ResNet34 ImageNet 416 False 8 270e - 34.3 model ResNet34 ImageNet 320 False 8 270e - 31.4 model ResNet50_vd ImageNet 608 True 8 270e - 39.1 model ResNet50_vd Object365 608 True 8 270e - 41.4 model YOLO v3 on Pascal VOC Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 608 8 270e 54.977 83.5 model DarkNet53 416 8 270e - 83.6 model DarkNet53 320 8 270e - 82.2 model MobileNet-V1 608 8 270e 104.291 76.2 model MobileNet-V1 416 8 270e - 76.7 model MobileNet-V1 320 8 270e - 75.3 model ResNet34 608 8 270e 82.247 82.6 model ResNet34 416 8 270e - 81.9 model ResNet34 320 8 270e - 80.1 model Notes: YOLOv3-DarkNet53 performance in paper YOLOv3 is also provided above, our implements improved performance mainly by using L1 loss in bounding box width and height regression, image mixup and label smooth. YOLO v3 is trained in 8 GPU with total batch size as 64 and trained 270 epoches. YOLO v3 training data augmentations: mixup, randomly color distortion, randomly cropping, randomly expansion, randomly interpolation method, randomly flippling. YOLO v3 used randomly reshaped minibatch in training, inferences can be performed on different image sizes with the same model weights, and we provided evaluation results of image size 608/416/320 above. Deformable conv is added on stage 5 of backbone. RetinaNet Backbone Image/gpu Lr schd Box AP Download ResNet50-FPN 2 1x 36.0 model ResNet101-FPN 2 1x 37.3 model ResNeXt101-vd-FPN 1 1x 40.5 model Notes: In RetinaNet, the base LR is changed to 0.01 for minibatch size 16. SSD Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download VGG16 300 8 40w 81.613 25.1 model VGG16 512 8 40w 46.007 29.1 model Notes: VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 400000 iters. SSD on Pascal VOC Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download MobileNet v1 300 32 120e 159.543 73.2 model VGG16 300 8 240e 117.279 77.5 model VGG16 512 8 240e 65.975 80.2 model NOTE : MobileNet-SSD is trained in 2 GPU with totoal batch size as 64 and trained 120 epoches. VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 240 epoches. SSD training data augmentations: randomly color distortion, randomly cropping, randomly expansion, randomly flipping. Face Detection Please refer face detection models for details. Object Detection in Open Images Dataset V5 Please refer Open Images Dataset V5 Baseline model for details.","title":"MODEL ZOO"},{"location":"MODEL_ZOO/#model-zoo-and-benchmark","text":"","title":"Model Zoo and Benchmark"},{"location":"MODEL_ZOO/#environment","text":"Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2","title":"Environment"},{"location":"MODEL_ZOO/#common-settings","text":"All models below were trained on coco_2017_train , and tested on coco_2017_val . Batch Normalization layers in backbones are replaced by Affine Channel layers. Unless otherwise noted, all ResNet backbones adopt the ResNet-B variant.. For RCNN and RetinaNet models, only horizontal flipping data augmentation was used in the training phase and no augmentations were used in the testing phase. Inf time (fps) : the inference time is measured with fps (image/s) on a single GPU (Tesla V100) with cuDNN 7.5 by running 'tools/eval.py' on all validation set, which including data loadding, network forward and post processing. The batch size is 1.","title":"Common settings"},{"location":"MODEL_ZOO/#training-schedules","text":"We adopt exactly the same training schedules as Detectron . 1x indicates the schedule starts at a LR of 0.02 and is decreased by a factor of 10 after 60k and 80k iterations and eventually terminates at 90k iterations for minibatch size 16. For batch size 8, LR is decreased to 0.01, total training iterations are doubled, and the decay milestones are scaled by 2. 2x schedule is twice as long as 1x, with the LR milestones scaled accordingly.","title":"Training Schedules"},{"location":"MODEL_ZOO/#imagenet-pretrained-models","text":"The backbone models pretrained on ImageNet are available. All backbone models are pretrained on standard ImageNet-1k dataset and can be downloaded here . Notes: The ResNet50 model was trained with cosine LR decay schedule and can be downloaded here .","title":"ImageNet Pretrained Models"},{"location":"MODEL_ZOO/#baselines","text":"","title":"Baselines"},{"location":"MODEL_ZOO/#faster-mask-r-cnn","text":"Backbone Type Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50 Faster 1 1x 12.747 35.2 - model ResNet50 Faster 1 2x 12.686 37.1 - model ResNet50 Mask 1 1x 11.615 36.5 32.2 model ResNet50 Mask 1 2x 11.494 38.2 33.4 model ResNet50-vd Faster 1 1x 12.575 36.4 - model ResNet50-FPN Faster 2 1x 22.273 37.2 - model ResNet50-FPN Faster 2 2x 22.297 37.7 - model ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 model ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 model ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - model ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 model ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - model ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 model CBResNet50-vd-FPN Faster 2 1x - 39.7 - model ResNet101 Faster 1 1x 9.316 38.3 - model ResNet101-FPN Faster 1 1x 17.297 38.7 - model ResNet101-FPN Faster 1 2x 17.246 39.1 - model ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 model ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - model ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - model ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 model CBResNet101-vd-FPN Faster 2 1x - 42.7 - model ResNeXt101-vd-64x4d-FPN Faster 1 1x 8.815 42.2 - model ResNeXt101-vd-64x4d-FPN Faster 1 2x 8.809 41.7 - model ResNeXt101-vd-64x4d-FPN Mask 1 1x 7.689 42.9 37.9 model ResNeXt101-vd-64x4d-FPN Mask 1 2x 7.859 42.6 37.6 model SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - model SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 model ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - model","title":"Faster &amp; Mask R-CNN"},{"location":"MODEL_ZOO/#deformable-convnets-v2","text":"Backbone Type Conv Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - model ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - model ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - model ResNeXt101-vd-64x4d-FPN Faster c3-c5 1 1x 7.209 45.2 - model ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 model ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 model ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 model ResNeXt101-vd-64x4d-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 model ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - model ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - model ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - model SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 model ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - model CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - model","title":"Deformable ConvNets v2"},{"location":"MODEL_ZOO/#notes","text":"Deformable ConvNets v2(dcn_v2) reference from Deformable ConvNets v2 . c3-c5 means adding dcn in resnet stage 3 to 5. Detailed configuration file in configs/dcn","title":"Notes:"},{"location":"MODEL_ZOO/#group-normalization","text":"Backbone Type Image/gpu Lr schd Box AP Mask AP Download ResNet50-FPN Faster 2 2x 39.7 - model ResNet50-FPN Mask 1 2x 40.1 35.8 model","title":"Group Normalization"},{"location":"MODEL_ZOO/#notes_1","text":"Group Normalization reference from Group Normalization . Detailed configuration file in configs/gn","title":"Notes:"},{"location":"MODEL_ZOO/#yolo-v3","text":"Backbone Pretrain dataset Size deformable Conv Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 (paper) ImageNet 608 False 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 False 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 False 8 270e - 28.2 - DarkNet53 ImageNet 608 False 8 270e 45.571 38.9 model DarkNet53 ImageNet 416 False 8 270e - 37.5 model DarkNet53 ImageNet 320 False 8 270e - 34.8 model MobileNet-V1 ImageNet 608 False 8 270e 78.302 29.3 model MobileNet-V1 ImageNet 416 False 8 270e - 29.3 model MobileNet-V1 ImageNet 320 False 8 270e - 27.1 model ResNet34 ImageNet 608 False 8 270e 63.356 36.2 model ResNet34 ImageNet 416 False 8 270e - 34.3 model ResNet34 ImageNet 320 False 8 270e - 31.4 model ResNet50_vd ImageNet 608 True 8 270e - 39.1 model ResNet50_vd Object365 608 True 8 270e - 41.4 model","title":"YOLO v3"},{"location":"MODEL_ZOO/#yolo-v3-on-pascal-voc","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 608 8 270e 54.977 83.5 model DarkNet53 416 8 270e - 83.6 model DarkNet53 320 8 270e - 82.2 model MobileNet-V1 608 8 270e 104.291 76.2 model MobileNet-V1 416 8 270e - 76.7 model MobileNet-V1 320 8 270e - 75.3 model ResNet34 608 8 270e 82.247 82.6 model ResNet34 416 8 270e - 81.9 model ResNet34 320 8 270e - 80.1 model","title":"YOLO v3 on Pascal VOC"},{"location":"MODEL_ZOO/#notes_2","text":"YOLOv3-DarkNet53 performance in paper YOLOv3 is also provided above, our implements improved performance mainly by using L1 loss in bounding box width and height regression, image mixup and label smooth. YOLO v3 is trained in 8 GPU with total batch size as 64 and trained 270 epoches. YOLO v3 training data augmentations: mixup, randomly color distortion, randomly cropping, randomly expansion, randomly interpolation method, randomly flippling. YOLO v3 used randomly reshaped minibatch in training, inferences can be performed on different image sizes with the same model weights, and we provided evaluation results of image size 608/416/320 above. Deformable conv is added on stage 5 of backbone.","title":"Notes:"},{"location":"MODEL_ZOO/#retinanet","text":"Backbone Image/gpu Lr schd Box AP Download ResNet50-FPN 2 1x 36.0 model ResNet101-FPN 2 1x 37.3 model ResNeXt101-vd-FPN 1 1x 40.5 model Notes: In RetinaNet, the base LR is changed to 0.01 for minibatch size 16.","title":"RetinaNet"},{"location":"MODEL_ZOO/#ssd","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download VGG16 300 8 40w 81.613 25.1 model VGG16 512 8 40w 46.007 29.1 model Notes: VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 400000 iters.","title":"SSD"},{"location":"MODEL_ZOO/#ssd-on-pascal-voc","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download MobileNet v1 300 32 120e 159.543 73.2 model VGG16 300 8 240e 117.279 77.5 model VGG16 512 8 240e 65.975 80.2 model NOTE : MobileNet-SSD is trained in 2 GPU with totoal batch size as 64 and trained 120 epoches. VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 240 epoches. SSD training data augmentations: randomly color distortion, randomly cropping, randomly expansion, randomly flipping.","title":"SSD on Pascal VOC"},{"location":"MODEL_ZOO/#face-detection","text":"Please refer face detection models for details.","title":"Face Detection"},{"location":"MODEL_ZOO/#object-detection-in-open-images-dataset-v5","text":"Please refer Open Images Dataset V5 Baseline model for details.","title":"Object Detection in Open Images Dataset V5"},{"location":"MODEL_ZOO_cn/","text":"\u6a21\u578b\u5e93\u548c\u57fa\u7ebf \u6d4b\u8bd5\u73af\u5883 Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2 \u901a\u7528\u8bbe\u7f6e \u6240\u6709\u6a21\u578b\u5747\u5728COCO17\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002 \u9664\u975e\u7279\u6b8a\u8bf4\u660e\uff0c\u6240\u6709ResNet\u9aa8\u5e72\u7f51\u7edc\u91c7\u7528 ResNet-B \u7ed3\u6784\u3002 \u5bf9\u4e8eRCNN\u548cRetinaNet\u7cfb\u5217\u6a21\u578b\uff0c\u8bad\u7ec3\u9636\u6bb5\u4ec5\u4f7f\u7528\u6c34\u5e73\u7ffb\u8f6c\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\uff0c\u6d4b\u8bd5\u9636\u6bb5\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u3002 \u63a8\u7406\u65f6\u95f4(fps) : \u63a8\u7406\u65f6\u95f4\u662f\u5728\u4e00\u5f20Tesla V100\u7684GPU\u4e0a\u901a\u8fc7'tools/eval.py'\u6d4b\u8bd5\u6240\u6709\u9a8c\u8bc1\u96c6\u5f97\u5230\uff0c\u5355\u4f4d\u662ffps(\u56fe\u7247\u6570/\u79d2), cuDNN\u7248\u672c\u662f7.5\uff0c\u5305\u62ec\u6570\u636e\u52a0\u8f7d\u3001\u7f51\u7edc\u524d\u5411\u6267\u884c\u548c\u540e\u5904\u7406, batch size\u662f1\u3002 \u8bad\u7ec3\u7b56\u7565 \u6211\u4eec\u91c7\u7528\u548c Detectron \u76f8\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u3002 1x \u7b56\u7565\u8868\u793a\uff1a\u5728\u603bbatch size\u4e3a16\u65f6\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u4e3a0.02\uff0c\u57286\u4e07\u8f6e\u548c8\u4e07\u8f6e\u540e\u5b66\u4e60\u7387\u5206\u522b\u4e0b\u964d10\u500d\uff0c\u6700\u7ec8\u8bad\u7ec39\u4e07\u8f6e\u3002\u5728\u603bbatch size\u4e3a8\u65f6\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u4e3a0.01\uff0c\u572812\u4e07\u8f6e\u548c16\u4e07\u8f6e\u540e\u5b66\u4e60\u7387\u5206\u522b\u4e0b\u964d10\u500d\uff0c\u6700\u7ec8\u8bad\u7ec318\u4e07\u8f6e\u3002 2x \u7b56\u7565\u4e3a1x\u7b56\u7565\u7684\u4e24\u500d\uff0c\u540c\u65f6\u5b66\u4e60\u7387\u8c03\u6574\u4f4d\u7f6e\u4e5f\u4e3a1x\u7684\u4e24\u500d\u3002 ImageNet\u9884\u8bad\u7ec3\u6a21\u578b Paddle\u63d0\u4f9b\u57fa\u4e8eImageNet\u7684\u9aa8\u67b6\u7f51\u7edc\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6240\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u5747\u901a\u8fc7\u6807\u51c6\u7684Imagenet-1k\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\u3002 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\uff1aResNet50\u6a21\u578b\u901a\u8fc7\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u8bad\u7ec3\u5f97\u5230\u3002 \u4e0b\u8f7d\u94fe\u63a5 \u57fa\u7ebf Faster & Mask R-CNN \u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP Mask AP \u4e0b\u8f7d ResNet50 Faster 1 1x 12.747 35.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Faster 1 2x 12.686 37.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Mask 1 1x 11.615 36.5 32.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Mask 1 2x 11.494 38.2 33.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd Faster 1 1x 12.575 36.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Faster 2 1x 22.273 37.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Faster 2 2x 22.297 37.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 \u4e0b\u8f7d\u94fe\u63a5 CBResNet50-vd-FPN Faster 2 1x - 39.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101 Faster 1 1x 9.316 38.3 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Faster 1 1x 17.297 38.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Faster 1 2x 17.246 39.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster 1 1x 8.815 42.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster 1 2x 8.809 41.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask 1 1x 7.689 42.9 37.9 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask 1 2x 7.859 42.6 37.6 \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - \u4e0b\u8f7d\u94fe\u63a5 Deformable \u5377\u79ef\u7f51\u7edcv2 \u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u5377\u79ef \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP Mask AP \u4e0b\u8f7d ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster c3-c5 1 1x 7.209 45.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 \u4e0b\u8f7d\u94fe\u63a5 ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - \u4e0b\u8f7d\u94fe\u63a5 CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: Deformable\u5377\u79ef\u7f51\u7edcv2(dcn_v2)\u53c2\u8003\u81ea\u8bba\u6587 Deformable ConvNets v2 . c3-c5 \u610f\u601d\u662f\u5728resnet\u6a21\u5757\u76843\u52305\u9636\u6bb5\u589e\u52a0 dcn . \u8be6\u7ec6\u7684\u914d\u7f6e\u6587\u4ef6\u5728 configs/dcn Group Normalization \u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 Box AP Mask AP \u4e0b\u8f7d ResNet50-FPN Faster 2 2x 39.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 2x 40.1 35.8 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: Group Normalization\u53c2\u8003\u8bba\u6587 Group Normalization . \u8be6\u7ec6\u7684\u914d\u7f6e\u6587\u4ef6\u5728 configs/gn YOLO v3 \u9aa8\u67b6\u7f51\u7edc \u9884\u8bad\u7ec3\u6570\u636e\u96c6 \u8f93\u5165\u5c3a\u5bf8 \u52a0\u5165deformable\u5377\u79ef \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d DarkNet53 (paper) ImageNet 608 \u5426 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 \u5426 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 \u5426 8 270e - 28.2 - DarkNet53 ImageNet 608 \u5426 8 270e 45.571 38.9 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 ImageNet 416 \u5426 8 270e - 37.5 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 ImageNet 320 \u5426 8 270e - 34.8 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 608 \u5426 8 270e 78.302 29.3 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 416 \u5426 8 270e - 29.3 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 320 \u5426 8 270e - 27.1 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 608 \u5426 8 270e 63.356 36.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 416 \u5426 8 270e - 34.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 320 \u5426 8 270e - 31.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50_vd ImageNet 608 \u662f 8 270e - 39.1 \u4e0b\u8f7d\u94fe\u63a5 ResNet50_vd Object365 608 \u662f 8 270e - 41.4 \u4e0b\u8f7d\u94fe\u63a5 YOLO v3 \u57fa\u4e8ePasacl VOC\u6570\u636e\u96c6 \u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d DarkNet53 608 8 270e 54.977 83.5 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 416 8 270e - 83.6 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 320 8 270e - 82.2 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 608 8 270e 104.291 76.2 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 416 8 270e - 76.7 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 320 8 270e - 75.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 608 8 270e 82.247 82.6 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 416 8 270e - 81.9 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 320 8 270e - 80.1 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: \u4e0a\u8868\u4e2d\u4e5f\u63d0\u4f9b\u4e86\u539f\u8bba\u6587 YOLOv3 \u4e2dYOLOv3-DarkNet53\u7684\u7cbe\u5ea6\uff0c\u6211\u4eec\u7684\u5b9e\u73b0\u7248\u672c\u4e3b\u8981\u4ece\u5728bounding box\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u56de\u5f52\u4e0a\u4f7f\u7528\u4e86L1\u635f\u5931\uff0c\u56fe\u50cfmixup\u548clabel smooth\u7b49\u65b9\u6cd5\u4f18\u5316\u4e86\u5176\u7cbe\u5ea6\u3002 YOLO v3\u57288\u5361\uff0c\u603bbatch size\u4e3a64\u4e0b\u8bad\u7ec3270\u8f6e\u3002\u6570\u636e\u589e\u5f3a\u5305\u62ec\uff1amixup, \u968f\u673a\u989c\u8272\u5931\u771f\uff0c\u968f\u673a\u526a\u88c1\uff0c\u968f\u673a\u6269\u5f20\uff0c\u968f\u673a\u63d2\u503c\u6cd5\uff0c\u968f\u673a\u7ffb\u8f6c\u3002YOLO v3\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9minibatch\u91c7\u7528\u968f\u673areshape\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u6a21\u578b\u6d4b\u8bd5\u4e0d\u540c\u5c3a\u5bf8\u56fe\u7247\uff0c\u6211\u4eec\u5206\u522b\u63d0\u4f9b\u4e86\u5c3a\u5bf8\u4e3a608/416/320\u5927\u5c0f\u7684\u6d4b\u8bd5\u7ed3\u679c\u3002deformable\u5377\u79ef\u4f5c\u7528\u5728\u9aa8\u67b6\u7f51\u7edc5\u9636\u6bb5\u3002 RetinaNet \u9aa8\u67b6\u7f51\u7edc \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 Box AP \u4e0b\u8f7d ResNet50-FPN 2 1x 36.0 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN 2 1x 37.3 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN 1 1x 40.5 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: RetinaNet\u7cfb\u5217\u6a21\u578b\u4e2d\uff0c\u5728\u603bbatch size\u4e3a16\u4e0b\u60c5\u51b5\u4e0b\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u6539\u4e3a0.01\u3002 SSD \u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d VGG16 300 8 40\u4e07 81.613 25.1 \u4e0b\u8f7d\u94fe\u63a5 VGG16 512 8 40\u4e07 46.007 29.1 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: VGG-SSD\u5728\u603bbatch size\u4e3a32\u4e0b\u8bad\u7ec340\u4e07\u8f6e\u3002 SSD \u57fa\u4e8ePascal VOC\u6570\u636e\u96c6 \u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d MobileNet v1 300 32 120e 159.543 73.2 \u4e0b\u8f7d\u94fe\u63a5 VGG16 300 8 240e 117.279 77.5 \u4e0b\u8f7d\u94fe\u63a5 VGG16 512 8 240e 65.975 80.2 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: MobileNet-SSD\u57282\u5361\uff0c\u603bbatch size\u4e3a64\u4e0b\u8bad\u7ec3120\u5468\u671f\u3002VGG-SSD\u5728\u603bbatch size\u4e3a32\u4e0b\u8bad\u7ec3240\u5468\u671f\u3002\u6570\u636e\u589e\u5f3a\u5305\u62ec\uff1a\u968f\u673a\u989c\u8272\u5931\u771f\uff0c\u968f\u673a\u526a\u88c1\uff0c\u968f\u673a\u6269\u5f20\uff0c\u968f\u673a\u7ffb\u8f6c\u3002 \u4eba\u8138\u68c0\u6d4b \u8be6\u7ec6\u8bf7\u53c2\u8003 \u4eba\u8138\u68c0\u6d4b\u6a21\u578b \u3002 \u57fa\u4e8eOpen Images V5\u6570\u636e\u96c6\u7684\u7269\u4f53\u68c0\u6d4b \u8be6\u7ec6\u8bf7\u53c2\u8003 Open Images V5\u6570\u636e\u96c6\u57fa\u7ebf\u6a21\u578b \u3002","title":"\u6a21\u578b\u5e93\u548c\u57fa\u7ebf"},{"location":"MODEL_ZOO_cn/#_1","text":"","title":"\u6a21\u578b\u5e93\u548c\u57fa\u7ebf"},{"location":"MODEL_ZOO_cn/#_2","text":"Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2","title":"\u6d4b\u8bd5\u73af\u5883"},{"location":"MODEL_ZOO_cn/#_3","text":"\u6240\u6709\u6a21\u578b\u5747\u5728COCO17\u6570\u636e\u96c6\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002 \u9664\u975e\u7279\u6b8a\u8bf4\u660e\uff0c\u6240\u6709ResNet\u9aa8\u5e72\u7f51\u7edc\u91c7\u7528 ResNet-B \u7ed3\u6784\u3002 \u5bf9\u4e8eRCNN\u548cRetinaNet\u7cfb\u5217\u6a21\u578b\uff0c\u8bad\u7ec3\u9636\u6bb5\u4ec5\u4f7f\u7528\u6c34\u5e73\u7ffb\u8f6c\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\uff0c\u6d4b\u8bd5\u9636\u6bb5\u4e0d\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u3002 \u63a8\u7406\u65f6\u95f4(fps) : \u63a8\u7406\u65f6\u95f4\u662f\u5728\u4e00\u5f20Tesla V100\u7684GPU\u4e0a\u901a\u8fc7'tools/eval.py'\u6d4b\u8bd5\u6240\u6709\u9a8c\u8bc1\u96c6\u5f97\u5230\uff0c\u5355\u4f4d\u662ffps(\u56fe\u7247\u6570/\u79d2), cuDNN\u7248\u672c\u662f7.5\uff0c\u5305\u62ec\u6570\u636e\u52a0\u8f7d\u3001\u7f51\u7edc\u524d\u5411\u6267\u884c\u548c\u540e\u5904\u7406, batch size\u662f1\u3002","title":"\u901a\u7528\u8bbe\u7f6e"},{"location":"MODEL_ZOO_cn/#_4","text":"\u6211\u4eec\u91c7\u7528\u548c Detectron \u76f8\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u3002 1x \u7b56\u7565\u8868\u793a\uff1a\u5728\u603bbatch size\u4e3a16\u65f6\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u4e3a0.02\uff0c\u57286\u4e07\u8f6e\u548c8\u4e07\u8f6e\u540e\u5b66\u4e60\u7387\u5206\u522b\u4e0b\u964d10\u500d\uff0c\u6700\u7ec8\u8bad\u7ec39\u4e07\u8f6e\u3002\u5728\u603bbatch size\u4e3a8\u65f6\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u4e3a0.01\uff0c\u572812\u4e07\u8f6e\u548c16\u4e07\u8f6e\u540e\u5b66\u4e60\u7387\u5206\u522b\u4e0b\u964d10\u500d\uff0c\u6700\u7ec8\u8bad\u7ec318\u4e07\u8f6e\u3002 2x \u7b56\u7565\u4e3a1x\u7b56\u7565\u7684\u4e24\u500d\uff0c\u540c\u65f6\u5b66\u4e60\u7387\u8c03\u6574\u4f4d\u7f6e\u4e5f\u4e3a1x\u7684\u4e24\u500d\u3002","title":"\u8bad\u7ec3\u7b56\u7565"},{"location":"MODEL_ZOO_cn/#imagenet","text":"Paddle\u63d0\u4f9b\u57fa\u4e8eImageNet\u7684\u9aa8\u67b6\u7f51\u7edc\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u6240\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u5747\u901a\u8fc7\u6807\u51c6\u7684Imagenet-1k\u6570\u636e\u96c6\u8bad\u7ec3\u5f97\u5230\u3002 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\uff1aResNet50\u6a21\u578b\u901a\u8fc7\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u6574\u7b56\u7565\u8bad\u7ec3\u5f97\u5230\u3002 \u4e0b\u8f7d\u94fe\u63a5","title":"ImageNet\u9884\u8bad\u7ec3\u6a21\u578b"},{"location":"MODEL_ZOO_cn/#_5","text":"","title":"\u57fa\u7ebf"},{"location":"MODEL_ZOO_cn/#faster-mask-r-cnn","text":"\u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP Mask AP \u4e0b\u8f7d ResNet50 Faster 1 1x 12.747 35.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Faster 1 2x 12.686 37.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Mask 1 1x 11.615 36.5 32.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet50 Mask 1 2x 11.494 38.2 33.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd Faster 1 1x 12.575 36.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Faster 2 1x 22.273 37.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Faster 2 2x 22.297 37.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 \u4e0b\u8f7d\u94fe\u63a5 CBResNet50-vd-FPN Faster 2 1x - 39.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101 Faster 1 1x 9.316 38.3 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Faster 1 1x 17.297 38.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Faster 1 2x 17.246 39.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster 1 1x 8.815 42.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster 1 2x 8.809 41.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask 1 1x 7.689 42.9 37.9 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask 1 2x 7.859 42.6 37.6 \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - \u4e0b\u8f7d\u94fe\u63a5","title":"Faster &amp; Mask R-CNN"},{"location":"MODEL_ZOO_cn/#deformable-v2","text":"\u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u5377\u79ef \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP Mask AP \u4e0b\u8f7d ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Faster c3-c5 1 1x 7.209 45.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - \u4e0b\u8f7d\u94fe\u63a5 ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - \u4e0b\u8f7d\u94fe\u63a5 SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 \u4e0b\u8f7d\u94fe\u63a5 ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - \u4e0b\u8f7d\u94fe\u63a5 CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - \u4e0b\u8f7d\u94fe\u63a5","title":"Deformable \u5377\u79ef\u7f51\u7edcv2"},{"location":"MODEL_ZOO_cn/#_6","text":"Deformable\u5377\u79ef\u7f51\u7edcv2(dcn_v2)\u53c2\u8003\u81ea\u8bba\u6587 Deformable ConvNets v2 . c3-c5 \u610f\u601d\u662f\u5728resnet\u6a21\u5757\u76843\u52305\u9636\u6bb5\u589e\u52a0 dcn . \u8be6\u7ec6\u7684\u914d\u7f6e\u6587\u4ef6\u5728 configs/dcn","title":"\u6ce8\u610f\u4e8b\u9879:"},{"location":"MODEL_ZOO_cn/#group-normalization","text":"\u9aa8\u67b6\u7f51\u7edc \u7f51\u7edc\u7c7b\u578b \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 Box AP Mask AP \u4e0b\u8f7d ResNet50-FPN Faster 2 2x 39.7 - \u4e0b\u8f7d\u94fe\u63a5 ResNet50-FPN Mask 1 2x 40.1 35.8 \u4e0b\u8f7d\u94fe\u63a5","title":"Group Normalization"},{"location":"MODEL_ZOO_cn/#_7","text":"Group Normalization\u53c2\u8003\u8bba\u6587 Group Normalization . \u8be6\u7ec6\u7684\u914d\u7f6e\u6587\u4ef6\u5728 configs/gn","title":"\u6ce8\u610f\u4e8b\u9879:"},{"location":"MODEL_ZOO_cn/#yolo-v3","text":"\u9aa8\u67b6\u7f51\u7edc \u9884\u8bad\u7ec3\u6570\u636e\u96c6 \u8f93\u5165\u5c3a\u5bf8 \u52a0\u5165deformable\u5377\u79ef \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d DarkNet53 (paper) ImageNet 608 \u5426 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 \u5426 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 \u5426 8 270e - 28.2 - DarkNet53 ImageNet 608 \u5426 8 270e 45.571 38.9 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 ImageNet 416 \u5426 8 270e - 37.5 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 ImageNet 320 \u5426 8 270e - 34.8 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 608 \u5426 8 270e 78.302 29.3 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 416 \u5426 8 270e - 29.3 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 ImageNet 320 \u5426 8 270e - 27.1 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 608 \u5426 8 270e 63.356 36.2 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 416 \u5426 8 270e - 34.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 ImageNet 320 \u5426 8 270e - 31.4 \u4e0b\u8f7d\u94fe\u63a5 ResNet50_vd ImageNet 608 \u662f 8 270e - 39.1 \u4e0b\u8f7d\u94fe\u63a5 ResNet50_vd Object365 608 \u662f 8 270e - 41.4 \u4e0b\u8f7d\u94fe\u63a5","title":"YOLO v3"},{"location":"MODEL_ZOO_cn/#yolo-v3-pasacl-voc","text":"\u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d DarkNet53 608 8 270e 54.977 83.5 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 416 8 270e - 83.6 \u4e0b\u8f7d\u94fe\u63a5 DarkNet53 320 8 270e - 82.2 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 608 8 270e 104.291 76.2 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 416 8 270e - 76.7 \u4e0b\u8f7d\u94fe\u63a5 MobileNet-V1 320 8 270e - 75.3 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 608 8 270e 82.247 82.6 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 416 8 270e - 81.9 \u4e0b\u8f7d\u94fe\u63a5 ResNet34 320 8 270e - 80.1 \u4e0b\u8f7d\u94fe\u63a5","title":"YOLO v3 \u57fa\u4e8ePasacl VOC\u6570\u636e\u96c6"},{"location":"MODEL_ZOO_cn/#_8","text":"\u4e0a\u8868\u4e2d\u4e5f\u63d0\u4f9b\u4e86\u539f\u8bba\u6587 YOLOv3 \u4e2dYOLOv3-DarkNet53\u7684\u7cbe\u5ea6\uff0c\u6211\u4eec\u7684\u5b9e\u73b0\u7248\u672c\u4e3b\u8981\u4ece\u5728bounding box\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\u56de\u5f52\u4e0a\u4f7f\u7528\u4e86L1\u635f\u5931\uff0c\u56fe\u50cfmixup\u548clabel smooth\u7b49\u65b9\u6cd5\u4f18\u5316\u4e86\u5176\u7cbe\u5ea6\u3002 YOLO v3\u57288\u5361\uff0c\u603bbatch size\u4e3a64\u4e0b\u8bad\u7ec3270\u8f6e\u3002\u6570\u636e\u589e\u5f3a\u5305\u62ec\uff1amixup, \u968f\u673a\u989c\u8272\u5931\u771f\uff0c\u968f\u673a\u526a\u88c1\uff0c\u968f\u673a\u6269\u5f20\uff0c\u968f\u673a\u63d2\u503c\u6cd5\uff0c\u968f\u673a\u7ffb\u8f6c\u3002YOLO v3\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9minibatch\u91c7\u7528\u968f\u673areshape\uff0c\u53ef\u4ee5\u91c7\u7528\u76f8\u540c\u7684\u6a21\u578b\u6d4b\u8bd5\u4e0d\u540c\u5c3a\u5bf8\u56fe\u7247\uff0c\u6211\u4eec\u5206\u522b\u63d0\u4f9b\u4e86\u5c3a\u5bf8\u4e3a608/416/320\u5927\u5c0f\u7684\u6d4b\u8bd5\u7ed3\u679c\u3002deformable\u5377\u79ef\u4f5c\u7528\u5728\u9aa8\u67b6\u7f51\u7edc5\u9636\u6bb5\u3002","title":"\u6ce8\u610f\u4e8b\u9879:"},{"location":"MODEL_ZOO_cn/#retinanet","text":"\u9aa8\u67b6\u7f51\u7edc \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 Box AP \u4e0b\u8f7d ResNet50-FPN 2 1x 36.0 \u4e0b\u8f7d\u94fe\u63a5 ResNet101-FPN 2 1x 37.3 \u4e0b\u8f7d\u94fe\u63a5 ResNeXt101-vd-FPN 1 1x 40.5 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: RetinaNet\u7cfb\u5217\u6a21\u578b\u4e2d\uff0c\u5728\u603bbatch size\u4e3a16\u4e0b\u60c5\u51b5\u4e0b\uff0c\u521d\u59cb\u5b66\u4e60\u7387\u6539\u4e3a0.01\u3002","title":"RetinaNet"},{"location":"MODEL_ZOO_cn/#ssd","text":"\u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d VGG16 300 8 40\u4e07 81.613 25.1 \u4e0b\u8f7d\u94fe\u63a5 VGG16 512 8 40\u4e07 46.007 29.1 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: VGG-SSD\u5728\u603bbatch size\u4e3a32\u4e0b\u8bad\u7ec340\u4e07\u8f6e\u3002","title":"SSD"},{"location":"MODEL_ZOO_cn/#ssd-pascal-voc","text":"\u9aa8\u67b6\u7f51\u7edc \u8f93\u5165\u5c3a\u5bf8 \u6bcf\u5f20GPU\u56fe\u7247\u4e2a\u6570 \u5b66\u4e60\u7387\u7b56\u7565 \u63a8\u7406\u65f6\u95f4(fps) Box AP \u4e0b\u8f7d MobileNet v1 300 32 120e 159.543 73.2 \u4e0b\u8f7d\u94fe\u63a5 VGG16 300 8 240e 117.279 77.5 \u4e0b\u8f7d\u94fe\u63a5 VGG16 512 8 240e 65.975 80.2 \u4e0b\u8f7d\u94fe\u63a5 \u6ce8\u610f\u4e8b\u9879: MobileNet-SSD\u57282\u5361\uff0c\u603bbatch size\u4e3a64\u4e0b\u8bad\u7ec3120\u5468\u671f\u3002VGG-SSD\u5728\u603bbatch size\u4e3a32\u4e0b\u8bad\u7ec3240\u5468\u671f\u3002\u6570\u636e\u589e\u5f3a\u5305\u62ec\uff1a\u968f\u673a\u989c\u8272\u5931\u771f\uff0c\u968f\u673a\u526a\u88c1\uff0c\u968f\u673a\u6269\u5f20\uff0c\u968f\u673a\u7ffb\u8f6c\u3002","title":"SSD \u57fa\u4e8ePascal VOC\u6570\u636e\u96c6"},{"location":"MODEL_ZOO_cn/#_9","text":"\u8be6\u7ec6\u8bf7\u53c2\u8003 \u4eba\u8138\u68c0\u6d4b\u6a21\u578b \u3002","title":"\u4eba\u8138\u68c0\u6d4b"},{"location":"MODEL_ZOO_cn/#open-images-v5","text":"\u8be6\u7ec6\u8bf7\u53c2\u8003 Open Images V5\u6570\u636e\u96c6\u57fa\u7ebf\u6a21\u578b \u3002","title":"\u57fa\u4e8eOpen Images V5\u6570\u636e\u96c6\u7684\u7269\u4f53\u68c0\u6d4b"},{"location":"OIDV5_BASELINE_MODEL/","text":"CascadeCA RCNN \u7b80\u4ecb CascadeCA RCNN\u662f\u767e\u5ea6\u89c6\u89c9\u6280\u672f\u90e8\u5728Google AI Open Images 2019-Object Detction\u6bd4\u8d5b\u4e2d\u7684\u6700\u4f73\u5355\u6a21\u578b\uff0c\u8be5\u5355\u6a21\u578b\u52a9\u529b\u56e2\u961f\u5728500\u591a\u53c2\u6570\u961f\u4f0d\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\u3002Open Images Dataset V5(OIDV5)\u5305\u542b500\u4e2a\u7c7b\u522b\u3001173W\u8bad\u7ec3\u56fe\u50cf\u548c\u8d85\u8fc71400W\u4e2a\u6807\u6ce8\u8fb9\u6846\uff0c\u662f\u76ee\u524d\u5df2\u77e5\u89c4\u6a21\u6700\u5927\u7684\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6570\u636e\u96c6\u5730\u5740\uff1a https://storage.googleapis.com/openimages/web/index.html \u3002\u56e2\u961f\u5728\u6bd4\u8d5b\u4e2d\u7684\u6280\u672f\u65b9\u6848\u62a5\u544a\u5730\u5740\uff1a https://arxiv.org/pdf/1911.07171.pdf \u65b9\u6cd5\u63cf\u8ff0 \u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5f53\u524d\u8f83\u4f18\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u5177\u4f53\u5730\uff0c\u5b83\u5c06ResNet200-vd\u4f5c\u4e3a\u68c0\u6d4b\u6a21\u578b\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5176imagenet\u5206\u7c7b\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5728 \u8fd9\u91cc \u4e0b\u8f7d\uff1b\u7ed3\u5408\u4e86CascadeCA RCNN\u3001Feature Pyramid Networks\u3001Non-local\u3001Deformable V2\u7b49\u65b9\u6cd5\u3002\u5728\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6807\u51c6\u7684CascadeRCNN\u662f\u53ea\u9884\u6d4b2\u4e2a\u6846\uff08\u524d\u666f\u548c\u80cc\u666f\uff0c\u4f7f\u7528\u5f97\u5206\u4fe1\u606f\u53bb\u5224\u65ad\u6700\u7ec8\u524d\u666f\u6240\u5c5e\u7684\u7c7b\u522b\uff09\uff0c\u800c\u8be5\u6a21\u578b\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u90fd\u5355\u72ec\u9884\u6d4b\u4e86\u4e00\u4e2a\u6846\uff08Cascade Class Aware\uff09\u3002\u6700\u7ec8\u6a21\u578b\u6846\u56fe\u5982\u4e0b\u56fe\u6240\u793a\u3002 \u7531\u4e8eOIDV5\u7684\u7c7b\u522b\u4e0d\u5747\u8861\u73b0\u8c61\u6bd4\u8f83\u4e25\u91cd\uff0c\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u4e86\u52a8\u6001\u91c7\u6837\u7684\u7b56\u7565\u53bb\u9009\u62e9\u6837\u672c\u5e76\u8fdb\u884c\u8bad\u7ec3\uff1b\u591a\u5c3a\u5ea6\u8bad\u7ec3\u88ab\u7528\u4e8e\u89e3\u51b3\u8fb9\u6846\u9762\u79ef\u8303\u56f4\u592a\u5927\u7684\u60c5\u51b5\uff1b\u6b64\u5916\uff0c\u56e2\u961f\u4f7f\u7528Libra loss\u66ff\u4ee3Smooth L1 loss\uff0c\u6765\u8ba1\u7b97\u9884\u6d4b\u6846\u7684loss\uff1b\u5728\u9884\u6d4b\u65f6\uff0c\u4f7f\u7528SoftNMS\u65b9\u6cd5\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4fdd\u8bc1\u66f4\u591a\u7684\u6846\u53ef\u4ee5\u88ab\u53ec\u56de\u3002 Objects365 Dataset\u548cOIDV5\u6709\u5927\u7ea6189\u4e2a\u7c7b\u522b\u662f\u91cd\u590d\u7684\uff0c\u56e0\u6b64\u5c06\u4e24\u4e2a\u6570\u636e\u96c6\u5408\u5e76\u8fdb\u884c\u8bad\u7ec3\uff0c\u7528\u4e8e\u6269\u5145OIDV5\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6700\u7ec8\u8be5\u6a21\u578b\u4e0e\u5176\u6027\u80fd\u6307\u6807\u5982\u4e0b\u8868\u6240\u793a\u3002\u66f4\u5177\u4f53\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u878d\u5408\u7b56\u7565\u53ef\u4ee5\u89c1\uff1a OIDV5\u6280\u672f\u62a5\u544a \u3002 OIDV5\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u5982\u4e0b\u3002 \u6a21\u578b\u7ed3\u6784 Public/Private Score \u4e0b\u8f7d\u94fe\u63a5 CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd 0.62690/0.59459 \u6a21\u578b \u6b64\u5916\uff0c\u4e3a\u9a8c\u8bc1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e2\u961f\u57fa\u4e8e\u8be5\u6a21\u578b\u7ed3\u6784\uff0c\u4e5f\u8bad\u7ec3\u4e86\u9488\u5bf9COCO2017\u548cObjects365 Dataset\u7684\u6a21\u578b\uff0c\u6a21\u578b\u548c\u9a8c\u8bc1\u96c6\u6307\u6807\u5982\u4e0b\u8868\u3002 \u6a21\u578b\u7ed3\u6784 \u6570\u636e\u96c6 \u9a8c\u8bc1\u96c6mAP \u4e0b\u8f7d\u94fe\u63a5 CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd COCO2017 51.7% \u6a21\u578b CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd Objects365 34.5% \u6a21\u578b COCO\u548cObjects365 Dataset\u6570\u636e\u683c\u5f0f\u76f8\u540c\uff0c\u76ee\u524d\u53ea\u652f\u6301\u9884\u6d4b\u548c\u8bc4\u4f30\u3002 \u4f7f\u7528\u65b9\u6cd5 OIDV5\u6570\u636e\u96c6\u683c\u5f0f\u4e0eCOCO\u4e0d\u540c\uff0c\u76ee\u524d\u4ec5\u652f\u6301\u5355\u5f20\u56fe\u7247\u7684\u9884\u6d4b\u3002OIDV5\u7684\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u53ef\u4ee5\u53c2\u8003 \u8fd9\u91cc \u4e0b\u8f7d\u6a21\u578b\u5e76\u89e3\u538b\u3002 \u8fd0\u884c\u9884\u6d4b\u7a0b\u5e8f\u3002 python -u tools/infer.py -c configs/oidv5/cascade_rcnn_cls_aware_r200_vd_fpn_dcnv2_nonlocal_softnms.yml -o weights=./oidv5_cascade_rcnn_cls_aware_r200_vd_fpn_dcnv2_nonlocal_softnms/ --infer_img=demo/000000570688.jpg \u5176\u4e2d\u6a21\u578b\u6240\u5728\u6587\u4ef6\u5939\u9700\u8981\u6839\u636e\u81ea\u5df1\u653e\u7f6e\u7684\u4f4d\u7f6e\u8fdb\u884c\u4fee\u6539\u3002 \u68c0\u6d4b\u7ed3\u679c\u56fe\u50cf\u53ef\u4ee5\u5728 output \u6587\u4ef6\u5939\u4e2d\u67e5\u770b\u3002 \u6a21\u578b\u68c0\u6d4b\u6548\u679c","title":"CascadeCA RCNN"},{"location":"OIDV5_BASELINE_MODEL/#cascadeca-rcnn","text":"","title":"CascadeCA RCNN"},{"location":"OIDV5_BASELINE_MODEL/#_1","text":"CascadeCA RCNN\u662f\u767e\u5ea6\u89c6\u89c9\u6280\u672f\u90e8\u5728Google AI Open Images 2019-Object Detction\u6bd4\u8d5b\u4e2d\u7684\u6700\u4f73\u5355\u6a21\u578b\uff0c\u8be5\u5355\u6a21\u578b\u52a9\u529b\u56e2\u961f\u5728500\u591a\u53c2\u6570\u961f\u4f0d\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\u3002Open Images Dataset V5(OIDV5)\u5305\u542b500\u4e2a\u7c7b\u522b\u3001173W\u8bad\u7ec3\u56fe\u50cf\u548c\u8d85\u8fc71400W\u4e2a\u6807\u6ce8\u8fb9\u6846\uff0c\u662f\u76ee\u524d\u5df2\u77e5\u89c4\u6a21\u6700\u5927\u7684\u76ee\u6807\u68c0\u6d4b\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6570\u636e\u96c6\u5730\u5740\uff1a https://storage.googleapis.com/openimages/web/index.html \u3002\u56e2\u961f\u5728\u6bd4\u8d5b\u4e2d\u7684\u6280\u672f\u65b9\u6848\u62a5\u544a\u5730\u5740\uff1a https://arxiv.org/pdf/1911.07171.pdf","title":"\u7b80\u4ecb"},{"location":"OIDV5_BASELINE_MODEL/#_2","text":"\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u5f53\u524d\u8f83\u4f18\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u5177\u4f53\u5730\uff0c\u5b83\u5c06ResNet200-vd\u4f5c\u4e3a\u68c0\u6d4b\u6a21\u578b\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u5176imagenet\u5206\u7c7b\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u5728 \u8fd9\u91cc \u4e0b\u8f7d\uff1b\u7ed3\u5408\u4e86CascadeCA RCNN\u3001Feature Pyramid Networks\u3001Non-local\u3001Deformable V2\u7b49\u65b9\u6cd5\u3002\u5728\u8fd9\u91cc\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6807\u51c6\u7684CascadeRCNN\u662f\u53ea\u9884\u6d4b2\u4e2a\u6846\uff08\u524d\u666f\u548c\u80cc\u666f\uff0c\u4f7f\u7528\u5f97\u5206\u4fe1\u606f\u53bb\u5224\u65ad\u6700\u7ec8\u524d\u666f\u6240\u5c5e\u7684\u7c7b\u522b\uff09\uff0c\u800c\u8be5\u6a21\u578b\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u90fd\u5355\u72ec\u9884\u6d4b\u4e86\u4e00\u4e2a\u6846\uff08Cascade Class Aware\uff09\u3002\u6700\u7ec8\u6a21\u578b\u6846\u56fe\u5982\u4e0b\u56fe\u6240\u793a\u3002 \u7531\u4e8eOIDV5\u7684\u7c7b\u522b\u4e0d\u5747\u8861\u73b0\u8c61\u6bd4\u8f83\u4e25\u91cd\uff0c\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u4e86\u52a8\u6001\u91c7\u6837\u7684\u7b56\u7565\u53bb\u9009\u62e9\u6837\u672c\u5e76\u8fdb\u884c\u8bad\u7ec3\uff1b\u591a\u5c3a\u5ea6\u8bad\u7ec3\u88ab\u7528\u4e8e\u89e3\u51b3\u8fb9\u6846\u9762\u79ef\u8303\u56f4\u592a\u5927\u7684\u60c5\u51b5\uff1b\u6b64\u5916\uff0c\u56e2\u961f\u4f7f\u7528Libra loss\u66ff\u4ee3Smooth L1 loss\uff0c\u6765\u8ba1\u7b97\u9884\u6d4b\u6846\u7684loss\uff1b\u5728\u9884\u6d4b\u65f6\uff0c\u4f7f\u7528SoftNMS\u65b9\u6cd5\u8fdb\u884c\u540e\u5904\u7406\uff0c\u4fdd\u8bc1\u66f4\u591a\u7684\u6846\u53ef\u4ee5\u88ab\u53ec\u56de\u3002 Objects365 Dataset\u548cOIDV5\u6709\u5927\u7ea6189\u4e2a\u7c7b\u522b\u662f\u91cd\u590d\u7684\uff0c\u56e0\u6b64\u5c06\u4e24\u4e2a\u6570\u636e\u96c6\u5408\u5e76\u8fdb\u884c\u8bad\u7ec3\uff0c\u7528\u4e8e\u6269\u5145OIDV5\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6700\u7ec8\u8be5\u6a21\u578b\u4e0e\u5176\u6027\u80fd\u6307\u6807\u5982\u4e0b\u8868\u6240\u793a\u3002\u66f4\u5177\u4f53\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u878d\u5408\u7b56\u7565\u53ef\u4ee5\u89c1\uff1a OIDV5\u6280\u672f\u62a5\u544a \u3002 OIDV5\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u5982\u4e0b\u3002 \u6a21\u578b\u7ed3\u6784 Public/Private Score \u4e0b\u8f7d\u94fe\u63a5 CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd 0.62690/0.59459 \u6a21\u578b \u6b64\u5916\uff0c\u4e3a\u9a8c\u8bc1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u56e2\u961f\u57fa\u4e8e\u8be5\u6a21\u578b\u7ed3\u6784\uff0c\u4e5f\u8bad\u7ec3\u4e86\u9488\u5bf9COCO2017\u548cObjects365 Dataset\u7684\u6a21\u578b\uff0c\u6a21\u578b\u548c\u9a8c\u8bc1\u96c6\u6307\u6807\u5982\u4e0b\u8868\u3002 \u6a21\u578b\u7ed3\u6784 \u6570\u636e\u96c6 \u9a8c\u8bc1\u96c6mAP \u4e0b\u8f7d\u94fe\u63a5 CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd COCO2017 51.7% \u6a21\u578b CascadeCARCNN-FPN-Dcnv2-Nonlocal ResNet200-vd Objects365 34.5% \u6a21\u578b COCO\u548cObjects365 Dataset\u6570\u636e\u683c\u5f0f\u76f8\u540c\uff0c\u76ee\u524d\u53ea\u652f\u6301\u9884\u6d4b\u548c\u8bc4\u4f30\u3002","title":"\u65b9\u6cd5\u63cf\u8ff0"},{"location":"OIDV5_BASELINE_MODEL/#_3","text":"OIDV5\u6570\u636e\u96c6\u683c\u5f0f\u4e0eCOCO\u4e0d\u540c\uff0c\u76ee\u524d\u4ec5\u652f\u6301\u5355\u5f20\u56fe\u7247\u7684\u9884\u6d4b\u3002OIDV5\u7684\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u53ef\u4ee5\u53c2\u8003 \u8fd9\u91cc \u4e0b\u8f7d\u6a21\u578b\u5e76\u89e3\u538b\u3002 \u8fd0\u884c\u9884\u6d4b\u7a0b\u5e8f\u3002 python -u tools/infer.py -c configs/oidv5/cascade_rcnn_cls_aware_r200_vd_fpn_dcnv2_nonlocal_softnms.yml -o weights=./oidv5_cascade_rcnn_cls_aware_r200_vd_fpn_dcnv2_nonlocal_softnms/ --infer_img=demo/000000570688.jpg \u5176\u4e2d\u6a21\u578b\u6240\u5728\u6587\u4ef6\u5939\u9700\u8981\u6839\u636e\u81ea\u5df1\u653e\u7f6e\u7684\u4f4d\u7f6e\u8fdb\u884c\u4fee\u6539\u3002 \u68c0\u6d4b\u7ed3\u679c\u56fe\u50cf\u53ef\u4ee5\u5728 output \u6587\u4ef6\u5939\u4e2d\u67e5\u770b\u3002","title":"\u4f7f\u7528\u65b9\u6cd5"},{"location":"OIDV5_BASELINE_MODEL/#_4","text":"","title":"\u6a21\u578b\u68c0\u6d4b\u6548\u679c"},{"location":"QUICK_STARTED/","text":"English | \u7b80\u4f53\u4e2d\u6587 Quick Start This tutorial fine-tunes a tiny dataset by pretrained detection model for users to get a model and learn PaddleDetection quickly. The model can be trained in around 20min with good performance. Data Preparation Dataset refers to Kaggle , which contains 240 images in train dataset and 60 images in test dataset. Data categories are apple, orange and banana. Download here and uncompress the dataset after download, script for data preparation is located at download_fruit.py . Command is as follows: export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py Note: before started, run the following command and specifiy the GPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 Training: python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval Use yolov3_mobilenet_v1 to fine-tune the model from COCO dataset. Meanwhile, loss and mAP can be observed on tensorboard. tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> Result on tensorboard is shown below: Model can be downloaded here Evaluation: python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml Inference: python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg Inference images are shown below: For detailed infomation of training and evalution, please refer to GETTING_STARTED.md .","title":"QUICK STARTED"},{"location":"QUICK_STARTED/#quick-start","text":"This tutorial fine-tunes a tiny dataset by pretrained detection model for users to get a model and learn PaddleDetection quickly. The model can be trained in around 20min with good performance.","title":"Quick Start"},{"location":"QUICK_STARTED/#data-preparation","text":"Dataset refers to Kaggle , which contains 240 images in train dataset and 60 images in test dataset. Data categories are apple, orange and banana. Download here and uncompress the dataset after download, script for data preparation is located at download_fruit.py . Command is as follows: export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py Note: before started, run the following command and specifiy the GPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 Training: python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval Use yolov3_mobilenet_v1 to fine-tune the model from COCO dataset. Meanwhile, loss and mAP can be observed on tensorboard. tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> Result on tensorboard is shown below: Model can be downloaded here Evaluation: python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml Inference: python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg Inference images are shown below: For detailed infomation of training and evalution, please refer to GETTING_STARTED.md .","title":"Data Preparation"},{"location":"QUICK_STARTED_cn/","text":"English | \u7b80\u4f53\u4e2d\u6587 \u5feb\u901f\u5f00\u59cb \u4e3a\u4e86\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5728\u5f88\u77ed\u7684\u65f6\u95f4\u5185\u5feb\u901f\u4ea7\u51fa\u6a21\u578b\uff0c\u638c\u63e1PaddleDetection\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u7bc7\u6559\u7a0b\u901a\u8fc7\u4e00\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u5bf9\u5c0f\u6570\u636e\u96c6\u8fdb\u884cfinetune\u3002\u5728P40\u4e0a\u5355\u5361\u5927\u7ea620min\u5373\u53ef\u4ea7\u51fa\u4e00\u4e2a\u6548\u679c\u4e0d\u9519\u7684\u6a21\u578b\u3002 \u6570\u636e\u51c6\u5907 \u6570\u636e\u96c6\u53c2\u8003 Kaggle\u6570\u636e\u96c6 \uff0c\u5176\u4e2d\u8bad\u7ec3\u6570\u636e\u96c6240\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u6570\u636e\u96c660\u5f20\u56fe\u7247\uff0c\u6570\u636e\u7c7b\u522b\u4e3a3\u7c7b\uff1a\u82f9\u679c\uff0c\u6a58\u5b50\uff0c\u9999\u8549\u3002 \u4e0b\u8f7d\u94fe\u63a5 \u3002\u6570\u636e\u4e0b\u8f7d\u540e\u5206\u522b\u89e3\u538b\u5373\u53ef, \u6570\u636e\u51c6\u5907\u811a\u672c\u4f4d\u4e8e download_fruit.py \u3002\u4e0b\u8f7d\u6570\u636e\u65b9\u5f0f\u5982\u4e0b\uff1a export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py \u6ce8\uff1a\u5728\u5f00\u59cb\u524d\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\u5e76\u6307\u5b9aGPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 \u8bad\u7ec3\u547d\u4ee4\u5982\u4e0b\uff1a python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval \u8bad\u7ec3\u4f7f\u7528 yolov3_mobilenet_v1 \u57fa\u4e8eCOCO\u6570\u636e\u96c6\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884cfinetune\u3002\u8bad\u7ec3\u671f\u95f4\u53ef\u4ee5\u901a\u8fc7tensorboard\u5b9e\u65f6\u89c2\u5bdfloss\u548c\u7cbe\u5ea6\u503c\uff0c\u542f\u52a8\u547d\u4ee4\u5982\u4e0b\uff1a tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> tensorboard\u7ed3\u679c\u663e\u793a\u5982\u4e0b\uff1a \u8bad\u7ec3\u6a21\u578b \u4e0b\u8f7d\u94fe\u63a5 \u8bc4\u4f30\u547d\u4ee4\u5982\u4e0b\uff1a python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml \u9884\u6d4b\u547d\u4ee4\u5982\u4e0b python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg \u9884\u6d4b\u56fe\u7247\u5982\u4e0b\uff1a \u66f4\u591a\u8bad\u7ec3\u53ca\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8bf7\u53c2\u8003 GETTING_STARTED_cn.md .","title":"QUICK STARTED cn"},{"location":"QUICK_STARTED_cn/#_1","text":"\u4e3a\u4e86\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5728\u5f88\u77ed\u7684\u65f6\u95f4\u5185\u5feb\u901f\u4ea7\u51fa\u6a21\u578b\uff0c\u638c\u63e1PaddleDetection\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u8fd9\u7bc7\u6559\u7a0b\u901a\u8fc7\u4e00\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u5bf9\u5c0f\u6570\u636e\u96c6\u8fdb\u884cfinetune\u3002\u5728P40\u4e0a\u5355\u5361\u5927\u7ea620min\u5373\u53ef\u4ea7\u51fa\u4e00\u4e2a\u6548\u679c\u4e0d\u9519\u7684\u6a21\u578b\u3002","title":"\u5feb\u901f\u5f00\u59cb"},{"location":"QUICK_STARTED_cn/#_2","text":"\u6570\u636e\u96c6\u53c2\u8003 Kaggle\u6570\u636e\u96c6 \uff0c\u5176\u4e2d\u8bad\u7ec3\u6570\u636e\u96c6240\u5f20\u56fe\u7247\uff0c\u6d4b\u8bd5\u6570\u636e\u96c660\u5f20\u56fe\u7247\uff0c\u6570\u636e\u7c7b\u522b\u4e3a3\u7c7b\uff1a\u82f9\u679c\uff0c\u6a58\u5b50\uff0c\u9999\u8549\u3002 \u4e0b\u8f7d\u94fe\u63a5 \u3002\u6570\u636e\u4e0b\u8f7d\u540e\u5206\u522b\u89e3\u538b\u5373\u53ef, \u6570\u636e\u51c6\u5907\u811a\u672c\u4f4d\u4e8e download_fruit.py \u3002\u4e0b\u8f7d\u6570\u636e\u65b9\u5f0f\u5982\u4e0b\uff1a export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py \u6ce8\uff1a\u5728\u5f00\u59cb\u524d\uff0c\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\u5e76\u6307\u5b9aGPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 \u8bad\u7ec3\u547d\u4ee4\u5982\u4e0b\uff1a python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval \u8bad\u7ec3\u4f7f\u7528 yolov3_mobilenet_v1 \u57fa\u4e8eCOCO\u6570\u636e\u96c6\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884cfinetune\u3002\u8bad\u7ec3\u671f\u95f4\u53ef\u4ee5\u901a\u8fc7tensorboard\u5b9e\u65f6\u89c2\u5bdfloss\u548c\u7cbe\u5ea6\u503c\uff0c\u542f\u52a8\u547d\u4ee4\u5982\u4e0b\uff1a tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> tensorboard\u7ed3\u679c\u663e\u793a\u5982\u4e0b\uff1a \u8bad\u7ec3\u6a21\u578b \u4e0b\u8f7d\u94fe\u63a5 \u8bc4\u4f30\u547d\u4ee4\u5982\u4e0b\uff1a python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml \u9884\u6d4b\u547d\u4ee4\u5982\u4e0b python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg \u9884\u6d4b\u56fe\u7247\u5982\u4e0b\uff1a \u66f4\u591a\u8bad\u7ec3\u53ca\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8bf7\u53c2\u8003 GETTING_STARTED_cn.md .","title":"\u6570\u636e\u51c6\u5907"},{"location":"TRANSFER_LEARNING/","text":"English | \u7b80\u4f53\u4e2d\u6587 Transfer Learning Transfer learning aims at learning new knowledge from existing knowledge. For example, take pretrained model from ImageNet to initialize detection models, or take pretrained model from COCO dataset to initialize train detection models in PascalVOC dataset. In transfer learning, if different dataset and the number of classes is used, the dimensional inconsistency will causes in loading parameters related to the number of classes; On the other hand, if more complicated model is used, need to motify the open-source model construction and selective load parameters. Thus, PaddleDetection should designate parameter fields and ignore loading the parameters which match the fields. Transfer Learning in PaddleDetection In transfer learning, it's needed to load pretrained model selectively. The following two methods can be used: Set finetune_exclude_pretrained_params in YAML configuration files. Please refer to configure file Set -o finetune_exclude_pretrained_params in command line. For example: export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] Note: The path in pretrain_weights is the open-source model link of faster RCNN from COCO dataset. For full models link, please refer to MODEL_ZOO The parameter fields are set in finetune_exclude_pretrained_params. If the name of parameter matches field (wildcard matching), the parameter will be ignored in loading. If users want to fine-tune by own dataet, and remain the model construction, need to ignore the parameters related to the number of classes. PaddleDetection lists ignored parameter fields corresponding to different model type. The table is shown below: model type ignored parameter fields Faster RCNN cls_score, bbox_pred Cascade RCNN cls_score, bbox_pred Mask RCNN cls_score, bbox_pred, mask_fcn_logits Cascade-Mask RCNN cls_score, bbox_pred, mask_fcn_logits RetinaNet retnet_cls_pred_fpn SSD ^conv2d_ YOLOv3 yolo_output","title":"TRANSFER LEARNING"},{"location":"TRANSFER_LEARNING/#transfer-learning","text":"Transfer learning aims at learning new knowledge from existing knowledge. For example, take pretrained model from ImageNet to initialize detection models, or take pretrained model from COCO dataset to initialize train detection models in PascalVOC dataset. In transfer learning, if different dataset and the number of classes is used, the dimensional inconsistency will causes in loading parameters related to the number of classes; On the other hand, if more complicated model is used, need to motify the open-source model construction and selective load parameters. Thus, PaddleDetection should designate parameter fields and ignore loading the parameters which match the fields.","title":"Transfer Learning"},{"location":"TRANSFER_LEARNING/#transfer-learning-in-paddledetection","text":"In transfer learning, it's needed to load pretrained model selectively. The following two methods can be used: Set finetune_exclude_pretrained_params in YAML configuration files. Please refer to configure file Set -o finetune_exclude_pretrained_params in command line. For example: export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] Note: The path in pretrain_weights is the open-source model link of faster RCNN from COCO dataset. For full models link, please refer to MODEL_ZOO The parameter fields are set in finetune_exclude_pretrained_params. If the name of parameter matches field (wildcard matching), the parameter will be ignored in loading. If users want to fine-tune by own dataet, and remain the model construction, need to ignore the parameters related to the number of classes. PaddleDetection lists ignored parameter fields corresponding to different model type. The table is shown below: model type ignored parameter fields Faster RCNN cls_score, bbox_pred Cascade RCNN cls_score, bbox_pred Mask RCNN cls_score, bbox_pred, mask_fcn_logits Cascade-Mask RCNN cls_score, bbox_pred, mask_fcn_logits RetinaNet retnet_cls_pred_fpn SSD ^conv2d_ YOLOv3 yolo_output","title":"Transfer Learning in PaddleDetection"},{"location":"TRANSFER_LEARNING_cn/","text":"\u8fc1\u79fb\u5b66\u4e60 \u8fc1\u79fb\u5b66\u4e60\u4e3a\u5229\u7528\u5df2\u6709\u77e5\u8bc6\uff0c\u5bf9\u65b0\u77e5\u8bc6\u8fdb\u884c\u5b66\u4e60\u3002\u4f8b\u5982\u5229\u7528ImageNet\u5206\u7c7b\u9884\u8bad\u7ec3\u6a21\u578b\u505a\u521d\u59cb\u5316\u6765\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u5728COCO\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u6a21\u578b\u505a\u521d\u59cb\u5316\u6765\u8bad\u7ec3\u57fa\u4e8ePascalVOC\u6570\u636e\u96c6\u7684\u68c0\u6d4b\u6a21\u578b\u3002 \u5728\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u65f6\uff0c\u7531\u4e8e\u4f1a\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff0c\u6570\u636e\u7c7b\u522b\u6570\u4e0eCOCO/VOC\u6570\u636e\u7c7b\u522b\u4e0d\u540c\uff0c\u5bfc\u81f4\u5728\u52a0\u8f7dPaddlePaddle\u5f00\u6e90\u6a21\u578b\u65f6\uff0c\u4e0e\u7c7b\u522b\u6570\u76f8\u5173\u7684\u6743\u91cd\uff08\u4f8b\u5982\u5206\u7c7b\u6a21\u5757\u7684fc\u5c42\uff09\u4f1a\u51fa\u73b0\u7ef4\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff1b\u53e6\u5916\uff0c\u5982\u679c\u9700\u8981\u7ed3\u6784\u66f4\u52a0\u590d\u6742\u7684\u6a21\u578b\uff0c\u9700\u8981\u5bf9\u5df2\u6709\u5f00\u6e90\u6a21\u578b\u7ed3\u6784\u8fdb\u884c\u8c03\u6574\uff0c\u5bf9\u5e94\u6743\u91cd\u4e5f\u9700\u8981\u9009\u62e9\u6027\u52a0\u8f7d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u68c0\u6d4b\u5e93\u80fd\u591f\u6307\u5b9a\u53c2\u6570\u5b57\u6bb5\uff0c\u5728\u52a0\u8f7d\u6a21\u578b\u65f6\u4e0d\u52a0\u8f7d\u5339\u914d\u7684\u6743\u91cd\u3002 PaddleDetection\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60 \u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9009\u62e9\u6027\u52a0\u8f7d\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a \u5728 YMAL \u914d\u7f6e\u6587\u4ef6\u4e2d\u901a\u8fc7\u8bbe\u7f6e finetune_exclude_pretrained_params \u5b57\u6bb5\u3002\u53ef\u53c2\u8003 \u914d\u7f6e\u6587\u4ef6 \u5728 train.py\u7684\u542f\u52a8\u53c2\u6570\u4e2d\u8bbe\u7f6e -o finetune_exclude_pretrained_params\u3002\u4f8b\u5982\uff1a export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] \u8bf4\u660e\uff1a pretrain_weights\u7684\u8def\u5f84\u4e3aCOCO\u6570\u636e\u96c6\u4e0a\u5f00\u6e90\u7684faster RCNN\u6a21\u578b\u94fe\u63a5\uff0c\u5b8c\u6574\u6a21\u578b\u94fe\u63a5\u53ef\u53c2\u8003 MODEL_ZOO finetune_exclude_pretrained_params\u4e2d\u8bbe\u7f6e\u53c2\u6570\u5b57\u6bb5\uff0c\u5982\u679c\u53c2\u6570\u540d\u80fd\u591f\u5339\u914d\u4ee5\u4e0a\u53c2\u6570\u5b57\u6bb5\uff08\u901a\u914d\u7b26\u5339\u914d\u65b9\u5f0f\uff09\uff0c\u5219\u5728\u6a21\u578b\u52a0\u8f7d\u65f6\u5ffd\u7565\u8be5\u53c2\u6570\u3002 \u5982\u679c\u7528\u6237\u9700\u8981\u5229\u7528\u81ea\u5df1\u7684\u6570\u636e\u8fdb\u884cfinetune\uff0c\u6a21\u578b\u7ed3\u6784\u4e0d\u53d8\uff0c\u53ea\u9700\u8981\u5ffd\u7565\u4e0e\u7c7b\u522b\u6570\u76f8\u5173\u7684\u53c2\u6570\u3002PaddleDetection\u7ed9\u51fa\u4e86\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u6240\u5bf9\u5e94\u7684\u5ffd\u7565\u53c2\u6570\u5b57\u6bb5\u3002\u5982\u4e0b\u8868\u6240\u793a\uff1a \u6a21\u578b\u7c7b\u578b \u5ffd\u7565\u53c2\u6570\u5b57\u6bb5 Faster RCNN cls_score, bbox_pred Cascade RCNN cls_score, bbox_pred Mask RCNN cls_score, bbox_pred, mask_fcn_logits Cascade-Mask RCNN cls_score, bbox_pred, mask_fcn_logits RetinaNet retnet_cls_pred_fpn SSD ^conv2d_ YOLOv3 yolo_output","title":"\u8fc1\u79fb\u5b66\u4e60"},{"location":"TRANSFER_LEARNING_cn/#_1","text":"\u8fc1\u79fb\u5b66\u4e60\u4e3a\u5229\u7528\u5df2\u6709\u77e5\u8bc6\uff0c\u5bf9\u65b0\u77e5\u8bc6\u8fdb\u884c\u5b66\u4e60\u3002\u4f8b\u5982\u5229\u7528ImageNet\u5206\u7c7b\u9884\u8bad\u7ec3\u6a21\u578b\u505a\u521d\u59cb\u5316\u6765\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u5728COCO\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u6a21\u578b\u505a\u521d\u59cb\u5316\u6765\u8bad\u7ec3\u57fa\u4e8ePascalVOC\u6570\u636e\u96c6\u7684\u68c0\u6d4b\u6a21\u578b\u3002 \u5728\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u65f6\uff0c\u7531\u4e8e\u4f1a\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff0c\u6570\u636e\u7c7b\u522b\u6570\u4e0eCOCO/VOC\u6570\u636e\u7c7b\u522b\u4e0d\u540c\uff0c\u5bfc\u81f4\u5728\u52a0\u8f7dPaddlePaddle\u5f00\u6e90\u6a21\u578b\u65f6\uff0c\u4e0e\u7c7b\u522b\u6570\u76f8\u5173\u7684\u6743\u91cd\uff08\u4f8b\u5982\u5206\u7c7b\u6a21\u5757\u7684fc\u5c42\uff09\u4f1a\u51fa\u73b0\u7ef4\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff1b\u53e6\u5916\uff0c\u5982\u679c\u9700\u8981\u7ed3\u6784\u66f4\u52a0\u590d\u6742\u7684\u6a21\u578b\uff0c\u9700\u8981\u5bf9\u5df2\u6709\u5f00\u6e90\u6a21\u578b\u7ed3\u6784\u8fdb\u884c\u8c03\u6574\uff0c\u5bf9\u5e94\u6743\u91cd\u4e5f\u9700\u8981\u9009\u62e9\u6027\u52a0\u8f7d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u68c0\u6d4b\u5e93\u80fd\u591f\u6307\u5b9a\u53c2\u6570\u5b57\u6bb5\uff0c\u5728\u52a0\u8f7d\u6a21\u578b\u65f6\u4e0d\u52a0\u8f7d\u5339\u914d\u7684\u6743\u91cd\u3002","title":"\u8fc1\u79fb\u5b66\u4e60"},{"location":"TRANSFER_LEARNING_cn/#paddledetection","text":"\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\uff0c\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9009\u62e9\u6027\u52a0\u8f7d\uff0c\u53ef\u901a\u8fc7\u5982\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a \u5728 YMAL \u914d\u7f6e\u6587\u4ef6\u4e2d\u901a\u8fc7\u8bbe\u7f6e finetune_exclude_pretrained_params \u5b57\u6bb5\u3002\u53ef\u53c2\u8003 \u914d\u7f6e\u6587\u4ef6 \u5728 train.py\u7684\u542f\u52a8\u53c2\u6570\u4e2d\u8bbe\u7f6e -o finetune_exclude_pretrained_params\u3002\u4f8b\u5982\uff1a export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ finetune_exclude_pretrained_params=['cls_score','bbox_pred'] \u8bf4\u660e\uff1a pretrain_weights\u7684\u8def\u5f84\u4e3aCOCO\u6570\u636e\u96c6\u4e0a\u5f00\u6e90\u7684faster RCNN\u6a21\u578b\u94fe\u63a5\uff0c\u5b8c\u6574\u6a21\u578b\u94fe\u63a5\u53ef\u53c2\u8003 MODEL_ZOO finetune_exclude_pretrained_params\u4e2d\u8bbe\u7f6e\u53c2\u6570\u5b57\u6bb5\uff0c\u5982\u679c\u53c2\u6570\u540d\u80fd\u591f\u5339\u914d\u4ee5\u4e0a\u53c2\u6570\u5b57\u6bb5\uff08\u901a\u914d\u7b26\u5339\u914d\u65b9\u5f0f\uff09\uff0c\u5219\u5728\u6a21\u578b\u52a0\u8f7d\u65f6\u5ffd\u7565\u8be5\u53c2\u6570\u3002 \u5982\u679c\u7528\u6237\u9700\u8981\u5229\u7528\u81ea\u5df1\u7684\u6570\u636e\u8fdb\u884cfinetune\uff0c\u6a21\u578b\u7ed3\u6784\u4e0d\u53d8\uff0c\u53ea\u9700\u8981\u5ffd\u7565\u4e0e\u7c7b\u522b\u6570\u76f8\u5173\u7684\u53c2\u6570\u3002PaddleDetection\u7ed9\u51fa\u4e86\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u6240\u5bf9\u5e94\u7684\u5ffd\u7565\u53c2\u6570\u5b57\u6bb5\u3002\u5982\u4e0b\u8868\u6240\u793a\uff1a \u6a21\u578b\u7c7b\u578b \u5ffd\u7565\u53c2\u6570\u5b57\u6bb5 Faster RCNN cls_score, bbox_pred Cascade RCNN cls_score, bbox_pred Mask RCNN cls_score, bbox_pred, mask_fcn_logits Cascade-Mask RCNN cls_score, bbox_pred, mask_fcn_logits RetinaNet retnet_cls_pred_fpn SSD ^conv2d_ YOLOv3 yolo_output","title":"PaddleDetection\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60"},{"location":"YOLOv3_ENHANCEMENT/","text":"YOLOv3\u589e\u5f3a\u6a21\u578b \u7b80\u4ecb YOLOv3 \u662f\u7531 Joseph Redmon \u548c Ali Farhadi \u63d0\u51fa\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668, \u8be5\u68c0\u6d4b \u5668\u4e0e\u8fbe\u5230\u540c\u6837\u7cbe\u5ea6\u7684\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63a8\u65ad\u901f\u5ea6\u80fd\u8fbe\u5230\u63a5\u8fd1\u4e24\u500d. PaddleDetection\u5b9e\u73b0\u7248\u672c\u4e2d\u4f7f\u7528\u4e86 Bag of Freebies for Training Object Detection Neural Networks \u4e2d\u63d0\u51fa\u7684\u56fe\u50cf\u589e\u5f3a\u548clabel smooth\u7b49\u4f18\u5316\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u4f18\u4e8edarknet\u6846\u67b6\u7684\u5b9e\u73b0\u7248\u672c\uff0c\u5728COCO-2017\u6570\u636e\u96c6\u4e0a\uff0cYOLOv3(DarkNet)\u8fbe\u5230 mAP(0.50:0.95)= 38.9 \u7684\u7cbe\u5ea6\uff0c\u6bd4darknet\u5b9e\u73b0\u7248\u672c\u7684\u7cbe\u5ea6(33.0)\u8981\u9ad85.9\u3002\u540c\u65f6\uff0c\u5728\u63a8\u65ad\u901f\u5ea6\u65b9\u9762\uff0c\u57fa\u4e8ePaddle\u9884\u6d4b\u5e93\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u63a8\u65ad\u901f\u5ea6\u6bd4darknet\u9ad830%\u3002 \u5728\u6b64\u57fa\u7840\u4e0a\uff0cPaddleDetection\u5bf9YOLOv3\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5f97\u5230\u4e86\u66f4\u5927\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u4f18\u52bf\u3002 \u65b9\u6cd5\u63cf\u8ff0 \u5c06YOLOv3\u9aa8\u67b6\u7f51\u7edc\u66f4\u6362\u4e3aResNet50-vd\uff0c\u540c\u65f6\u5728\u6700\u540e\u4e00\u4e2aResidual block\u4e2d\u5f15\u5165 Deformable convolution v2 (\u53ef\u53d8\u5f62\u5377\u79ef)\u66ff\u4ee3\u539f\u59cb\u5377\u79ef\u64cd\u4f5c\u3002\u53e6\u5916\uff0c\u4f7f\u7528 object365\u6570\u636e\u96c6 \u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4f5c\u4e3acoco\u6570\u636e\u96c6\u4e0a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8YOLOv3\u7684\u7cbe\u5ea6\u3002 \u4f7f\u7528\u65b9\u6cd5 \u6a21\u578b\u8bad\u7ec3 export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/dcn/yolov3_r50vd_dcn.yml \u66f4\u591a\u6a21\u578b\u53c2\u6570\u8bf7\u4f7f\u7528 python tools/train.py --help \u67e5\u770b\uff0c\u6216\u53c2\u8003 \u8bad\u7ec3\u3001\u8bc4\u4f30\u53ca\u53c2\u6570\u8bf4\u660e \u6587\u6863 \u6a21\u578b\u6548\u679c \u6a21\u578b \u9884\u8bad\u7ec3\u6a21\u578b \u9a8c\u8bc1\u96c6 mAP P4\u9884\u6d4b\u901f\u5ea6 \u4e0b\u8f7d YOLOv3 DarkNet DarkNet pretrain 38.9 \u539f\u751f\uff1a88.3ms tensorRT-FP32: 42.5ms \u4e0b\u8f7d\u94fe\u63a5 YOLOv3 ResNet50_vd dcn ImageNet pretrain 39.1 \u539f\u751f\uff1a74.4ms tensorRT-FP32: 35.2ms \u4e0b\u8f7d\u94fe\u63a5 YOLOv3 ResNet50_vd dcn Object365 pretrain 41.4 \u539f\u751f\uff1a74.4ms tensorRT-FP32: 35.2ms \u4e0b\u8f7d\u94fe\u63a5","title":"YOLOv3\u589e\u5f3a\u6a21\u578b"},{"location":"YOLOv3_ENHANCEMENT/#yolov3","text":"","title":"YOLOv3\u589e\u5f3a\u6a21\u578b"},{"location":"YOLOv3_ENHANCEMENT/#_1","text":"YOLOv3 \u662f\u7531 Joseph Redmon \u548c Ali Farhadi \u63d0\u51fa\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668, \u8be5\u68c0\u6d4b \u5668\u4e0e\u8fbe\u5230\u540c\u6837\u7cbe\u5ea6\u7684\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63a8\u65ad\u901f\u5ea6\u80fd\u8fbe\u5230\u63a5\u8fd1\u4e24\u500d. PaddleDetection\u5b9e\u73b0\u7248\u672c\u4e2d\u4f7f\u7528\u4e86 Bag of Freebies for Training Object Detection Neural Networks \u4e2d\u63d0\u51fa\u7684\u56fe\u50cf\u589e\u5f3a\u548clabel smooth\u7b49\u4f18\u5316\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u4f18\u4e8edarknet\u6846\u67b6\u7684\u5b9e\u73b0\u7248\u672c\uff0c\u5728COCO-2017\u6570\u636e\u96c6\u4e0a\uff0cYOLOv3(DarkNet)\u8fbe\u5230 mAP(0.50:0.95)= 38.9 \u7684\u7cbe\u5ea6\uff0c\u6bd4darknet\u5b9e\u73b0\u7248\u672c\u7684\u7cbe\u5ea6(33.0)\u8981\u9ad85.9\u3002\u540c\u65f6\uff0c\u5728\u63a8\u65ad\u901f\u5ea6\u65b9\u9762\uff0c\u57fa\u4e8ePaddle\u9884\u6d4b\u5e93\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u63a8\u65ad\u901f\u5ea6\u6bd4darknet\u9ad830%\u3002 \u5728\u6b64\u57fa\u7840\u4e0a\uff0cPaddleDetection\u5bf9YOLOv3\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u5f97\u5230\u4e86\u66f4\u5927\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u4f18\u52bf\u3002","title":"\u7b80\u4ecb"},{"location":"YOLOv3_ENHANCEMENT/#_2","text":"\u5c06YOLOv3\u9aa8\u67b6\u7f51\u7edc\u66f4\u6362\u4e3aResNet50-vd\uff0c\u540c\u65f6\u5728\u6700\u540e\u4e00\u4e2aResidual block\u4e2d\u5f15\u5165 Deformable convolution v2 (\u53ef\u53d8\u5f62\u5377\u79ef)\u66ff\u4ee3\u539f\u59cb\u5377\u79ef\u64cd\u4f5c\u3002\u53e6\u5916\uff0c\u4f7f\u7528 object365\u6570\u636e\u96c6 \u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4f5c\u4e3acoco\u6570\u636e\u96c6\u4e0a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8YOLOv3\u7684\u7cbe\u5ea6\u3002","title":"\u65b9\u6cd5\u63cf\u8ff0"},{"location":"YOLOv3_ENHANCEMENT/#_3","text":"","title":"\u4f7f\u7528\u65b9\u6cd5"},{"location":"YOLOv3_ENHANCEMENT/#_4","text":"export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/dcn/yolov3_r50vd_dcn.yml \u66f4\u591a\u6a21\u578b\u53c2\u6570\u8bf7\u4f7f\u7528 python tools/train.py --help \u67e5\u770b\uff0c\u6216\u53c2\u8003 \u8bad\u7ec3\u3001\u8bc4\u4f30\u53ca\u53c2\u6570\u8bf4\u660e \u6587\u6863","title":"\u6a21\u578b\u8bad\u7ec3"},{"location":"YOLOv3_ENHANCEMENT/#_5","text":"\u6a21\u578b \u9884\u8bad\u7ec3\u6a21\u578b \u9a8c\u8bc1\u96c6 mAP P4\u9884\u6d4b\u901f\u5ea6 \u4e0b\u8f7d YOLOv3 DarkNet DarkNet pretrain 38.9 \u539f\u751f\uff1a88.3ms tensorRT-FP32: 42.5ms \u4e0b\u8f7d\u94fe\u63a5 YOLOv3 ResNet50_vd dcn ImageNet pretrain 39.1 \u539f\u751f\uff1a74.4ms tensorRT-FP32: 35.2ms \u4e0b\u8f7d\u94fe\u63a5 YOLOv3 ResNet50_vd dcn Object365 pretrain 41.4 \u539f\u751f\uff1a74.4ms tensorRT-FP32: 35.2ms \u4e0b\u8f7d\u94fe\u63a5","title":"\u6a21\u578b\u6548\u679c"}]}