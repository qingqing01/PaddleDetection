{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"EXPORT_MODEL/","text":"\u6a21\u578b\u5bfc\u51fa \u8bad\u7ec3\u5f97\u5230\u4e00\u4e2a\u6ee1\u8db3\u8981\u6c42\u7684\u6a21\u578b\u540e\uff0c\u5982\u679c\u60f3\u8981\u5c06\u8be5\u6a21\u578b\u63a5\u5165\u5230C++\u9884\u6d4b\u5e93\u6216\u8005Serving\u670d\u52a1\uff0c\u9700\u8981\u901a\u8fc7 tools/export_model.py \u5bfc\u51fa\u8be5\u6a21\u578b\u3002 \u542f\u52a8\u53c2\u6570\u8bf4\u660e FLAG \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None --output_dir \u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./output \u6a21\u578b\u9ed8\u8ba4\u4fdd\u5b58\u5728 output/\u914d\u7f6e\u6587\u4ef6\u540d/ \u8def\u5f84\u4e0b \u4f7f\u7528\u793a\u4f8b \u4f7f\u7528 \u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad \u4e2d\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u8fdb\u884c\u8bd5\u7528\uff0c\u811a\u672c\u5982\u4e0b # \u5bfc\u51faFasterRCNN\u6a21\u578b, \u6a21\u578b\u4e2ddata\u5c42\u9ed8\u8ba4\u7684shape\u4e3a3x800x1333 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ \u9884\u6d4b\u6a21\u578b\u4f1a\u5bfc\u51fa\u5230 inference_model/faster_rcnn_r50_1x \u76ee\u5f55\u4e0b\uff0c\u6a21\u578b\u540d\u548c\u53c2\u6570\u540d\u5206\u522b\u4e3a __model__ \u548c __params__ \u3002 \u8bbe\u7f6e\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u5927\u5c0f \u4f7f\u7528Fluid-TensorRT\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u7531\u4e8e<=TensorRT 5.1\u7684\u7248\u672c\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u4fdd\u5b58\u6a21\u578b\u7684 data \u5c42\u7684\u56fe\u7247\u5927\u5c0f\u9700\u8981\u548c\u5b9e\u9645\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4e00\u81f4\u3002\u800cFluid C++\u9884\u6d4b\u5f15\u64ce\u6ca1\u6709\u6b64\u9650\u5236\u3002\u53ef\u901a\u8fc7\u8bbe\u7f6eTestFeed\u7684 image_shape \u53ef\u4ee5\u4fee\u6539\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u3002\u793a\u4f8b\u5982\u4e0b: # \u5bfc\u51faFasterRCNN\u6a21\u578b\uff0c\u8f93\u5165\u662f3x640x640 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ FasterRCNNTestFeed.image_shape=[3,640,640] # \u5bfc\u51faYOLOv3\u6a21\u578b\uff0c\u8f93\u5165\u662f3x320x320 python tools/export_model.py -c configs/yolov3_darknet.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_darknet.tar \\ YoloTestFeed.image_shape=[3,320,320] # \u5bfc\u51faSSD\u6a21\u578b\uff0c\u8f93\u5165\u662f3x300x300 python tools/export_model.py -c configs/ssd/ssd_mobilenet_v1_voc.yml \\ --output_dir=./inference_model \\ -o weights= https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_voc.tar \\ SSDTestFeed.image_shape=[3,300,300]","title":"Export model"},{"location":"EXPORT_MODEL/#_1","text":"\u8bad\u7ec3\u5f97\u5230\u4e00\u4e2a\u6ee1\u8db3\u8981\u6c42\u7684\u6a21\u578b\u540e\uff0c\u5982\u679c\u60f3\u8981\u5c06\u8be5\u6a21\u578b\u63a5\u5165\u5230C++\u9884\u6d4b\u5e93\u6216\u8005Serving\u670d\u52a1\uff0c\u9700\u8981\u901a\u8fc7 tools/export_model.py \u5bfc\u51fa\u8be5\u6a21\u578b\u3002","title":"\u6a21\u578b\u5bfc\u51fa"},{"location":"EXPORT_MODEL/#_2","text":"FLAG \u7528\u9014 \u9ed8\u8ba4\u503c \u5907\u6ce8 -c \u6307\u5b9a\u914d\u7f6e\u6587\u4ef6 None --output_dir \u6a21\u578b\u4fdd\u5b58\u8def\u5f84 ./output \u6a21\u578b\u9ed8\u8ba4\u4fdd\u5b58\u5728 output/\u914d\u7f6e\u6587\u4ef6\u540d/ \u8def\u5f84\u4e0b","title":"\u542f\u52a8\u53c2\u6570\u8bf4\u660e"},{"location":"EXPORT_MODEL/#_3","text":"\u4f7f\u7528 \u8bad\u7ec3/\u8bc4\u4f30/\u63a8\u65ad \u4e2d\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u8fdb\u884c\u8bd5\u7528\uff0c\u811a\u672c\u5982\u4e0b # \u5bfc\u51faFasterRCNN\u6a21\u578b, \u6a21\u578b\u4e2ddata\u5c42\u9ed8\u8ba4\u7684shape\u4e3a3x800x1333 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ \u9884\u6d4b\u6a21\u578b\u4f1a\u5bfc\u51fa\u5230 inference_model/faster_rcnn_r50_1x \u76ee\u5f55\u4e0b\uff0c\u6a21\u578b\u540d\u548c\u53c2\u6570\u540d\u5206\u522b\u4e3a __model__ \u548c __params__ \u3002","title":"\u4f7f\u7528\u793a\u4f8b"},{"location":"EXPORT_MODEL/#_4","text":"\u4f7f\u7528Fluid-TensorRT\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u7531\u4e8e<=TensorRT 5.1\u7684\u7248\u672c\u4ec5\u652f\u6301\u5b9a\u957f\u8f93\u5165\uff0c\u4fdd\u5b58\u6a21\u578b\u7684 data \u5c42\u7684\u56fe\u7247\u5927\u5c0f\u9700\u8981\u548c\u5b9e\u9645\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u4e00\u81f4\u3002\u800cFluid C++\u9884\u6d4b\u5f15\u64ce\u6ca1\u6709\u6b64\u9650\u5236\u3002\u53ef\u901a\u8fc7\u8bbe\u7f6eTestFeed\u7684 image_shape \u53ef\u4ee5\u4fee\u6539\u4fdd\u5b58\u6a21\u578b\u4e2d\u7684\u8f93\u5165\u56fe\u7247\u5927\u5c0f\u3002\u793a\u4f8b\u5982\u4e0b: # \u5bfc\u51faFasterRCNN\u6a21\u578b\uff0c\u8f93\u5165\u662f3x640x640 python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ FasterRCNNTestFeed.image_shape=[3,640,640] # \u5bfc\u51faYOLOv3\u6a21\u578b\uff0c\u8f93\u5165\u662f3x320x320 python tools/export_model.py -c configs/yolov3_darknet.yml \\ --output_dir=./inference_model \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_darknet.tar \\ YoloTestFeed.image_shape=[3,320,320] # \u5bfc\u51faSSD\u6a21\u578b\uff0c\u8f93\u5165\u662f3x300x300 python tools/export_model.py -c configs/ssd/ssd_mobilenet_v1_voc.yml \\ --output_dir=./inference_model \\ -o weights= https://paddlemodels.bj.bcebos.com/object_detection/ssd_mobilenet_v1_voc.tar \\ SSDTestFeed.image_shape=[3,300,300]","title":"\u8bbe\u7f6e\u5bfc\u51fa\u6a21\u578b\u7684\u8f93\u5165\u5927\u5c0f"},{"location":"GETTING_STARTED/","text":"English | \u7b80\u4f53\u4e2d\u6587 Getting Started For setting up the running environment, please refer to installation instructions . Training/Evaluation/Inference PaddleDetection provides scripots for training, evalution and inference with various features according to different configure. # set PYTHONPATH export PYTHONPATH=$PYTHONPATH:. # training in single-GPU and multi-GPU. specify different GPU numbers by CUDA_VISIBLE_DEVICES export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU evalution export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # Inference python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg Optional argument list list below can be viewed by --help FLAG script supported description default remark -c ALL Select config file None The whole description of configure can refer to config_example -o ALL Set parameters in configure file None -o has higher priority to file configured by -c . Such as -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train Checkpoint path for resuming training None -r output/faster_rcnn_r50_1x/10000 --eval train Whether to perform evaluation in training False --output_eval train/eval json path in evalution current path --output_eval ./json_result -d/--dataset_dir train/eval path for dataset, same as dataset_dir in configs None -d dataset/coco --fp16 train Whether to enable mixed precision training False GPU training is required --loss_scale train Loss scaling factor for mixed precision training 8.0 enable when --fp16 is True --json_eval eval Whether to evaluate with already existed bbox.json or mask.json False json path is set in --output_eval --output_dir infer Directory for storing the output visualization files ./output --output_dir output --draw_threshold infer Threshold to reserve the result for visualization 0.5 --draw_threshold 0.7 --infer_dir infer Directory for images to perform inference on None --infer_img infer Image path None higher priority over --infer_dir --use_tb train/infer Whether to record the data with tb-paddle , so as to display in Tensorboard False --tb_log_dir train/infer tb-paddle logging directory for image train: tb_log_dir/scalar infer: tb_log_dir/image Examples Training Perform evaluation in training bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval Perform training and evalution alternatively and evaluate at each snapshot_iter. Meanwhile, the best model with highest MAP is saved at each snapshot_iter which has the same path as model_final . If evaluation dataset is large, we suggest decreasing evaluation times or evaluating after training. Fine-tune other task When using pre-trained model to fine-tune other task, two methods can be used: The excluded pre-trained parameters can be set by finetune_exclude_pretrained_params in YAML config Set -o finetune_exclude_pretrained_params in the arguments. bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params = ['cls_score','bbox_pred'] NOTES CUDA_VISIBLE_DEVICES can specify different gpu numbers. Such as: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU calculation rules can refer FAQ Dataset will be downloaded automatically and cached in ~/.cache/paddle/dataset if not be found locally. Pretrained model is downloaded automatically and cached in ~/.cache/paddle/weights . Checkpoints are saved in output by default, and can be revised from save_dir in configure files. RCNN models training on CPU is not supported on PaddlePaddle<=1.5.1 and will be fixed on later version. Mixed Precision Training Mixed precision training can be enabled with --fp16 flag. Currently Faster-FPN, Mask-FPN and Yolov3 have been verified to be working with little to no loss of precision (less than 0.2 mAP) To speed up mixed precision training, it is recommended to train in multi-process mode, for example python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml If loss becomes NaN during training, try tweak the --loss_scale value. Please refer to the Nvidia documentation on mixed precision training for details. Also, please note mixed precision training currently requires changing norm_type from affine_channel to bn . Evaluation Evaluate by specified weights path and dataset path bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco The path of model to be evaluted can be both local path and link in MODEL_ZOO . Evaluate with json bash export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ -f evaluation/ The json file must be named bbox.json or mask.json, placed in the evaluation/ directory. NOTES Multi-GPU evaluation for R-CNN and SSD models is not supported at the moment, but it is a planned feature Inference Output specified directory && Set up threshold bash export CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --use_tb=Ture --draw_threshold is an optional argument. Default is 0.5. Different thresholds will produce different results depending on the calculation of NMS . Export model bash python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ FasterRCNNTestFeed.image_shape=[3,800,1333] Save inference model tools/export_model.py , which can be loaded by PaddlePaddle predict library. FAQ Q: Why do I get NaN loss values during single GPU training? A: The default learning rate is tuned to multi-GPU training (8x GPUs), it must be adapted for single GPU training accordingly (e.g., divide by 8). The calculation rules are as follows\uff0cthey are equivalent: GPU number Learning rate Max_iters Milestones 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: How to reduce GPU memory usage? A: Setting environment variable FLAGS_conv_workspace_size_limit to a smaller number can reduce GPU memory footprint without affecting training speed. Take Mask-RCNN (R50) as example, by setting export FLAGS_conv_workspace_size_limit=512 , batch size could reach 4 per GPU (Tesla V100 16GB). Q: How to change data preprocessing? A: Set sample_transform in configuration. Note that the whole transforms need to be added in configuration. For example, DecodeImage , NormalizeImage and Permute in RCNN models. For detail description, please refer to config_example .","title":"GETTING STARTED"},{"location":"GETTING_STARTED/#getting-started","text":"For setting up the running environment, please refer to installation instructions .","title":"Getting Started"},{"location":"GETTING_STARTED/#trainingevaluationinference","text":"PaddleDetection provides scripots for training, evalution and inference with various features according to different configure. # set PYTHONPATH export PYTHONPATH=$PYTHONPATH:. # training in single-GPU and multi-GPU. specify different GPU numbers by CUDA_VISIBLE_DEVICES export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python tools/train.py -c configs/faster_rcnn_r50_1x.yml # GPU evalution export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml # Inference python tools/infer.py -c configs/faster_rcnn_r50_1x.yml --infer_img=demo/000000570688.jpg","title":"Training/Evaluation/Inference"},{"location":"GETTING_STARTED/#optional-argument-list","text":"list below can be viewed by --help FLAG script supported description default remark -c ALL Select config file None The whole description of configure can refer to config_example -o ALL Set parameters in configure file None -o has higher priority to file configured by -c . Such as -o use_gpu=False max_iter=10000 -r/--resume_checkpoint train Checkpoint path for resuming training None -r output/faster_rcnn_r50_1x/10000 --eval train Whether to perform evaluation in training False --output_eval train/eval json path in evalution current path --output_eval ./json_result -d/--dataset_dir train/eval path for dataset, same as dataset_dir in configs None -d dataset/coco --fp16 train Whether to enable mixed precision training False GPU training is required --loss_scale train Loss scaling factor for mixed precision training 8.0 enable when --fp16 is True --json_eval eval Whether to evaluate with already existed bbox.json or mask.json False json path is set in --output_eval --output_dir infer Directory for storing the output visualization files ./output --output_dir output --draw_threshold infer Threshold to reserve the result for visualization 0.5 --draw_threshold 0.7 --infer_dir infer Directory for images to perform inference on None --infer_img infer Image path None higher priority over --infer_dir --use_tb train/infer Whether to record the data with tb-paddle , so as to display in Tensorboard False --tb_log_dir train/infer tb-paddle logging directory for image train: tb_log_dir/scalar infer: tb_log_dir/image","title":"Optional argument list"},{"location":"GETTING_STARTED/#examples","text":"","title":"Examples"},{"location":"GETTING_STARTED/#training","text":"Perform evaluation in training bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml --eval Perform training and evalution alternatively and evaluate at each snapshot_iter. Meanwhile, the best model with highest MAP is saved at each snapshot_iter which has the same path as model_final . If evaluation dataset is large, we suggest decreasing evaluation times or evaluating after training. Fine-tune other task When using pre-trained model to fine-tune other task, two methods can be used: The excluded pre-trained parameters can be set by finetune_exclude_pretrained_params in YAML config Set -o finetune_exclude_pretrained_params in the arguments. bash export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u tools/train.py -c configs/faster_rcnn_r50_1x.yml \\ -o pretrain_weights=output/faster_rcnn_r50_1x/model_final/ \\ finetune_exclude_pretrained_params = ['cls_score','bbox_pred']","title":"Training"},{"location":"GETTING_STARTED/#notes","text":"CUDA_VISIBLE_DEVICES can specify different gpu numbers. Such as: export CUDA_VISIBLE_DEVICES=0,1,2,3 . GPU calculation rules can refer FAQ Dataset will be downloaded automatically and cached in ~/.cache/paddle/dataset if not be found locally. Pretrained model is downloaded automatically and cached in ~/.cache/paddle/weights . Checkpoints are saved in output by default, and can be revised from save_dir in configure files. RCNN models training on CPU is not supported on PaddlePaddle<=1.5.1 and will be fixed on later version.","title":"NOTES"},{"location":"GETTING_STARTED/#mixed-precision-training","text":"Mixed precision training can be enabled with --fp16 flag. Currently Faster-FPN, Mask-FPN and Yolov3 have been verified to be working with little to no loss of precision (less than 0.2 mAP) To speed up mixed precision training, it is recommended to train in multi-process mode, for example python -m paddle.distributed.launch --selected_gpus 0,1,2,3,4,5,6,7 tools/train.py --fp16 -c configs/faster_rcnn_r50_fpn_1x.yml If loss becomes NaN during training, try tweak the --loss_scale value. Please refer to the Nvidia documentation on mixed precision training for details. Also, please note mixed precision training currently requires changing norm_type from affine_channel to bn .","title":"Mixed Precision Training"},{"location":"GETTING_STARTED/#evaluation","text":"Evaluate by specified weights path and dataset path bash export CUDA_VISIBLE_DEVICES=0 python -u tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/faster_rcnn_r50_1x.tar \\ -d dataset/coco The path of model to be evaluted can be both local path and link in MODEL_ZOO . Evaluate with json bash export CUDA_VISIBLE_DEVICES=0 python tools/eval.py -c configs/faster_rcnn_r50_1x.yml \\ --json_eval \\ -f evaluation/ The json file must be named bbox.json or mask.json, placed in the evaluation/ directory.","title":"Evaluation"},{"location":"GETTING_STARTED/#notes_1","text":"Multi-GPU evaluation for R-CNN and SSD models is not supported at the moment, but it is a planned feature","title":"NOTES"},{"location":"GETTING_STARTED/#inference","text":"Output specified directory && Set up threshold bash export CUDA_VISIBLE_DEVICES=0 python tools/infer.py -c configs/faster_rcnn_r50_1x.yml \\ --infer_img=demo/000000570688.jpg \\ --output_dir=infer_output/ \\ --draw_threshold=0.5 \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ --use_tb=Ture --draw_threshold is an optional argument. Default is 0.5. Different thresholds will produce different results depending on the calculation of NMS . Export model bash python tools/export_model.py -c configs/faster_rcnn_r50_1x.yml \\ --output_dir=inference_model \\ -o weights=output/faster_rcnn_r50_1x/model_final \\ FasterRCNNTestFeed.image_shape=[3,800,1333] Save inference model tools/export_model.py , which can be loaded by PaddlePaddle predict library.","title":"Inference"},{"location":"GETTING_STARTED/#faq","text":"Q: Why do I get NaN loss values during single GPU training? A: The default learning rate is tuned to multi-GPU training (8x GPUs), it must be adapted for single GPU training accordingly (e.g., divide by 8). The calculation rules are as follows\uff0cthey are equivalent: GPU number Learning rate Max_iters Milestones 2 0.0025 720000 [480000, 640000] 4 0.005 360000 [240000, 320000] 8 0.01 180000 [120000, 160000] Q: How to reduce GPU memory usage? A: Setting environment variable FLAGS_conv_workspace_size_limit to a smaller number can reduce GPU memory footprint without affecting training speed. Take Mask-RCNN (R50) as example, by setting export FLAGS_conv_workspace_size_limit=512 , batch size could reach 4 per GPU (Tesla V100 16GB). Q: How to change data preprocessing? A: Set sample_transform in configuration. Note that the whole transforms need to be added in configuration. For example, DecodeImage , NormalizeImage and Permute in RCNN models. For detail description, please refer to config_example .","title":"FAQ"},{"location":"INSTALL/","text":"English | \u7b80\u4f53\u4e2d\u6587 Installation Table of Contents Introduction PaddlePaddle Other Dependencies PaddleDetection Datasets Introduction This document covers how to install PaddleDetection, its dependencies (including PaddlePaddle), together with COCO and Pascal VOC dataset. PaddlePaddle Running PaddleDetection requires PaddlePaddle Fluid v.1.6 and later. please follow the instructions in installation document . Please make sure your PaddlePaddle installation was successful and the version of your PaddlePaddle is not lower than required. Verify with the following commands. # To check PaddlePaddle installation in your Python interpreter >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # To check PaddlePaddle version python -c \"import paddle; print(paddle.__version__)\" Requirements: Python2 or Python3 (Only support Python3 for windows) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2 Other Dependencies COCO-API : COCO-API is needed for running. Installation is as follows: git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # if cython is not installed pip install Cython # Install into global site-packages make install # Alternatively, if you do not have permissions or prefer # not to install the COCO API into global site-packages python setup.py install --user Installation of COCO-API in windows: # if cython is not installed pip install Cython # Because the origin version of cocoapi does not support windows, another version is used which only supports Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI PaddleDetection Clone Paddle models repository: You can clone PaddleDetection with the following commands: cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git Install Python dependencies: Required python packages are specified in requirements.txt , and can be installed with: pip install -r requirements.txt Make sure the tests pass: export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py Datasets PaddleDetection includes support for COCO and Pascal VOC by default, please follow these instructions to set up the dataset. Create symlinks for local datasets: Default dataset path in config files is dataset/coco and dataset/voc , if the datasets are already available on disk, you can simply create symlinks to their directories: ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc For Pascal VOC dataset, you should create file list by: export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py Download datasets manually: On the other hand, to download the datasets, run the following commands: COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO dataset with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC dataset with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Download datasets automatically: If a training session is started but the dataset is not setup properly (e.g, not found in dataset/coco or dataset/voc ), PaddleDetection can automatically download them from COCO-2017 and VOC2012 , the decompressed datasets will be cached in ~/.cache/paddle/dataset/ and can be discovered automatically subsequently. NOTE: For further informations on the datasets, please see DATA.md","title":"Installation"},{"location":"INSTALL/#installation","text":"","title":"Installation"},{"location":"INSTALL/#table-of-contents","text":"Introduction PaddlePaddle Other Dependencies PaddleDetection Datasets","title":"Table of Contents"},{"location":"INSTALL/#introduction","text":"This document covers how to install PaddleDetection, its dependencies (including PaddlePaddle), together with COCO and Pascal VOC dataset.","title":"Introduction"},{"location":"INSTALL/#paddlepaddle","text":"Running PaddleDetection requires PaddlePaddle Fluid v.1.6 and later. please follow the instructions in installation document . Please make sure your PaddlePaddle installation was successful and the version of your PaddlePaddle is not lower than required. Verify with the following commands. # To check PaddlePaddle installation in your Python interpreter >>> import paddle.fluid as fluid >>> fluid.install_check.run_check() # To check PaddlePaddle version python -c \"import paddle; print(paddle.__version__)\"","title":"PaddlePaddle"},{"location":"INSTALL/#requirements","text":"Python2 or Python3 (Only support Python3 for windows) CUDA >= 8.0 cuDNN >= 5.0 nccl >= 2.1.2","title":"Requirements:"},{"location":"INSTALL/#other-dependencies","text":"COCO-API : COCO-API is needed for running. Installation is as follows: git clone https://github.com/cocodataset/cocoapi.git cd cocoapi/PythonAPI # if cython is not installed pip install Cython # Install into global site-packages make install # Alternatively, if you do not have permissions or prefer # not to install the COCO API into global site-packages python setup.py install --user Installation of COCO-API in windows: # if cython is not installed pip install Cython # Because the origin version of cocoapi does not support windows, another version is used which only supports Python3 pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI","title":"Other Dependencies"},{"location":"INSTALL/#paddledetection","text":"Clone Paddle models repository: You can clone PaddleDetection with the following commands: cd <path/to/clone/PaddleDetection> git clone https://github.com/PaddlePaddle/PaddleDetection.git Install Python dependencies: Required python packages are specified in requirements.txt , and can be installed with: pip install -r requirements.txt Make sure the tests pass: export PYTHONPATH=`pwd`:$PYTHONPATH python ppdet/modeling/tests/test_architectures.py","title":"PaddleDetection"},{"location":"INSTALL/#datasets","text":"PaddleDetection includes support for COCO and Pascal VOC by default, please follow these instructions to set up the dataset. Create symlinks for local datasets: Default dataset path in config files is dataset/coco and dataset/voc , if the datasets are already available on disk, you can simply create symlinks to their directories: ln -sf <path/to/coco> <path/to/paddle_detection>/dataset/coco ln -sf <path/to/voc> <path/to/paddle_detection>/dataset/voc For Pascal VOC dataset, you should create file list by: export PYTHONPATH=$PYTHONPATH:. python dataset/voc/create_list.py Download datasets manually: On the other hand, to download the datasets, run the following commands: COCO export PYTHONPATH=$PYTHONPATH:. python dataset/coco/download_coco.py COCO dataset with directory structures like this: dataset/coco/ \u251c\u2500\u2500 annotations \u2502 \u251c\u2500\u2500 instances_train2014.json \u2502 \u251c\u2500\u2500 instances_train2017.json \u2502 \u251c\u2500\u2500 instances_val2014.json \u2502 \u251c\u2500\u2500 instances_val2017.json \u2502 | ... \u251c\u2500\u2500 train2017 \u2502 \u251c\u2500\u2500 000000000009.jpg \u2502 \u251c\u2500\u2500 000000580008.jpg \u2502 | ... \u251c\u2500\u2500 val2017 \u2502 \u251c\u2500\u2500 000000000139.jpg \u2502 \u251c\u2500\u2500 000000000285.jpg \u2502 | ... | ... Pascal VOC export PYTHONPATH=$PYTHONPATH:. python dataset/voc/download_voc.py python dataset/voc/create_list.py Pascal VOC dataset with directory structure like this: dataset/voc/ \u251c\u2500\u2500 train.txt \u251c\u2500\u2500 val.txt \u251c\u2500\u2500 test.txt \u251c\u2500\u2500 label_list.txt (optional) \u251c\u2500\u2500 VOCdevkit/VOC2007 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 001789.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... \u251c\u2500\u2500 VOCdevkit/VOC2012 \u2502 \u251c\u2500\u2500 Annotations \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 JPEGImages \u2502 \u251c\u2500\u2500 003876.xml \u2502 | ... \u2502 \u251c\u2500\u2500 ImageSets \u2502 | ... | ... NOTE: If you set use_default_label=False in yaml configs, the label_list.txt of Pascal VOC dataset will be read, otherwise, label_list.txt is unnecessary and the default Pascal VOC label list which defined in voc_loader.py will be used. Download datasets automatically: If a training session is started but the dataset is not setup properly (e.g, not found in dataset/coco or dataset/voc ), PaddleDetection can automatically download them from COCO-2017 and VOC2012 , the decompressed datasets will be cached in ~/.cache/paddle/dataset/ and can be discovered automatically subsequently. NOTE: For further informations on the datasets, please see DATA.md","title":"Datasets"},{"location":"MODEL_ZOO/","text":"English | \u7b80\u4f53\u4e2d\u6587 Model Zoo and Benchmark Environment Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2 Common settings All models below were trained on coco_2017_train , and tested on coco_2017_val . Batch Normalization layers in backbones are replaced by Affine Channel layers. Unless otherwise noted, all ResNet backbones adopt the ResNet-B variant.. For RCNN and RetinaNet models, only horizontal flipping data augmentation was used in the training phase and no augmentations were used in the testing phase. Inf time (fps) : the inference time is measured with fps (image/s) on a single GPU (Tesla V100) with cuDNN 7.5 by running 'tools/eval.py' on all validation set, which including data loadding, network forward and post processing. The batch size is 1. Training Schedules We adopt exactly the same training schedules as Detectron . 1x indicates the schedule starts at a LR of 0.02 and is decreased by a factor of 10 after 60k and 80k iterations and eventually terminates at 90k iterations for minibatch size 16. For batch size 8, LR is decreased to 0.01, total training iterations are doubled, and the decay milestones are scaled by 2. 2x schedule is twice as long as 1x, with the LR milestones scaled accordingly. ImageNet Pretrained Models The backbone models pretrained on ImageNet are available. All backbone models are pretrained on standard ImageNet-1k dataset and can be downloaded here . Notes: The ResNet50 model was trained with cosine LR decay schedule and can be downloaded here . Baselines Faster & Mask R-CNN Backbone Type Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50 Faster 1 1x 12.747 35.2 - model ResNet50 Faster 1 2x 12.686 37.1 - model ResNet50 Mask 1 1x 11.615 36.5 32.2 model ResNet50 Mask 1 2x 11.494 38.2 33.4 model ResNet50-vd Faster 1 1x 12.575 36.4 - model ResNet50-FPN Faster 2 1x 22.273 37.2 - model ResNet50-FPN Faster 2 2x 22.297 37.7 - model ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 model ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 model ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - model ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 model ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - model ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 model CBResNet50-vd-FPN Faster 2 1x - 39.7 - model ResNet101 Faster 1 1x 9.316 38.3 - model ResNet101-FPN Faster 1 1x 17.297 38.7 - model ResNet101-FPN Faster 1 2x 17.246 39.1 - model ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 model ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - model ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - model ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 model CBResNet101-vd-FPN Faster 2 1x - 42.7 - model ResNeXt101-vd-64x4d-FPN Faster 1 1x 8.815 42.2 - model ResNeXt101-vd-64x4d-FPN Faster 1 2x 8.809 41.7 - model ResNeXt101-vd-64x4d-FPN Mask 1 1x 7.689 42.9 37.9 model ResNeXt101-vd-64x4d-FPN Mask 1 2x 7.859 42.6 37.6 model SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - model SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 model ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - model Deformable ConvNets v2 Backbone Type Conv Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - model ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - model ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - model ResNeXt101-vd-64x4d-FPN Faster c3-c5 1 1x 7.209 45.2 - model ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 model ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 model ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 model ResNeXt101-vd-64x4d-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 model ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - model ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - model ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - model SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 model ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - model CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - model Notes: Deformable ConvNets v2(dcn_v2) reference from Deformable ConvNets v2 . c3-c5 means adding dcn in resnet stage 3 to 5. Detailed configuration file in configs/dcn Group Normalization Backbone Type Image/gpu Lr schd Box AP Mask AP Download ResNet50-FPN Faster 2 2x 39.7 - model ResNet50-FPN Mask 1 2x 40.1 35.8 model Notes: Group Normalization reference from Group Normalization . Detailed configuration file in configs/gn YOLO v3 Backbone Pretrain dataset Size deformable Conv Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 (paper) ImageNet 608 False 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 False 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 False 8 270e - 28.2 - DarkNet53 ImageNet 608 False 8 270e 45.571 38.9 model DarkNet53 ImageNet 416 False 8 270e - 37.5 model DarkNet53 ImageNet 320 False 8 270e - 34.8 model MobileNet-V1 ImageNet 608 False 8 270e 78.302 29.3 model MobileNet-V1 ImageNet 416 False 8 270e - 29.3 model MobileNet-V1 ImageNet 320 False 8 270e - 27.1 model ResNet34 ImageNet 608 False 8 270e 63.356 36.2 model ResNet34 ImageNet 416 False 8 270e - 34.3 model ResNet34 ImageNet 320 False 8 270e - 31.4 model ResNet50_vd ImageNet 608 True 8 270e - 39.1 model ResNet50_vd Object365 608 True 8 270e - 41.4 model YOLO v3 on Pascal VOC Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 608 8 270e 54.977 83.5 model DarkNet53 416 8 270e - 83.6 model DarkNet53 320 8 270e - 82.2 model MobileNet-V1 608 8 270e 104.291 76.2 model MobileNet-V1 416 8 270e - 76.7 model MobileNet-V1 320 8 270e - 75.3 model ResNet34 608 8 270e 82.247 82.6 model ResNet34 416 8 270e - 81.9 model ResNet34 320 8 270e - 80.1 model Notes: YOLOv3-DarkNet53 performance in paper YOLOv3 is also provided above, our implements improved performance mainly by using L1 loss in bounding box width and height regression, image mixup and label smooth. YOLO v3 is trained in 8 GPU with total batch size as 64 and trained 270 epoches. YOLO v3 training data augmentations: mixup, randomly color distortion, randomly cropping, randomly expansion, randomly interpolation method, randomly flippling. YOLO v3 used randomly reshaped minibatch in training, inferences can be performed on different image sizes with the same model weights, and we provided evaluation results of image size 608/416/320 above. Deformable conv is added on stage 5 of backbone. RetinaNet Backbone Image/gpu Lr schd Box AP Download ResNet50-FPN 2 1x 36.0 model ResNet101-FPN 2 1x 37.3 model ResNeXt101-vd-FPN 1 1x 40.5 model Notes: In RetinaNet, the base LR is changed to 0.01 for minibatch size 16. SSD Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download VGG16 300 8 40w 81.613 25.1 model VGG16 512 8 40w 46.007 29.1 model Notes: VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 400000 iters. SSD on Pascal VOC Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download MobileNet v1 300 32 120e 159.543 73.2 model VGG16 300 8 240e 117.279 77.5 model VGG16 512 8 240e 65.975 80.2 model NOTE : MobileNet-SSD is trained in 2 GPU with totoal batch size as 64 and trained 120 epoches. VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 240 epoches. SSD training data augmentations: randomly color distortion, randomly cropping, randomly expansion, randomly flipping. Face Detection Please refer face detection models for details. Object Detection in Open Images Dataset V5 Please refer Open Images Dataset V5 Baseline model for details.","title":"MODEL ZOO"},{"location":"MODEL_ZOO/#model-zoo-and-benchmark","text":"","title":"Model Zoo and Benchmark"},{"location":"MODEL_ZOO/#environment","text":"Python 2.7.1 PaddlePaddle >=1.5 CUDA 9.0 cuDNN >=7.4 NCCL 2.1.2","title":"Environment"},{"location":"MODEL_ZOO/#common-settings","text":"All models below were trained on coco_2017_train , and tested on coco_2017_val . Batch Normalization layers in backbones are replaced by Affine Channel layers. Unless otherwise noted, all ResNet backbones adopt the ResNet-B variant.. For RCNN and RetinaNet models, only horizontal flipping data augmentation was used in the training phase and no augmentations were used in the testing phase. Inf time (fps) : the inference time is measured with fps (image/s) on a single GPU (Tesla V100) with cuDNN 7.5 by running 'tools/eval.py' on all validation set, which including data loadding, network forward and post processing. The batch size is 1.","title":"Common settings"},{"location":"MODEL_ZOO/#training-schedules","text":"We adopt exactly the same training schedules as Detectron . 1x indicates the schedule starts at a LR of 0.02 and is decreased by a factor of 10 after 60k and 80k iterations and eventually terminates at 90k iterations for minibatch size 16. For batch size 8, LR is decreased to 0.01, total training iterations are doubled, and the decay milestones are scaled by 2. 2x schedule is twice as long as 1x, with the LR milestones scaled accordingly.","title":"Training Schedules"},{"location":"MODEL_ZOO/#imagenet-pretrained-models","text":"The backbone models pretrained on ImageNet are available. All backbone models are pretrained on standard ImageNet-1k dataset and can be downloaded here . Notes: The ResNet50 model was trained with cosine LR decay schedule and can be downloaded here .","title":"ImageNet Pretrained Models"},{"location":"MODEL_ZOO/#baselines","text":"","title":"Baselines"},{"location":"MODEL_ZOO/#faster-mask-r-cnn","text":"Backbone Type Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50 Faster 1 1x 12.747 35.2 - model ResNet50 Faster 1 2x 12.686 37.1 - model ResNet50 Mask 1 1x 11.615 36.5 32.2 model ResNet50 Mask 1 2x 11.494 38.2 33.4 model ResNet50-vd Faster 1 1x 12.575 36.4 - model ResNet50-FPN Faster 2 1x 22.273 37.2 - model ResNet50-FPN Faster 2 2x 22.297 37.7 - model ResNet50-FPN Mask 1 1x 15.184 37.9 34.2 model ResNet50-FPN Mask 1 2x 15.881 38.7 34.7 model ResNet50-FPN Cascade Faster 2 1x 17.507 40.9 - model ResNet50-FPN Cascade Mask 1 1x - 41.3 35.5 model ResNet50-vd-FPN Faster 2 2x 21.847 38.9 - model ResNet50-vd-FPN Mask 1 2x 15.825 39.8 35.4 model CBResNet50-vd-FPN Faster 2 1x - 39.7 - model ResNet101 Faster 1 1x 9.316 38.3 - model ResNet101-FPN Faster 1 1x 17.297 38.7 - model ResNet101-FPN Faster 1 2x 17.246 39.1 - model ResNet101-FPN Mask 1 1x 12.983 39.5 35.2 model ResNet101-vd-FPN Faster 1 1x 17.011 40.5 - model ResNet101-vd-FPN Faster 1 2x 16.934 40.8 - model ResNet101-vd-FPN Mask 1 1x 13.105 41.4 36.8 model CBResNet101-vd-FPN Faster 2 1x - 42.7 - model ResNeXt101-vd-64x4d-FPN Faster 1 1x 8.815 42.2 - model ResNeXt101-vd-64x4d-FPN Faster 1 2x 8.809 41.7 - model ResNeXt101-vd-64x4d-FPN Mask 1 1x 7.689 42.9 37.9 model ResNeXt101-vd-64x4d-FPN Mask 1 2x 7.859 42.6 37.6 model SENet154-vd-FPN Faster 1 1.44x 3.408 42.9 - model SENet154-vd-FPN Mask 1 1.44x 3.233 44.0 38.7 model ResNet101-vd-FPN CascadeClsAware Faster 2 1x - 44.7(softnms) - model","title":"Faster &amp; Mask R-CNN"},{"location":"MODEL_ZOO/#deformable-convnets-v2","text":"Backbone Type Conv Image/gpu Lr schd Inf time (fps) Box AP Mask AP Download ResNet50-FPN Faster c3-c5 2 1x 19.978 41.0 - model ResNet50-vd-FPN Faster c3-c5 2 2x 19.222 42.4 - model ResNet101-vd-FPN Faster c3-c5 2 1x 14.477 44.1 - model ResNeXt101-vd-64x4d-FPN Faster c3-c5 1 1x 7.209 45.2 - model ResNet50-FPN Mask c3-c5 1 1x 14.53 41.9 37.3 model ResNet50-vd-FPN Mask c3-c5 1 2x 14.832 42.9 38.0 model ResNet101-vd-FPN Mask c3-c5 1 1x 11.546 44.6 39.2 model ResNeXt101-vd-64x4d-FPN Mask c3-c5 1 1x 6.45 46.2 40.4 model ResNet50-FPN Cascade Faster c3-c5 2 1x - 44.2 - model ResNet101-vd-FPN Cascade Faster c3-c5 2 1x - 46.4 - model ResNeXt101-vd-FPN Cascade Faster c3-c5 2 1x - 47.3 - model SENet154-vd-FPN Cascade Mask c3-c5 1 1.44x - 51.9 43.9 model ResNet200-vd-FPN-Nonlocal CascadeClsAware Faster c3-c5 1 2.5x - 51.7%(softnms) - model CBResNet200-vd-FPN-Nonlocal Cascade Faster c3-c5 1 2.5x - 53.3%(softnms) - model","title":"Deformable ConvNets v2"},{"location":"MODEL_ZOO/#notes","text":"Deformable ConvNets v2(dcn_v2) reference from Deformable ConvNets v2 . c3-c5 means adding dcn in resnet stage 3 to 5. Detailed configuration file in configs/dcn","title":"Notes:"},{"location":"MODEL_ZOO/#group-normalization","text":"Backbone Type Image/gpu Lr schd Box AP Mask AP Download ResNet50-FPN Faster 2 2x 39.7 - model ResNet50-FPN Mask 1 2x 40.1 35.8 model","title":"Group Normalization"},{"location":"MODEL_ZOO/#notes_1","text":"Group Normalization reference from Group Normalization . Detailed configuration file in configs/gn","title":"Notes:"},{"location":"MODEL_ZOO/#yolo-v3","text":"Backbone Pretrain dataset Size deformable Conv Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 (paper) ImageNet 608 False 8 270e - 33.0 - DarkNet53 (paper) ImageNet 416 False 8 270e - 31.0 - DarkNet53 (paper) ImageNet 320 False 8 270e - 28.2 - DarkNet53 ImageNet 608 False 8 270e 45.571 38.9 model DarkNet53 ImageNet 416 False 8 270e - 37.5 model DarkNet53 ImageNet 320 False 8 270e - 34.8 model MobileNet-V1 ImageNet 608 False 8 270e 78.302 29.3 model MobileNet-V1 ImageNet 416 False 8 270e - 29.3 model MobileNet-V1 ImageNet 320 False 8 270e - 27.1 model ResNet34 ImageNet 608 False 8 270e 63.356 36.2 model ResNet34 ImageNet 416 False 8 270e - 34.3 model ResNet34 ImageNet 320 False 8 270e - 31.4 model ResNet50_vd ImageNet 608 True 8 270e - 39.1 model ResNet50_vd Object365 608 True 8 270e - 41.4 model","title":"YOLO v3"},{"location":"MODEL_ZOO/#yolo-v3-on-pascal-voc","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download DarkNet53 608 8 270e 54.977 83.5 model DarkNet53 416 8 270e - 83.6 model DarkNet53 320 8 270e - 82.2 model MobileNet-V1 608 8 270e 104.291 76.2 model MobileNet-V1 416 8 270e - 76.7 model MobileNet-V1 320 8 270e - 75.3 model ResNet34 608 8 270e 82.247 82.6 model ResNet34 416 8 270e - 81.9 model ResNet34 320 8 270e - 80.1 model","title":"YOLO v3 on Pascal VOC"},{"location":"MODEL_ZOO/#notes_2","text":"YOLOv3-DarkNet53 performance in paper YOLOv3 is also provided above, our implements improved performance mainly by using L1 loss in bounding box width and height regression, image mixup and label smooth. YOLO v3 is trained in 8 GPU with total batch size as 64 and trained 270 epoches. YOLO v3 training data augmentations: mixup, randomly color distortion, randomly cropping, randomly expansion, randomly interpolation method, randomly flippling. YOLO v3 used randomly reshaped minibatch in training, inferences can be performed on different image sizes with the same model weights, and we provided evaluation results of image size 608/416/320 above. Deformable conv is added on stage 5 of backbone.","title":"Notes:"},{"location":"MODEL_ZOO/#retinanet","text":"Backbone Image/gpu Lr schd Box AP Download ResNet50-FPN 2 1x 36.0 model ResNet101-FPN 2 1x 37.3 model ResNeXt101-vd-FPN 1 1x 40.5 model Notes: In RetinaNet, the base LR is changed to 0.01 for minibatch size 16.","title":"RetinaNet"},{"location":"MODEL_ZOO/#ssd","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download VGG16 300 8 40w 81.613 25.1 model VGG16 512 8 40w 46.007 29.1 model Notes: VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 400000 iters.","title":"SSD"},{"location":"MODEL_ZOO/#ssd-on-pascal-voc","text":"Backbone Size Image/gpu Lr schd Inf time (fps) Box AP Download MobileNet v1 300 32 120e 159.543 73.2 model VGG16 300 8 240e 117.279 77.5 model VGG16 512 8 240e 65.975 80.2 model NOTE : MobileNet-SSD is trained in 2 GPU with totoal batch size as 64 and trained 120 epoches. VGG-SSD is trained in 4 GPU with total batch size as 32 and trained 240 epoches. SSD training data augmentations: randomly color distortion, randomly cropping, randomly expansion, randomly flipping.","title":"SSD on Pascal VOC"},{"location":"MODEL_ZOO/#face-detection","text":"Please refer face detection models for details.","title":"Face Detection"},{"location":"MODEL_ZOO/#object-detection-in-open-images-dataset-v5","text":"Please refer Open Images Dataset V5 Baseline model for details.","title":"Object Detection in Open Images Dataset V5"},{"location":"QUICK_STARTED/","text":"English | \u7b80\u4f53\u4e2d\u6587 Quick Start This tutorial fine-tunes a tiny dataset by pretrained detection model for users to get a model and learn PaddleDetection quickly. The model can be trained in around 20min with good performance. Data Preparation export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py Note: before started, run the following command and specifiy the GPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 Training: python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval Use yolov3_mobilenet_v1 to fine-tune the model from COCO dataset. Meanwhile, loss and mAP can be observed on tensorboard. tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> Result on tensorboard is shown below: Model can be downloaded here Evaluation: python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml Inference: python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg Inference images are shown below: For detailed infomation of training and evalution, please refer to GETTING_STARTED.md .","title":"Quick start"},{"location":"QUICK_STARTED/#quick-start","text":"This tutorial fine-tunes a tiny dataset by pretrained detection model for users to get a model and learn PaddleDetection quickly. The model can be trained in around 20min with good performance.","title":"Quick Start"},{"location":"QUICK_STARTED/#data-preparation","text":"export PYTHONPATH=$PYTHONPATH:. python dataset/fruit/download_fruit.py Note: before started, run the following command and specifiy the GPU export PYTHONPATH=$PYTHONPATH:. export CUDA_VISIBLE_DEVICES=0 Training: python -u tools/train.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ --use_tb=True \\ --tb_log_dir=tb_fruit_dir/scalar \\ --eval Use yolov3_mobilenet_v1 to fine-tune the model from COCO dataset. Meanwhile, loss and mAP can be observed on tensorboard. tensorboard --logdir tb_fruit_dir/scalar/ --host <host_IP> --port <port_num> Result on tensorboard is shown below: Model can be downloaded here Evaluation: python -u tools/eval.py -c configs/yolov3_mobilenet_v1_fruit.yml Inference: python -u tools/infer.py -c configs/yolov3_mobilenet_v1_fruit.yml \\ -o weights=https://paddlemodels.bj.bcebos.com/object_detection/yolov3_mobilenet_v1_fruit.tar \\ --infer_img=demo/orange_71.jpg Inference images are shown below: For detailed infomation of training and evalution, please refer to GETTING_STARTED.md .","title":"Data Preparation"}]}